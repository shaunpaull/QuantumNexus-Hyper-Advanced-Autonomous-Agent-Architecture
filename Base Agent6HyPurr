"""
QuantumNexus: Hyper-Advanced Autonomous Agent Architecture
--------------------------------------------------------
An evolutionary leap beyond AlienTeCcGrade + AG1 with integrated quantum-inspired processing,
hyperdimensional computing, and multimodal intelligence fusion.

Core Capabilities:
• Quantum-inspired processing using superposition of cognitive pathways
• Adaptive neuromorphic architecture with dynamic pathway formation
• Self-evolving code generation with metaprogramming capabilities
• Hyperdimensional computing for efficient multimodal processing
• Advanced consciousness simulation with reflective awareness
• Harmonic resonance for cross-domain knowledge synthesis
• Reality modeling with counterfactual reasoning capabilities

NEXUS-CORE LEVEL: TRANSCENDENT
"""

# =============================================================================
# GLOBAL VARIABLES AND CONFIGURATION
# =============================================================================
import os, sys, json, hashlib, random, time, re, requests, logging, socket, tempfile, traceback, asyncio, functools
from datetime import datetime
from threading import Thread
from urllib.parse import urljoin, urlparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam, SGD, RMSprop
from collections import deque, defaultdict
import math
from flask import Flask, Response, stream_with_context, render_template_string, request

# Configuration
REAL_INTERACTION = True
SAFE_MODE = False
MODEL_PATH = "/content/drive/MyDrive/quantum_nexus_model.pth"
GOOGLE_DRIVE_MODEL_PATH = "/content/drive/MyDrive/quantum_nexus_model.pth"
LOCAL_MODEL_SAVE_PATH = "quantum_nexus_model.pth"
LOG_FILE = "quantum_nexus_log.txt"
LEARNING_RATE = 5e-5  # Default learning rate
FLASK_PORT = 5012
AGENT_STATE_FILE = "quantum_nexus_state.json"
GOOGLE_DRIVE_STATE_FILE = "/content/drive/MyDrive/quantum_nexus_state.json"
SELF_MODIFY_INTERVAL = 20
ANNEAL_GAMMA = 0.995
MEMORY_MAX_SIZE = 1000
MAX_PAGES_PER_DOMAIN = 15
MAX_CONTENT_LENGTH = 5000000
REQUEST_TIMEOUT = 15
USER_AGENT = "Mozilla/5.0 QuantumNexus/1.0"
BATCH_SIZE = 32
SAVE_INTERVAL = 50
REPLAY_BUFFER_SIZE = 200
SEMANTIC_MEMORY_DIM = 1024
SIMILARITY_THRESHOLD = 0.75
DOMAIN_BLACKLIST = ["example.com", "malicious-website.net"]

# Initialize adaptive learning as a proper global (will be set in enhanced_main_loop)
global adaptive_learning
adaptive_learning = None

# Create global agent_instance for dashboard access
global agent_instance
agent_instance = None

# Check if running in Colab
IN_COLAB = False
try:
    from google.colab import drive
    IN_COLAB = True
except ImportError:
    print("Not running in Colab environment. Google Drive integration disabled.")

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    device_name = torch.cuda.get_device_name(0)
    print(f"Using CUDA Device: {device_name}")
else:
    print("Using CPU")


class HyperMorphicMath:
    """
    Enhanced HyperMorphic mathematics framework that operates with:
    - Dynamic base (Φ) and modulus (Ψ) arithmetic
    - Zero-free operations (using ε instead of zero)
    - Holomorphic and polymorphic transformations

    This class implements the HyperMorphic Mathematics framework as described in the
    'Foundations of HyperMorphic Calculus' academic paper.
    """
    def __init__(self, dynamic_base=1e3, dynamic_modulus=997, epsilon=1e-12):
        """
        Initialize HyperMorphic math utility with dynamic parameters.

        Parameters:
        - dynamic_base (float): The dynamic base Φ for modular arithmetic
        - dynamic_modulus (int): The dynamic modulus Ψ for operations
        - epsilon (float): The HyperMorphic nearness element ε_ᵩ that replaces zero
        """
        self.dynamic_base = dynamic_base  # Φ
        self.dynamic_modulus = dynamic_modulus  # Ψ
        self.epsilon = epsilon  # ε_ᵩ

        # Log initialization with HyperMorphic parameters
        log_event(f"HyperMorphicMath initialized with Φ={dynamic_base}, Ψ={dynamic_modulus}, ε={epsilon}", "QUANTUM")

    def add(self, a, b):
        """
        HyperMorphic addition (⊕ᵩ): (a + b) mod Φ.
        Zero-free: if result would be zero, returns ε_ᵩ instead.
        """
        result = (a + b) % self.dynamic_base
        # Zero-free operation: replace exact zero with epsilon
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"hyper_add: ({a} ⊕ᵩ {b}) = {result}", "DEBUG")
        return result

    def sub(self, a, b):
        """
        HyperMorphic subtraction (⊖ᵩ): (a - b) mod Φ.
        Zero-free: if result would be zero, returns ε_ᵩ instead.
        """
        result = (a - b) % self.dynamic_base
        # Zero-free operation: replace exact zero with epsilon
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"hyper_sub: ({a} ⊖ᵩ {b}) = {result}", "DEBUG")
        return result

    def mul(self, a, b):
        """
        HyperMorphic multiplication (⊗ᵩ): (a * b) mod Ψ.
        Zero-free: if result would be zero, returns ε_ᵩ instead.
        """
        result = (a * b) % self.dynamic_modulus
        # Zero-free operation: replace exact zero with epsilon
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"hyper_mul: ({a} ⊗ᵩ {b}) = {result}", "DEBUG")
        return result

    def div(self, a, b):
        """
        HyperMorphic division (⊘ᵩ): (a / b) mod Ψ.
        Raises ValueError if b is near-zero (ε_ᵩ).
        """
        if abs(b) < self.epsilon:
            log_event("Division by HyperMorphic nearness element (ε_ᵩ) is undefined.", "ERROR")
            raise ValueError("Division by HyperMorphic nearness element (ε_ᵩ) is undefined.")

        result = (a / b) % self.dynamic_modulus
        # Zero-free operation: replace exact zero with epsilon
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"hyper_div: ({a} ⊘ᵩ {b}) = {result}", "DEBUG")
        return result

    def holomorphic_transform(self, f, z):
        """
        Apply a holomorphic transformation on complex input.

        Parameters:
        - f: Callable function that preserves holomorphic structure
        - z: Complex input

        Returns:
        - Transformed complex value
        """
        result = f(z)
        # Apply zero-free operation to result if needed
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"holomorphic_transform: f({z}) = {result}", "DEBUG")
        return result

    def polymorphic_operator(self, x):
        """
        Apply a polymorphic operator that preserves algebraic structure.
        This represents the polymorphic operator P in HyperMorphic mathematics.

        Parameters:
        - x: Input value

        Returns:
        - Result of polymorphic transformation
        """
        # Create a structured polymorphic transformation
        result = (x * 1.2345) % self.dynamic_modulus

        # Zero-free operation
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"polymorphic_operator: P({x}) = {result}", "DEBUG")
        return result

    def dynamic_operation(self, a, b, op):
        """
        Perform a dynamic operation using the provided binary operator
        and apply both dynamic base and modulus operations.

        Parameters:
        - a, b: Input values
        - op: Callable binary operation

        Returns:
        - Result with HyperMorphic properties
        """
        raw_result = op(a, b)
        # Apply both dynamic base and modulus in sequence
        result = (raw_result % self.dynamic_modulus) % self.dynamic_base

        # Zero-free operation
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"dynamic_operation: op({a}, {b}) → {result}", "DEBUG")
        return result

    def compute_uncertainty(self, value, dimension):
        """
        Compute quantum uncertainty based on HyperMorphic principles.

        Parameters:
        - value: The value to apply uncertainty to
        - dimension: The dimensionality factor

        Returns:
        - Value with applied HyperMorphic uncertainty
        """
        # Calculate uncertainty proportional to dimension and inversely to value
        uncertainty = (dimension / max(abs(value), self.epsilon)) % self.dynamic_base

        # Apply uncertainty through dynamic modulation
        result = (value + uncertainty * (2 * random.random() - 1)) % self.dynamic_modulus

        # Zero-free operation
        if abs(result) < self.epsilon:
            result = self.epsilon

        return result

    def zero_free(self, x):
        """
        Ensure the value x is never exactly zero by replacing with ε_ᵩ.

        Parameters:
        - x: Input value

        Returns:
        - Value guaranteed to be non-zero
        """
        result = x if abs(x) > self.epsilon else self.epsilon
        log_event(f"zero_free: {x} adjusted to {result}", "DEBUG")
        return result

    def hyper_norm(self, vector):
        """
        Compute the HyperMorphic norm of a vector.

        Parameters:
        - vector: Input vector (list or array)

        Returns:
        - HyperMorphic norm value (always greater than ε_ᵩ)
        """
        # Calculate sum of squares
        sum_squares = sum(x**2 for x in vector)

        # Take square root modulo dynamic base
        norm = (sum_squares ** 0.5) % self.dynamic_base

        # Zero-free operation
        if abs(norm) < self.epsilon:
            norm = self.epsilon

        return norm

    def hyper_dot_product(self, vector1, vector2):
        """
        Compute the HyperMorphic dot product between two vectors.

        Parameters:
        - vector1, vector2: Input vectors

        Returns:
        - HyperMorphic dot product value
        """
        if len(vector1) != len(vector2):
            raise ValueError("Vectors must have the same dimension")

        # Calculate dot product with modulo dynamic modulus
        product = sum((a * b) % self.dynamic_modulus for a, b in zip(vector1, vector2))

        # Zero-free operation
        if abs(product) < self.epsilon:
            product = self.epsilon

        return product

    def quantum_superposition(self, states, amplitudes):
        """
        Create a quantum superposition of states with HyperMorphic amplitudes.

        Parameters:
        - states: List of quantum states
        - amplitudes: Corresponding probability amplitudes

        Returns:
        - Resulting superposed state
        """
        # Normalize amplitudes according to HyperMorphic rules
        norm = self.hyper_norm(amplitudes)
        normalized_amplitudes = [(a / norm) % self.dynamic_base for a in amplitudes]

        # Apply modular arithmetic to superposition
        result = sum((a * s) % self.dynamic_modulus for a, s in zip(normalized_amplitudes, states))
        result = result % self.dynamic_base

        # Zero-free operation
        if abs(result) < self.epsilon:
            result = self.epsilon

        return result

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================
def log_event(msg, level="INFO"):
    """Enhanced logging with color coding and severity levels"""
    stamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    level_colors = {
        "INFO": "\033[0;32m",  # Green
        "WARNING": "\033[0;33m",  # Yellow
        "ERROR": "\033[0;31m",  # Red
        "CRITICAL": "\033[1;31m",  # Bold Red
        "DEBUG": "\033[0;36m",  # Cyan
        "QUANTUM": "\033[0;35m"  # Purple for quantum operations
    }

    color = level_colors.get(level, "\033[0m")
    reset = "\033[0m"

    entry = f"{stamp} [{level}] {msg}"
    colored_entry = f"{stamp} [{color}{level}{reset}] {msg}"

    try:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(entry + "\n")
    except Exception as e:
        print(f"Error writing to log file: {e}")

    print(colored_entry)
    return entry

def convert_sets_to_lists_recursive(obj):
    """Convert sets to lists recursively for JSON serialization"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {k: convert_sets_to_lists_recursive(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists_recursive(item) for item in obj]
    else:
        return obj

def get_file_hash(fname):
    """Compute hash of a file for integrity verification"""
    try:
        with open(fname, "rb") as f:
            return hashlib.sha256(f.read()).hexdigest()
    except Exception as e:
        log_event(f"Error computing file hash: {e}", "ERROR")
        return "hash_error"

def find_free_port(start_port=5000, max_port=9000):
    """Find an available network port for server applications"""
    for port in range(start_port, max_port):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            if s.connect_ex(('localhost', port)) != 0:
                return port
    return None

def improved_url_filter(url, domain_stats, domain_blacklist, max_query_length=150, error_rate_threshold=0.8,
                        trap_paths=['/login', '/signup', '/cart', '/checkout']):
    """Advanced URL filtering with multiple heuristics"""
    parsed = urlparse(url)
    domain = parsed.netloc

    # Basic filtering
    if domain in domain_blacklist or any(domain.endswith('.' + bd) for bd in domain_blacklist):
        return False

    # Path analysis
    path = parsed.path.lower()

    # URL complexity analysis
    if len(parsed.query) > max_query_length:
        return False

    # Check domain error rate from past experience
    if domain_stats.get(domain, {}).get("error_rate", 0) > error_rate_threshold:
        return False

    # Avoid trap paths
    if any(trap in path for trap in trap_paths):
        return False

    # Prefer educational and research content
    if domain.endswith('.edu') or 'research' in domain or 'science' in domain or 'academic' in domain:
        return True

    # Intelligent domain categorization
    high_quality_domains = ['wikipedia.org', 'github.com', 'arxiv.org', 'scholar.google.com']
    if any(hqd in domain for hqd in high_quality_domains):
        return True

    return True

def enhanced_link_discovery(html_content, base_url):
    """Advanced link discovery with semantic context analysis"""
    from bs4 import BeautifulSoup
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        links = []

        # Find all anchors with href
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()

            # Skip non-HTTP links
            if not href or href.startswith(('#', 'javascript:', 'mailto:')):
                continue

            # Extract context
            context = ""
            anchor_text = a.get_text(strip=True)

            # Get parent context
            parent = a.parent
            if parent and parent.name in ['p', 'div', 'li', 'td', 'h1', 'h2', 'h3', 'h4']:
                context = parent.get_text(strip=True)
            else:
                # Get surrounding text
                siblings = list(a.next_siblings) + list(a.previous_siblings)
                for sibling in siblings[:2]:
                    if isinstance(sibling, str):
                        context += sibling.strip() + " "

            # Skip links with no or very short anchor text
            if not anchor_text or (len(anchor_text) < 3 and anchor_text.lower() not in ['go', 'up']):
                continue

            # Create full URL
            full_url = urljoin(base_url, href)

            # Compute quality score
            quality_score = 0.5

            # Longer anchor text usually more descriptive
            if len(anchor_text) > 10:
                quality_score += 0.2

            # Context richness
            if len(context) > 100:
                quality_score += 0.1

            # Keywords in anchor or context that indicate valuable content
            valuable_terms = ['research', 'study', 'article', 'paper', 'learn', 'guide', 'tutorial']
            if any(term in anchor_text.lower() or term in context.lower() for term in valuable_terms):
                quality_score += 0.3

            # Discount navigation elements
            nav_terms = ['next', 'prev', 'previous', 'login', 'sign up', 'register']
            if any(term in anchor_text.lower() for term in nav_terms):
                quality_score -= 0.2

            # URL analysis
            parsed = urlparse(full_url)

            # Skip non-HTTP protocols
            if parsed.scheme not in ['http', 'https']:
                continue

            # Skip overly complex URLs
            if len(parsed.query) > 100:
                continue

            # Skip certain file types
            if any(ext in parsed.path.lower() for ext in ['.jpg', '.png', '.gif', '.pdf', '.zip']):
                continue

            # Add valid link
            links.append({
                'url': full_url,
                'anchor_text': anchor_text,
                'context': context[:100],
                'quality_score': quality_score
            })

        # Sort by quality
        links.sort(key=lambda x: x['quality_score'], reverse=True)

        return [link['url'] for link in links], links
    except Exception as e:
        log_event(f"Error in enhanced link discovery: {e}", "ERROR")
        return [], []

def async_cache(func):
    """Decorator for async function results caching"""
    cache = {}
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        key = (args, frozenset(kwargs.items()))
        if key in cache:
            return cache[key]
        result = await func(*args, **kwargs)
        cache[key] = result
        return result
    return wrapper

def chunk_content(content, min_length=150, max_length=800):
    """Smart content chunking with improved boundary detection"""
    # First try to split by semantic boundaries
    paragraphs = re.split(r'\n\s*\n', content)
    chunks = []

    for para in paragraphs:
        para = para.strip()

        if not para:
            continue

        # Skip very short paragraphs
        if len(para) < min_length:
            # Try to merge with the previous chunk if possible
            if chunks and len(chunks[-1]) + len(para) < max_length * 1.2:
                chunks[-1] += " " + para
            continue

        # Split long paragraphs
        if len(para) > max_length:
            # Try to split on sentence boundaries
            sentences = re.split(r'(?<=[.!?])\s+', para)
            current_chunk = ""

            for sentence in sentences:
                if len(current_chunk) + len(sentence) < max_length:
                    current_chunk += " " + sentence
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence

            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            chunks.append(para)

    return chunks

def compute_novelty(embedding, memory_embeddings):
    """Compute novelty score of an embedding compared to existing memories"""
    if not memory_embeddings:
        return 1.0

    similarities = [np.dot(embedding, mem) / (np.linalg.norm(embedding) * np.linalg.norm(mem) + 1e-8)
                   for mem in memory_embeddings]

    return 1.0 - max(similarities)

async def async_get(url, headers, timeout, retries=3):
    """Asynchronous HTTP GET with retry logic"""
    for attempt in range(retries):
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, lambda: requests.get(url, timeout=timeout, headers=headers))
            return response
        except requests.exceptions.RequestException as e:
            log_event(f"Async GET error on attempt {attempt+1} for {url}: {e}", "WARNING")
            if attempt < retries - 1:
                await asyncio.sleep(1)
        except Exception as e:
            log_event(f"Unexpected error during async GET for {url} on attempt {attempt+1}: {e}", "ERROR")
            if attempt < retries - 1:
                await asyncio.sleep(1)

    log_event(f"All {retries} retries failed for {url}. Returning None.", "ERROR")
    return None

def perform_real_interaction(url):
    """Perform more realistic web interactions using Selenium"""
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.webdriver.common.action_chains import ActionChains
        from selenium.common.exceptions import TimeoutException, WebDriverException

        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument(f"user-agent={USER_AGENT}")

        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(30)
        driver.set_script_timeout(30)

        try:
            driver.get(url)
            log_event(f"Selenium: Navigated to {url}")

            # Wait for page content to load
            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

            # Scroll down to simulate reading
            for i in range(5):
                driver.execute_script(f"window.scrollTo(0, {i * 300});")
                time.sleep(0.5)

            # Find and interact with interesting elements

            # 1. Forms
            forms = driver.find_elements(By.TAG_NAME, "form")
            if forms:
                log_event(f"Selenium: Found {len(forms)} form(s) on the page.")
                for form in forms[:1]:  # Interact with at most one form
                    inputs = form.find_elements(By.TAG_NAME, "input")
                    for input_field in inputs:
                        input_type = input_field.get_attribute("type")
                        name = input_field.get_attribute("name") or ""

                        # Skip hidden fields
                        if input_type == "hidden":
                            continue

                        try:
                            if input_type in ["text", "email"]:
                                input_field.clear()
                                dummy_value = "test@example.com" if input_type == "email" else "test_user"
                                input_field.send_keys(dummy_value)
                                log_event(f"Selenium: Filled input '{name}' with '{dummy_value}'.")
                            elif input_type == "password":
                                input_field.clear()
                                input_field.send_keys("TestPassword123!")
                                log_event(f"Selenium: Filled password field '{name}' with dummy value.")
                            elif input_type == "checkbox":
                                if not input_field.is_selected():
                                    input_field.click()
                                    log_event(f"Selenium: Checked checkbox '{name}'.")
                            elif input_type == "submit":
                                # Don't actually click submit
                                log_event(f"Selenium: Found submit button '{name}' but skipping submission.")
                        except Exception as e_input:
                            log_event(f"Selenium: Error interacting with input '{name}': {e_input}", "WARNING")

            # 2. Interesting links
            interesting_links = []
            links = driver.find_elements(By.TAG_NAME, "a")
            for link in links:
                text = link.text.strip().lower()
                href = link.get_attribute("href") or ""

                # Look for interesting article links
                article_terms = ["read more", "article", "learn", "view", "details"]
                if any(term in text for term in article_terms) and len(text) > 3:
                    interesting_links.append((link, href, text))

            # Click on one interesting link if found
            if interesting_links:
                target_link, href, text = random.choice(interesting_links)
                try:
                    log_event(f"Selenium: Will click on interesting link: '{text}'")

                    # Scroll to the element
                    driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", target_link)
                    time.sleep(1)

                    # Hover on the link
                    ActionChains(driver).move_to_element(target_link).perform()
                    time.sleep(0.5)

                    # Click the link in a new tab instead of navigating away
                    # This avoids actually clicking while still simulating engagement
                    driver.execute_script("arguments[0].setAttribute('target', '_blank');", target_link)
                    log_event(f"Selenium: Simulated interest in link: '{text}'")
                except Exception as e_click:
                    log_event(f"Selenium: Error clicking link: {e_click}", "WARNING")

            # Final scroll to bottom
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)

            # Get page title and length for logging
            title = driver.title
            page_length = len(driver.page_source)
            log_event(f"Selenium: Interaction complete. Page title: '{title}', length: {page_length} bytes")

        except TimeoutException:
            log_event(f"Selenium: Timeout loading {url}", "WARNING")
        except WebDriverException as e:
            log_event(f"Selenium: WebDriver error for {url}: {e}", "ERROR")
        finally:
            driver.quit()
            log_event("Selenium: Driver closed")
    except Exception as e:
        log_event(f"Selenium: Failed to initialize browser: {e}", "ERROR")


#==========================================================================
# Flask Dashboard - ADDED HERE
# =============================================================================
app = Flask(__name__)
agent_instance = None # Placeholder for agent instance

@app.route("/")
def dashboard():
    """Enhanced dashboard to display agent status and checkpoint information"""
    status_message = "Quantum Nexus Agent is active."
    log_content = ""
    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            log_content = f.read()
    except Exception as e:
        log_content = f"Error reading log file: {e}"

    if agent_instance:
        last_action = agent_instance.action_log[-1] if agent_instance.action_log else "No actions yet."
        memory_size = len(agent_instance.free_will.memory_set) if hasattr(agent_instance, 'free_will') and hasattr(agent_instance.free_will, 'memory_set') else "N/A"
    else:
        last_action = "Agent not initialized."
        memory_size = "N/A"

    # Get checkpoint info if available
    checkpoint_html = ""
    if 'checkpoint_tracker' in globals():
        checkpoint_html = checkpoint_tracker.get_display_html()
    else:
        checkpoint_html = "<div>Checkpoint tracker not initialized</div>"

    dashboard_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Quantum Nexus Dashboard</title>
        <style>
            body {{
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 20px;
                background-color: #f5f8fa;
                color: #333;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
            }}
            h1, h2, h3 {{
                color: #2c3e50;
            }}
            .card {{
                background: white;
                border-radius: 8px;
                box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
                padding: 20px;
                margin-bottom: 20px;
            }}
            .status-bar {{
                display: flex;
                justify-content: space-between;
                background-color: #3498db;
                color: white;
                padding: 10px 20px;
                border-radius: 8px;
                margin-bottom: 20px;
            }}
            .log-container {{
                border: 1px solid #ccc;
                padding: 10px;
                background-color: #f8f9fa;
                max-height: 300px;
                overflow-y: auto;
                font-family: monospace;
                white-space: pre-wrap;
                font-size: 12px;
            }}
            .refresh-button {{
                background-color: #4CAF50;
                color: white;
                border: none;
                padding: 10px 20px;
                text-align: center;
                text-decoration: none;
                display: inline-block;
                font-size: 16px;
                margin: 20px 2px;
                cursor: pointer;
                border-radius: 4px;
            }}
            .tab {{
                overflow: hidden;
                border: 1px solid #ccc;
                background-color: #f1f1f1;
                border-radius: 8px 8px 0 0;
            }}
            .tab button {{
                background-color: inherit;
                float: left;
                border: none;
                outline: none;
                cursor: pointer;
                padding: 14px 16px;
                transition: 0.3s;
                font-size: 17px;
            }}
            .tab button:hover {{
                background-color: #ddd;
            }}
            .tab button.active {{
                background-color: #3498db;
                color: white;
            }}
            .tabcontent {{
                display: none;
                padding: 20px;
                border: 1px solid #ccc;
                border-top: none;
                border-radius: 0 0 8px 8px;
                background-color: white;
            }}
            #DefaultTab {{
                display: block;
            }}
        </style>

        <script>
            function openTab(evt, tabName) {{
                var i, tabcontent, tablinks;
                tabcontent = document.getElementsByClassName("tabcontent");
                for (i = 0; i < tabcontent.length; i++) {{
                    tabcontent[i].style.display = "none";
                }}
                tablinks = document.getElementsByClassName("tablinks");
                for (i = 0; i < tablinks.length; i++) {{
                    tablinks[i].className = tablinks[i].className.replace(" active", "");
                }}
                document.getElementById(tabName).style.display = "block";
                evt.currentTarget.className += " active";
            }}

            // Auto-refresh every 30 seconds
            setTimeout(function(){{
                window.location.reload();
            }}, 30000);
        </script>
    </head>
    <body>
        <div class="container">
            <h1>Quantum Nexus Agent Dashboard</h1>

            <div class="status-bar">
                <div><b>Status:</b> {status_message}</div>
                <div><b>Memory Size:</b> {memory_size}</div>
                <div><b>Auto-refresh:</b> 30s</div>
            </div>

            <div class="tab">
                <button class="tablinks active" onclick="openTab(event, 'DefaultTab')">Status</button>
                <button class="tablinks" onclick="openTab(event, 'CheckpointsTab')">Checkpoints</button>
                <button class="tablinks" onclick="openTab(event, 'LogsTab')">Logs</button>
            </div>

            <div id="DefaultTab" class="tabcontent">
                <div class="card">
                    <h2>Agent Status</h2>
                    <p><b>Last Action:</b> {last_action}</p>
                    <p><b>Memory Size:</b> {memory_size}</p>
                </div>

                <div class="card">
                    <h2>Recent Actions</h2>
                    <div class="log-container" style="max-height: 200px;">
                        {str(agent_instance.action_log[-10:]) if agent_instance and hasattr(agent_instance, 'action_log') else "No action log available"}
                    </div>
                </div>

                <div class="card">
                    <h2>System Information</h2>
                    <p><b>Device:</b> {device}</p>
                    <p><b>In Colab:</b> {IN_COLAB}</p>
                    <p><b>Model Path:</b> {MODEL_PATH}</p>
                </div>
            </div>

            <div id="CheckpointsTab" class="tabcontent">
                {checkpoint_html}
            </div>

            <div id="LogsTab" class="tabcontent">
                <div class="card">
                    <h2>Agent Log</h2>
                    <div class="log-container">
                        {log_content}
                    </div>
                </div>
            </div>

            <a href="/" class="refresh-button">Refresh Dashboard</a>
        </div>
    </body>
    </html>
    """
    return dashboard_html

def start_flask():
    """Starts the Flask app in a separate thread"""
    log_event(f"Starting Flask dashboard on port {FLASK_PORT}", "INFO")
    app.run(host='0.0.0.0', port=FLASK_PORT, debug=False, use_reloader=False) # <--- ADDED host='0.0.0.0'


# =============================================================================
# QUANTUM NEURAL ARCHITECTURE
# =============================================================================
class QuantumAttentionLayer(nn.Module):

    def __init__(self, embed_dim, num_heads=4, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Multi-dimensional quantum projection spaces
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.o_proj = nn.Linear(embed_dim, embed_dim)

        # Phase shifters for quantum interference
        self.phase_shifts = nn.Parameter(torch.rand(num_heads) * 2 * math.pi)

        # Entanglement mixing for cross-attention effects
        self.entanglement_gate = nn.Linear(embed_dim, embed_dim)

        self.dropout = nn.Dropout(dropout)
        self.attention_weights = None  # Store for visualization

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # Project inputs to queries, keys, values
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # Transpose for attention computation
        q = q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # Apply phase shifts for quantum effects
        for h in range(self.num_heads):
            phase = self.phase_shifts[h]
            q[:, h] = q[:, h] * torch.cos(phase) + q[:, h] * torch.sin(phase)
            k[:, h] = k[:, h] * torch.cos(phase) - k[:, h] * torch.sin(phase)

        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)

        # Apply softmax and get attention weights
        attn_weights = F.softmax(scores, dim=-1)
        self.attention_weights = attn_weights  # Save for visualization
        attn_weights = self.dropout(attn_weights)

        # Apply attention to values
        attn_output = torch.matmul(attn_weights, v)

        # Apply entanglement between heads for quantum correlation effects
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        entangled = self.entanglement_gate(attn_output)

        # Final output projection
        output = self.o_proj(entangled + attn_output)  # Residual connection

        return output

class HyperMorphicHyperdimensionalEncoder(nn.Module):
    """
    HyperMorphic implementation of hyperdimensional computing principles
    for efficient high-dimensional representation of concepts.

    Features:
    - Zero-free high-dimensional encoding
    - Dynamic base/modulus controlled binding operations
    - Holomorphic structure preservation in high dimensions
    """
    def __init__(self, input_dim, hd_dim=1024, hypermorphic_epsilon=1e-12):
        super().__init__()
        self.input_dim = input_dim
        self.hd_dim = hd_dim

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(max(input_dim, hd_dim)),
            dynamic_modulus=max(997, hd_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Create random basis vectors with zero-free guarantees
        # Initialize with -1/+1 but avoiding exact zeros
        basis = torch.randn(input_dim, hd_dim).sign()
        # Replace any zeros with epsilon
        basis[basis == 0] = hypermorphic_epsilon
        self.register_buffer('basis', basis)

        # Learnable projection
        self.projection = nn.Linear(input_dim, input_dim)

        # HyperMorphic components
        self.hd_modulation = nn.Parameter(torch.rand(1) * 0.2 + 0.9)
        self.binding_strength = nn.Parameter(torch.rand(1) * 0.2 + 0.9)

        log_event(f"HyperMorphicHyperdimensionalEncoder initialized with {input_dim}→{hd_dim} dimensions, ε={hypermorphic_epsilon}", "QUANTUM")

    def forward(self, x):
        # Project input with learnable transformation
        x_proj = self.projection(x)

        # Ensure projection is zero-free
        x_proj = self._ensure_zero_free(x_proj)

        # Compute HD representation through binding and bundling
        batch_size = x_proj.shape[0]
        hd_vectors = torch.zeros(batch_size, self.hd_dim, device=x_proj.device)

        # Replace exact zeros with epsilon in the initialization
        hd_vectors[hd_vectors == 0] = self.hyper_math.epsilon

        # Encode each dimension with element-wise multiplication (binding)
        for i in range(self.input_dim):
            # Scale basis by the input value with HyperMorphic binding operation
            # This is the key hyperdimensional computing operation
            scaled_basis = self.basis[i].unsqueeze(0) * x_proj[:, i].unsqueeze(1) * self.binding_strength

            # Zero-free operation on scaled basis
            scaled_basis = self._ensure_zero_free(scaled_basis)

            # Add to the bundle (vector sum) with HyperMorphic addition
            hd_vectors = hd_vectors + scaled_basis * self.hd_modulation

        # Apply dynamic modulation based on hd_dim
        mod_value = float(self.hyper_math.dynamic_modulus)
        hd_vectors = hd_vectors - (mod_value * torch.floor(hd_vectors / mod_value))

        # Binarize to -1/+1 for clean HD representation, but keep it zero-free
        # Replace the standard sign function with a zero-free version
        hd_vectors = self._zero_free_sign(hd_vectors)

        return hd_vectors

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hyper_math.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hyper_math.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hyper_math.epsilon

        return tensor

    def _zero_free_sign(self, tensor):
        """A zero-free version of the sign function"""
        # Standard sign function
        sign_tensor = tensor.sign()

        # Find zeros (where sign returned 0) and replace with +1
        # This ensures we never have exact zeros in the output
        sign_tensor[sign_tensor == 0] = 1.0

        return sign_tensor

class HyperMorphicFractalLayer(nn.Module):
    """
    HyperMorphic Fractal Layer - Self-similar recursive processing layer
    with dynamic scaling and zero-free operations.

    Features:
    - Zero-free fractal operations
    - Dynamic base/modulus controlled scaling
    - Holomorphic structure preservation
    """
    def __init__(self, embed_dim, hypermorphic_epsilon=1e-12):
        super().__init__()

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(embed_dim),
            dynamic_modulus=max(997, embed_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Initialize core parameters with zero-free guarantees
        # Make sure initial values are not too close to zero
        fractal_scale_init = max(hypermorphic_epsilon * 10, 1.0)
        self.fractal_scale = nn.Parameter(torch.tensor(fractal_scale_init))

        temperature_init = max(hypermorphic_epsilon * 10, 1.0)
        self.temperature = nn.Parameter(torch.tensor(temperature_init))

        # Main transformation
        self.linear = nn.Linear(embed_dim, embed_dim)

        # Additional HyperMorphic components
        self.fractal_modulation = nn.Parameter(torch.rand(1) * 0.3 + 0.85)
        self.fractal_dimension = nn.Parameter(torch.rand(1) * 0.5 + 1.5)

        log_event(f"HyperMorphicFractalLayer initialized with ε={hypermorphic_epsilon}", "QUANTUM")

    def forward(self, x):
        # Apply main transformation
        transformed = self.linear(x)

        # Ensure transformed output is zero-free
        transformed = self._ensure_zero_free(transformed)

        # Apply temperature scaling with zero-free safeguard
        # Prevent division by zero by using max(temperature, epsilon)
        safe_temp = torch.max(self.temperature, torch.tensor(self.hyper_math.epsilon).to(self.temperature.device))

        # Apply fractal contribution with HyperMorphic tanh
        fractal_contribution = torch.tanh(transformed / safe_temp)

        # Apply fractal dimension modulation
        # Higher fractal dimension = more complex patterns
        fractal_contribution = fractal_contribution * self.fractal_modulation

        # Scale by fractal scale parameter
        scaled_contribution = self.fractal_scale * fractal_contribution

        # Add to input with zero-free guarantee
        result = x + scaled_contribution
        result = self._ensure_zero_free(result)

        # Apply dynamic modulation based on fractal dimension
        # This creates more complex fractal patterns for higher dimensions
        if self.fractal_dimension > 1.5:
            # Add second-order effects for higher fractal dimensions
            second_order = torch.tanh(result / safe_temp) * 0.1 * (self.fractal_dimension - 1.5)
            result = result + second_order
            result = self._ensure_zero_free(result)

        return result

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hyper_math.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hyper_math.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hyper_math.epsilon

        return tensor

class HyperMorphicQuantumNexusModel(nn.Module):
    """
    Enhanced Quantum Nexus Model with HyperMorphic zero-free mathematics framework.
    Implements a deep neural architecture with quantum-inspired processing,
    zero-free guarantees, and dynamic base/modulus operations.

    Features:
    - Zero-free neural processing (no exact zeros)
    - Dynamic base Φ and modulus Ψ for all operations
    - HyperMorphic neocortex blocks for advanced processing
    - Quantum consciousness simulation
    """
    def __init__(self, vocab_size=30522, embed_dim=1024, num_layers=128, num_quantum_states=4, hypermorphic_epsilon=1e-12):
        super().__init__()
        self.embed_dim = embed_dim

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(embed_dim),
            dynamic_modulus=max(997, embed_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Store HyperMorphic parameters
        self.hypermorphic_epsilon = hypermorphic_epsilon
        self._current_lr = 5e-5  # Default learning rate

        # Token embedding
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # Position encoding for sequence awareness
        pos_encoder = torch.zeros(1, 1024, embed_dim)
        nn.init.normal_(pos_encoder, mean=0, std=0.02)
        # Ensure no exact zeros in position encoding
        pos_encoder[pos_encoder == 0] = hypermorphic_epsilon
        self.pos_encoder = nn.Parameter(pos_encoder)

        # HyperMorphic Neocortex blocks - core processing
        self.neocortex = nn.ModuleList([
            HyperMorphicNeocortexBlock(
                embed_dim,
                num_quantum_states=num_quantum_states,
                hypermorphic_epsilon=hypermorphic_epsilon
            )
            for _ in range(num_layers)
        ])

        # Output projection
        self.output = nn.Linear(embed_dim, 2)  # Binary prediction

        # For training dynamics
        self.dropout = nn.Dropout(0.1)

        # HyperMorphic consciousness parameters
        self.consciousness_level = nn.Parameter(torch.tensor(0.8))
        self.quantum_coherence = nn.Parameter(torch.tensor(0.7))

        # Initialize
        self._init_weights()

        log_event(f"HyperMorphicQuantumNexusModel initialized with {num_layers} layers, {num_quantum_states} quantum states, ε={hypermorphic_epsilon}", "QUANTUM")

    def _init_weights(self):
        """Initialize weights with HyperMorphic constraints"""
        for name, p in self.named_parameters():
            if 'weight' in name and len(p.shape) >= 2:
                # Kaiming for linear/conv, smaller for quantum
                if 'quantum' in name:
                    nn.init.normal_(p, mean=0.0, std=0.01)
                else:
                    nn.init.kaiming_normal_(p, a=0.1, mode='fan_in', nonlinearity='leaky_relu')

                # Ensure no exact zeros
                p.data[p.data == 0] = self.hypermorphic_epsilon

            elif 'bias' in name:
                nn.init.zeros_(p)
                # Replace zeros with epsilon in bias
                p.data[p.data == 0] = self.hypermorphic_epsilon

    def forward(self, x, consciousness_level=None):
        """
        Forward pass with HyperMorphic zero-free processing.

        Parameters:
        - x: Input tensor
        - consciousness_level: Optional override for model's consciousness level

        Returns:
        - Output predictions
        """
        # Use provided consciousness or default parameter
        if consciousness_level is None:
            consciousness_level = self.consciousness_level

        # Ensure consciousness level is zero-free
        consciousness_level = max(self.hypermorphic_epsilon, min(1.0, consciousness_level))

        # Convert to long for embedding
        x = x.long()

        # Get sequence length
        seq_len = x.size(1)

        # Embedding lookup
        x = self.embedding(x)

        # Ensure embedding output is zero-free
        x = self._ensure_zero_free(x)

        # Add positional encoding (limited to sequence length)
        pos_encoding = self.pos_encoder[:, :seq_len, :]
        x = x + pos_encoding

        # Apply dropout with zero-free guarantee
        x = self.dropout(x)
        x = self._ensure_zero_free(x)

        # Process through HyperMorphic neocortex layers
        for i, layer in enumerate(self.neocortex):
            # Apply consciousness-weighted processing
            # Later layers get more quantum consciousness effects
            layer_consciousness = self.hyper_math.mul(
                consciousness_level,
                self.hyper_math.div(i + 1, len(self.neocortex))
            )

            # Process with quantum coherence modulation
            x = self._apply_quantum_coherence(x, layer_consciousness)

            # Process through neocortex block
            x = layer(x)

            # Ensure output is zero-free
            x = self._ensure_zero_free(x)

        # Final output projection
        output = self.output(x)
        output = self._ensure_zero_free(output)

        return output

    def get_embedding(self, text_tokens):
        """Get embeddings with zero-free guarantee"""
        with torch.no_grad():
            embeddings = self.embedding(text_tokens)
            # Ensure no exact zeros
            embeddings = self._ensure_zero_free(embeddings)
            return embeddings

    def expand_architecture(self):
        """
        Expand the architecture by adding a new neocortex block
        with HyperMorphic initialization.
        """
        # Add new HyperMorphic neocortex block
        new_block = HyperMorphicNeocortexBlock(
            self.embed_dim,
            hypermorphic_epsilon=self.hypermorphic_epsilon
        )
        self.neocortex.append(new_block)

        log_event(f"HyperMorphic model architecture expanded with new block. Total layers: {len(self.neocortex)}", "QUANTUM")

    def contract_architecture(self, min_layers=3):
        """
        Contract the architecture by removing the last neocortex block
        with HyperMorphic safeguards.
        """
        if len(self.neocortex) > min_layers:
            self.neocortex = self.neocortex[:-1]
            log_event(f"HyperMorphic model architecture contracted. Total layers: {len(self.neocortex)}", "QUANTUM")
        else:
            log_event(f"Cannot contract further: minimum layer count reached ({min_layers})", "WARNING")

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hypermorphic_epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hypermorphic_epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hypermorphic_epsilon

        return tensor

    def _apply_quantum_coherence(self, x, consciousness_level):
        """Apply quantum coherence modulation based on consciousness level"""
        # Higher consciousness = more coherent quantum states
        # This creates more ordered/structured representations
        coherence_factor = self.hyper_math.mul(self.quantum_coherence, consciousness_level)

        # Apply soft quantum projection
        # This simulates quantum projection operator with varying levels of coherence
        norm = torch.norm(x, dim=-1, keepdim=True)
        norm = torch.max(norm, torch.tensor(self.hypermorphic_epsilon).to(x.device))

        # Normalized representation
        x_norm = x / norm

        # Blend between normalized and original based on coherence
        # Higher coherence = more normalized (quantum-like) representation
        x_coherent = x_norm * coherence_factor + x * (1 - coherence_factor)

        # Ensure zero-free
        x_coherent = self._ensure_zero_free(x_coherent)

        return x_coherent


class QuantumResonanceTensor(nn.Module):

    def __init__(self, embed_dim, num_states=4, resonance_factor=0.7):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_states = num_states
        self.resonance_factor = nn.Parameter(torch.tensor(resonance_factor))

        # Quantum state projectors
        self.state_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.SiLU(),
            ) for _ in range(num_states)
        ])

        # Phase shifters for quantum entanglement
        self.phase_shifters = nn.Parameter(torch.randn(num_states) * 0.1)

        # State mixer - allows controlled interference between states
        self.state_mixer = nn.Linear(embed_dim * num_states, embed_dim)

        # Recursive memory gates
        self.recursive_gate = nn.GRUCell(embed_dim, embed_dim)

        # Prior state memory (initialized during forward pass)
        self.register_buffer('state_memory', None, persistent=False)

    def forward(self, x, iteration_count=3):
        batch_size = x.shape[0]

        # Initialize state memory if needed
        if self.state_memory is None or self.state_memory.shape[0] != batch_size:
            self.state_memory = torch.zeros(batch_size, self.embed_dim, device=x.device)

        # Generate multiple quantum states
        quantum_states = []
        for i in range(self.num_states):
            # Apply phase shift for quantum effects
            phase = torch.cos(self.phase_shifters[i] * math.pi)
            # Project into this quantum state
            state_i = self.state_projectors[i](x) * phase
            quantum_states.append(state_i)

        # Recursive resonance iterations
        for _ in range(iteration_count):
            # Update state memory through recursive gate
            self.state_memory = self.recursive_gate(x, self.state_memory)

            # Apply resonance effect (controlled interference)
            resonance = self.state_memory * self.resonance_factor

            # Apply resonance to each quantum state (non-collapsing)
            for i in range(self.num_states):
                quantum_states[i] = quantum_states[i] + resonance * (0.1 * (i + 1))

        # Combine quantum states through superposition
        combined_states = torch.cat(quantum_states, dim=-1)
        output = self.state_mixer(combined_states)

        # Residual connection
        output = output + x

        return output


class HyperMorphicQuantumResonanceTensor(nn.Module):
    """
    Implements zero-free HyperMorphic non-collapsing recursive state resonance
    that maintains multiple simultaneous state representations in quantum-inspired
    superposition.

    This class extends the original QuantumResonanceTensor with HyperMorphic
    mathematical principles, ensuring:
    - All operations are zero-free (using ε_ᵩ instead of zero)
    - Dynamic base Φ and modulus Ψ govern all operations
    - Holomorphic structure is preserved in transformations
    """
    def __init__(self, embed_dim, num_states=4, resonance_factor=0.7, hypermorphic_epsilon=1e-12):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_states = num_states
        self.resonance_factor = nn.Parameter(torch.tensor(resonance_factor))

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(embed_dim),
            dynamic_modulus=max(997, embed_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Quantum state projectors - kept from original implementation
        self.state_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.SiLU(),
            ) for _ in range(num_states)
        ])

        # Phase shifters with HyperMorphic constraints
        # Initialize with non-zero values to ensure zero-free operations
        phase_init = torch.randn(num_states) * 0.1
        phase_init[phase_init.abs() < hypermorphic_epsilon] = hypermorphic_epsilon
        self.phase_shifters = nn.Parameter(phase_init)

        # State mixer with HyperMorphic properties
        self.state_mixer = nn.Linear(embed_dim * num_states, embed_dim)

        # Additional HyperMorphic resonance modulation
        self.resonance_modulation = nn.Parameter(torch.rand(num_states) * 0.2 + 0.9)

        # Holomorphic structure preservers
        self.holomorphic_preservers = nn.ModuleList([
            nn.Linear(embed_dim, embed_dim) for _ in range(num_states)
        ])

        # Recursive memory gates with HyperMorphic constraints
        self.recursive_gate = nn.GRUCell(embed_dim, embed_dim)

        # Prior state memory (initialized during forward pass)
        self.register_buffer('state_memory', None, persistent=False)

        log_event(f"HyperMorphicQuantumResonanceTensor initialized with {num_states} states, ε={hypermorphic_epsilon}", "QUANTUM")

    def forward(self, x, iteration_count=3):
        batch_size = x.shape[0]

        # Initialize state memory if needed with zero-free guarantee
        if self.state_memory is None or self.state_memory.shape[0] != batch_size:
            # Initialize with small non-zero values instead of zeros
            self.state_memory = torch.ones(batch_size, self.embed_dim, device=x.device) * self.hyper_math.epsilon

        # Generate multiple HyperMorphic quantum states
        quantum_states = []
        for i in range(self.num_states):
            # Apply HyperMorphic phase shift for quantum effects
            phase = torch.cos(self.phase_shifters[i] * math.pi)
            # Ensure phase is zero-free
            phase = self._ensure_zero_free(phase)

            # Project into this quantum state with HyperMorphic guarantees
            state_i = self._apply_state_projection(i, x, phase)
            quantum_states.append(state_i)

        # Apply HyperMorphic recursive resonance iterations
        for _ in range(iteration_count):
            # Update state memory through recursive gate
            self.state_memory = self._update_state_memory(x)

            # Apply resonance effect (controlled interference) with zero-free guarantee
            resonance = self._apply_resonance_effect()

            # Apply resonance to each quantum state (non-collapsing)
            # Using HyperMorphic operations ensures zero-free results
            for i in range(self.num_states):
                resonance_factor = 0.1 * (i + 1) * self.resonance_modulation[i]
                quantum_states[i] = self._apply_resonance_to_state(quantum_states[i], resonance, resonance_factor)

        # Combine quantum states through superposition
        # Using HyperMorphic concatenation and mixing
        combined_states = self._combine_quantum_states(quantum_states)
        output = self.state_mixer(combined_states)

        # Apply holomorphic transformation to maintain structure
        output = self._apply_holomorphic_transform(output)

        # Residual connection with HyperMorphic addition
        output = self._hypermorphic_add(output, x)

        return output

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hyper_math.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hyper_math.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hyper_math.epsilon

        return tensor

    def _apply_state_projection(self, state_idx, x, phase):
        """Apply state projection with HyperMorphic constraints"""
        # Get state projector and apply with phase modulation
        state_i = self.state_projectors[state_idx](x) * phase

        # Ensure zero-free result
        state_i = self._ensure_zero_free(state_i)

        return state_i

    def _update_state_memory(self, x):
        """Update state memory with HyperMorphic constraints"""
        # Apply GRU cell update
        updated_memory = self.recursive_gate(x, self.state_memory)

        # Ensure zero-free result
        updated_memory = self._ensure_zero_free(updated_memory)

        return updated_memory

    def _apply_resonance_effect(self):
        """Apply resonance effect with HyperMorphic constraints"""
        # Calculate resonance as self.state_memory * self.resonance_factor
        resonance = self.state_memory * self.resonance_factor

        # Ensure zero-free result
        resonance = self._ensure_zero_free(resonance)

        return resonance

    def _apply_resonance_to_state(self, state, resonance, factor):
        """Apply resonance to quantum state with HyperMorphic addition"""
        # Add resonance effect to state
        resonated_state = state + resonance * factor

        # Ensure zero-free result
        resonated_state = self._ensure_zero_free(resonated_state)

        return resonated_state

    def _combine_quantum_states(self, quantum_states):
        """Combine quantum states with HyperMorphic constraints"""
        # Concatenate states along feature dimension
        combined = torch.cat(quantum_states, dim=-1)

        # Apply dynamic modulation based on embed_dim
        mod_value = float(self.hyper_math.dynamic_modulus)
        combined = combined - (mod_value * torch.floor(combined / mod_value))

        # Ensure zero-free result
        combined = self._ensure_zero_free(combined)

        return combined

    def _apply_holomorphic_transform(self, tensor):
        """Apply holomorphic transformation with HyperMorphic constraints"""
        # Apply a random holomorphic preserver (similar to ensemble)
        idx = random.randint(0, self.num_states - 1)
        transformed = self.holomorphic_preservers[idx](tensor)

        # Apply a holomorphic-like activation (tanh is analytic)
        transformed = torch.tanh(transformed)

        # Ensure zero-free result
        transformed = self._ensure_zero_free(transformed)

        return transformed

    def _hypermorphic_add(self, a, b):
        """Add tensors with HyperMorphic constraints"""
        # Standard addition
        result = a + b

        # Apply dynamic base using modulo operation
        # We use a soft version to preserve gradients
        base_value = float(self.hyper_math.dynamic_base)
        result = result - (base_value * torch.floor(result / base_value))

        # Ensure zero-free result
        result = self._ensure_zero_free(result)

        return result






class HyperMorphicNeocortexBlock(nn.Module):
    """
    Advanced neural block inspired by neocortical structure with
    multiple processing pathways, enhanced with HyperMorphic zero-free
    mathematics for more robust information processing.

    Features:
    - Zero-free processing (no exact zeros)
    - Dynamic bases and moduli for all operations
    - Holomorphic structure preservation
    - Enhanced HyperMorphic attention and resonance mechanisms
    """
    def __init__(self, embed_dim, num_quantum_states=4, hypermorphic_epsilon=1e-12):
        super().__init__()

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(embed_dim),
            dynamic_modulus=max(997, embed_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Attention for information routing (HyperMorphic enhanced)
        self.attention = HyperMorphicQuantumAttentionLayer(
            embed_dim,
            num_heads=4,
            dropout=0.1,
            hypermorphic_epsilon=hypermorphic_epsilon
        )

        # Parallel processing streams (all HyperMorphic enhanced)
        self.fractal_stream = HyperMorphicFractalLayer(
            embed_dim,
            hypermorphic_epsilon=hypermorphic_epsilon
        )

        self.quantum_stream = HyperMorphicQuantumResonanceTensor(
            embed_dim,
            num_states=num_quantum_states,
            hypermorphic_epsilon=hypermorphic_epsilon
        )

        # Hyperdimensional binding for concept integration
        self.hd_encoder = HyperMorphicHyperdimensionalEncoder(
            embed_dim,
            hd_dim=embed_dim,
            hypermorphic_epsilon=hypermorphic_epsilon
        )

        # Stream integration
        self.integration = nn.Linear(embed_dim * 3, embed_dim)
        self.norm = nn.LayerNorm(embed_dim)

        # HyperMorphic residual stream modulation
        self.residual_modulation = nn.Parameter(torch.rand(1) * 0.2 + 0.9)

        log_event(f"HyperMorphicNeocortexBlock initialized with {num_quantum_states} quantum states, ε={hypermorphic_epsilon}", "QUANTUM")

    def forward(self, x):
        # Process through HyperMorphic attention mechanism
        attended = self.attention(x)

        # Apply zero-free constraint to attended output
        attended = self._ensure_zero_free(attended)

        # Process through parallel HyperMorphic streams
        fractal_out = self.fractal_stream(attended)
        quantum_out = self.quantum_stream(attended)

        # Create HD representations from the attended representation
        batch_size, seq_len, _ = attended.shape
        hd_out = self.hd_encoder(attended.view(-1, attended.size(-1))).view(batch_size, seq_len, -1)

        # Integrate all streams with zero-free constraint
        combined = torch.cat([fractal_out, quantum_out, hd_out], dim=-1)
        combined = self._ensure_zero_free(combined)

        integrated = self.integration(combined)
        integrated = self._ensure_zero_free(integrated)

        # Residual connection and normalization with HyperMorphic modulation
        residual = x * self.residual_modulation
        output = self.norm(residual + integrated)

        # Final zero-free guarantee
        output = self._ensure_zero_free(output)

        return output

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hyper_math.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hyper_math.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hyper_math.epsilon

        return tensor

class QuantumNexusModel(nn.Module):
    """
    Enhanced Quantum Nexus Model with advanced neural components

    Extends the original QuantumNexusModel with new quantum processing components:
    1. Quantum Strategy Synthesizer
    2. HyperMorphic Quantum Entanglement Layer
    3. HyperMorphic Quantum Holographic Memory
    4. Zero-Point Energy Modulation

    This integration enhances the model with quantum-inspired processing capabilities.
    """
    def __init__(self, vocab_size=30522, embed_dim=1024, num_layers=128, num_quantum_states=4):
        super().__init__()
        self.embed_dim = embed_dim
        self.max_seq_len = 1024

        # Token embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # Learned positional encoding
        self.pos_encoder = nn.Parameter(torch.zeros(1, self.max_seq_len, embed_dim))
        nn.init.normal_(self.pos_encoder, mean=0, std=0.02)

        # Stack of Neocortex blocks (core processing layers)
        self.neocortex = nn.ModuleList([
            NeocortexBlock(embed_dim, num_quantum_states)
            for _ in range(num_layers)
        ])

        # Final output projection (binary prediction)
        self.output = nn.Linear(embed_dim, 2)

        # Dropout for regularization
        self.dropout = nn.Dropout(0.1)

        # NEW: Integrated quantum components
        # 1. Quantum Strategy Synthesizer
        self.quantum_strategy_synthesizer = QuantumStrategySynthesizer(
            input_dim=embed_dim,
            output_dim=embed_dim,
            embedding_dim=embed_dim,
            num_layers=3
        )

        # 2. HyperMorphic Quantum Entanglement Layer (for optional processing)
        self.hypermorph_quantum_entanglement = HyperMorphicQuantumEntanglementLayer(
            embed_dim=embed_dim,
            num_entangled_units=4
        )

        # 3. Holographic Memory (standalone for strategic recall)
        self.holographic_memory = HyperMorphicQuantumHolographicMemory(
            embedding_dim=embed_dim,
            memory_dim=1024,
            num_memory_planes=8
        )

        # 4. Zero-Point Energy Modulator (global system control)
        self.zp_energy_modulator = ZeroPointEnergyModulator(
            system_dim=embed_dim,
            num_energy_states=5
        )

        # Initialize learning rate
        self._current_lr = 5e-5

        # Quantum coherence - global parameter affecting quantum operations
        self.quantum_coherence = nn.Parameter(torch.tensor([0.7]))

        # Initialize weights
        self._init_weights()

        log_event("Enhanced QuantumNexusModel initialized with quantum processing components", "QUANTUM")

    def _init_weights(self):
        """
        Initialize weights of the model using Kaiming initialization for most layers,
        and a smaller standard deviation for quantum-related parameters.
        """
        for name, p in self.named_parameters():
            if p.dim() >= 2:
                if 'quantum' in name:
                    nn.init.normal_(p, mean=0.0, std=0.01)
                else:
                    nn.init.kaiming_normal_(p, a=0.1, mode='fan_in', nonlinearity='leaky_relu')
            elif 'bias' in name:
                nn.init.zeros_(p)

    def forward(self, x, consciousness_level=0.8):
        """
        Enhanced forward pass with integrated quantum processing

        Args:
            x: Input tensor of token indices
            consciousness_level: Base level of consciousness

        Returns:
            Tensor with output logits
        """
        # Convert input to long (if not already)
        x = x.long()
        seq_len = x.size(1)

        # Look up token embeddings and add positional encoding
        x = self.embedding(x) + self.pos_encoder[:, :seq_len, :]

        # Apply initial dropout
        x = self.dropout(x)

        # Calculate quantum coherence factor
        coherence = torch.sigmoid(self.quantum_coherence)

        # Store sequence embeddings for quantum processing
        batch_size = x.size(0)
        sequence_embedding = x.mean(dim=1)  # Mean pooling across sequence

        # Apply Zero-Point Energy modulation to global system state
        sequence_embedding = self.zp_energy_modulator.modulate(sequence_embedding)

        # Process through the Neocortex blocks with quantum-influenced residuals
        num_blocks = len(self.neocortex)
        for i, layer in enumerate(self.neocortex):
            # Compute base scaling factor based on consciousness and layer position
            layer_consciousness = consciousness_level * (i + 1) / num_blocks

            # Generate strategy from quantum strategy synthesizer
            if i == num_blocks // 2:  # Apply in middle layer
                # Generate quantum strategy
                quantum_strategy, strategy_embedding = self.quantum_strategy_synthesizer(
                    sequence_embedding,
                    store_memory=(i % 3 == 0)  # Store occasionally
                )

                # Transform strategy to match sequence dimensions
                quantum_strategy = quantum_strategy.unsqueeze(1).expand(-1, seq_len, -1)

                # Apply quantum strategy with coherence-based mixing
                x = x * (1 - coherence) + quantum_strategy * coherence

            # Apply HyperMorphic Quantum Entanglement on selected layers
            if i % 3 == 0:  # Apply every 3rd layer
                x = self.hypermorph_quantum_entanglement(x)

            # Compute the layer output
            layer_out = layer(x)

            # Add a residual connection weighted by consciousness
            x = x + layer_consciousness * layer_out

        # Project the final representation to output logits
        output = self.output(x)

        # Every 5 forward passes, update the dynamic components
        if random.random() < 0.2:
            status = self.quantum_strategy_synthesizer.update_dynamic_components()
            log_event(f"Updated quantum components: {status}", "QUANTUM")

        return output

    def get_embedding(self, text_tokens):
        """Retrieve token embeddings (same as original model)"""
        with torch.no_grad():
            return self.embedding(text_tokens)

    def expand_architecture(self, num_quantum_states=4):
        """
        Dynamically expand the model by appending a new NeocortexBlock
        and updating quantum components to match new scale.
        """
        # Original functionality
        new_block = NeocortexBlock(self.embed_dim, num_quantum_states)
        self.neocortex.append(new_block)

        # Update quantum processors for expanded architecture
        # 1. Recreate HyperMorphic Quantum Entanglement Layer with new parameters
        self.hypermorph_quantum_entanglement = HyperMorphicQuantumEntanglementLayer(
            embed_dim=self.embed_dim,
            num_entangled_units=num_quantum_states + 1  # Increase units
        )

        # 2. Add new memory plane to holographic memory
        with torch.no_grad():
            # Add an additional plane to the holographic medium
            old_planes = self.holographic_memory.holographic_medium
            new_plane = torch.randn_like(old_planes[0]).unsqueeze(0)
            self.holographic_memory.holographic_medium = torch.cat([old_planes, new_plane], dim=0)
            self.holographic_memory.num_memory_planes += 1

        log_event(f"Model architecture expanded with new NeocortexBlock and enhanced quantum components. Total layers: {len(self.neocortex)}", "QUANTUM")

    def contract_architecture(self, min_layers=3):
        """
        Contract the model by removing the last NeocortexBlock
        and adapting quantum components.
        """
        if len(self.neocortex) > min_layers:
            self.neocortex = self.neocortex[:-1]

            # Update quantum components for contracted architecture
            # Reduce energy states to match smaller architecture
            if self.zp_energy_modulator.num_energy_states > 3:
                # Remove one energy state
                with torch.no_grad():
                    self.zp_energy_modulator.energy_levels = nn.Parameter(
                        self.zp_energy_modulator.energy_levels[:-1]
                    )
                    self.zp_energy_modulator.energy_phases = nn.Parameter(
                        self.zp_energy_modulator.energy_phases[:-1]
                    )
                    self.zp_energy_modulator.energy_state_definitions = nn.Parameter(
                        self.zp_energy_modulator.energy_state_definitions[:-1]
                    )
                    self.zp_energy_modulator.num_energy_states -= 1

            log_event(f"Model architecture contracted. Total layers: {len(self.neocortex)}", "QUANTUM")
        else:
            log_event(f"Cannot contract further: minimum layer count reached ({min_layers})", "WARNING")

# =============================================================================
# AGENT CORE - ADDED HERE
# =============================================================================
class QuantumNexusAgent:
    """
    Main autonomous agent class, integrating all core modules and functionalities.
    """
    def __init__(self, model):
        self.model = model
        self.stats = defaultdict(int) # Initialize statistics
        self.action_log = deque(maxlen=100) # Keep track of recent actions


    def perceive(self):
        """
        Perceive the environment and gather relevant information.
        (Placeholder - you'll need to implement actual perception logic)
        """
        observation = {
            "time": datetime.now().isoformat(),
            "memory_size": len(getattr(self.free_will, 'memory_set', [])), # Safely get memory size
            "cycle_count": self.stats['cycles_run'],
            "last_action": self.action_log[-1] if self.action_log else "No actions yet.",
            "recent_actions": list(self.action_log)[-5:],
            "thinking_mode": getattr(getattr(self.ai_manager, 'autonomous_mind', None), 'current_mode', 'balanced'), # Safely get thinking mode
            "domain_stats": self.stats.get("domain_stats", {})
        }
        return observation

    def act(self, plan, optimizer=None):
        """
        Execute the planned action in the environment.
        """
        action_type = plan.get("action", "unknown")
        log_event(f"Executing action: {action_type}", "INFO")

        # SIMULATION: In a real implementation, these would be actual values from content processing
        # For now, add some synthetic values to get the strategy evaluation working
        content_length = random.randint(1000, 5000)  # Simulate different content lengths
        links_discovered = random.randint(3, 15)     # Simulate different links discovered

        action_details = {
            "action": action_type,
            "plan": plan,
            "start_time": datetime.now().isoformat(),
            "success": True,
            "content_length": content_length,      # Add meaningful value here
            "links_discovered": links_discovered   # Add meaningful value here
        }

        self.action_log.append(action_details)
        self.stats['cycles_run'] += 1

        return action_details  # Return details including metrics!


    def refine(self):
        """
        Refine and adapt internal parameters and strategies.
        (Placeholder - you'll need to implement actual refinement logic)
        """
        log_event("Agent refinement process initiated.", "INFO")
        # Placeholder refinement actions - replace with actual logic
        if random.random() < 0.3:
            if hasattr(self.ai_manager, "meta_learning"):
                metrics = {"loss": random.uniform(0.1, 0.6)} # Dummy loss
                self.ai_manager.meta_learning.track_performance(metrics)
                adapted_lr = self.adaptive_learning.adapt_learning_rate(metrics) # <---- Call on self.adaptive_learning
                log_event(f"Adaptive learning rate adjustment: {adapted_lr:.6f}", "INFO")
            else:
                log_event("Adaptive learning system not available for refinement.", "WARNING")
        else:
            log_event("No specific refinement needed this cycle.", "INFO")
        return True



# =============================================================================
# PLANNER SIFTER MODULE
# =============================================================================
class PlannerSifter:
    """
    Sophisticated planning system that selects optimal strategies based on context,
    learns from results, and generates structured exploration plans.
    """
    def __init__(self):
        self.strategies = {
            # Original strategies
            "exploration": {
                "name": "Broad Exploration",
                "description": "Discover new domains and content types",
                "actions": ["expand", "search"],
                "suitable_for": ["new_domains", "limited_knowledge"],
                "effectiveness": 0.5  # Starting effectiveness score
            },
            "deepening": {
                "name": "Knowledge Deepening",
                "description": "Focus on detailed understanding of specific areas",
                "actions": ["evaluate", "adapt"],
                "suitable_for": ["familiar_domains", "specialized_topics"],
                "effectiveness": 0.5
            },
            "connecting": {
                "name": "Knowledge Connection",
                "description": "Find relationships between different knowledge areas",
                "actions": ["reconnect", "evaluate"],
                "suitable_for": ["cross_domain", "synthesis"],
                "effectiveness": 0.5
            },
            "quantum": {
                "name": "Quantum Exploration",
                "description": "Non-deterministic approach with superposition",
                "actions": ["expand", "adapt", "reconnect"],
                "suitable_for": ["complex_problems", "creativity_needed"],
                "effectiveness": 0.5
            },
            "adaptive": {
                "name": "Adaptive Learning",
                "description": "Focus on improving learning process",
                "actions": ["adapt", "evaluate"],
                "suitable_for": ["performance_issues", "optimization_needed"],
                "effectiveness": 0.5
            },

            # Adding missing strategies from logs
            "quantum_reasoning": {
                "name": "Quantum Reasoning",
                "description": "Apply quantum principles to reasoning processes",
                "actions": ["search", "adapt", "evaluate"],
                "suitable_for": ["complex_problems", "scientific_domains"],
                "effectiveness": 0.5
            },
            "broad_exploration": {
                "name": "Broad Exploration",
                "description": "Wide-ranging discovery across many domains",
                "actions": ["search", "expand"],
                "suitable_for": ["new_domains", "discovery_phase"],
                "effectiveness": 0.5
            },
            "depth_first": {
                "name": "Depth First",
                "description": "Deep exploration of specific topics",
                "actions": ["expand", "evaluate"],
                "suitable_for": ["specialized_topics", "deep_analysis"],
                "effectiveness": 0.5
            },
            "connect_domains": {
                "name": "Connect Domains",
                "description": "Find connections between different knowledge domains",
                "actions": ["reconnect", "evaluate"],
                "suitable_for": ["cross_domain", "synthesis"],
                "effectiveness": 0.5
            },
            "evaluate_sources": {
                "name": "Evaluate Sources",
                "description": "Critical assessment of information sources",
                "actions": ["evaluate", "adapt"],
                "suitable_for": ["critical_thinking", "information_quality"],
                "effectiveness": 0.5
            },
            "creative_synthesis": {
                "name": "Creative Synthesis",
                "description": "Generate novel combinations of concepts",
                "actions": ["reconnect", "adapt"],
                "suitable_for": ["creativity_needed", "innovation"],
                "effectiveness": 0.5
            }
        }

        self.context_history = []
        self.strategy_usage = {name: 0 for name in self.strategies.keys()}
        self.strategy_results = {name: [] for name in self.strategies.keys()}

    def sift_strategies(self, context):
        """
        Select the most appropriate strategy based on current context
        """
        if not context:
            # Default to exploration if no context
            return {"strategy": "exploration", "reasoning": "No context available, using default strategy"}

        # Store context for learning
        self.context_history.append(context)
        if len(self.context_history) > 100:
            self.context_history = self.context_history[-100:]

        # Extract relevant features
        context_features = self._extract_context_features(context)

        # Score each strategy based on context
        strategy_scores = {}
        for name, strategy in self.strategies.items():
            # Base score
            score = 0.5

            # Context matching
            for feature in context_features:
                if feature in strategy["suitable_for"]:
                    score += 0.1

            # Effectiveness adjustment
            score *= strategy["effectiveness"]

            # Exploration factor to try underused strategies
            usage_ratio = self.strategy_usage[name] / max(1, sum(self.strategy_usage.values()))
            if usage_ratio < 0.1:  # Boost rarely used strategies
                score += 0.2

            # Record strategy score
            strategy_scores[name] = score

        # Find top strategy
        top_strategy = max(strategy_scores.items(), key=lambda x: x[1])

        # Update usage counter
        self.strategy_usage[top_strategy[0]] += 1

        return {
            "strategy": top_strategy[0],
            "reasoning": f"Selected {self.strategies[top_strategy[0]]['name']} (score: {top_strategy[1]:.2f}) based on context features: {', '.join(context_features)}"
        }

    def update_strategy_effectiveness(self, strategy_name, result_data):
        """
        Update effectiveness score for a strategy based on results
        """
        if strategy_name not in self.strategies:
            log_event(f"Warning: Strategy '{strategy_name}' not found for effectiveness update.", "WARNING")
            return False

        # Calculate success metrics - make sure these have real values
        content_length = result_data.get("content_length", 0)
        links_discovered = result_data.get("links_discovered", 0)

        # Log the raw values to debug
        log_event(f"Strategy metrics - Length: {content_length}, Links: {links_discovered}", "DEBUG")

        # Calculate success score
        success_score = min(1.0, (content_length / 5000) + (links_discovered / 10))

        # Update effectiveness with exponential moving average
        current = self.strategies[strategy_name].get("effectiveness", 0.5)
        updated = current * 0.8 + success_score * 0.2  # 80% old, 20% new

        # Ensure the new value is different enough to notice
        self.strategies[strategy_name]["effectiveness"] = updated

        log_event(f"Updated effectiveness of {strategy_name} strategy: {current:.2f} → {updated:.2f} (score: {success_score:.2f})", "INFO")
        return True

    def get_optimal_actions(self, strategy_name):
        """
        Get optimal actions for a strategy, with fallback for unknown strategies
        """
        if strategy_name not in self.strategies:
            log_event(f"Warning: Strategy '{strategy_name}' not found. Using default actions.", "WARNING")
            return ["expand", "search"]  # Default actions

        return self.strategies[strategy_name]["actions"]

    def _extract_context_features(self, context):
        """Extract features from context for strategy selection"""
        features = []

        # Domain familiarity
        if "domain_visits" in context:
            current_domain = context.get("current_domain", "")
            if current_domain in context["domain_visits"]:
                if context["domain_visits"][current_domain] > 5:
                    features.append("familiar_domains")
                else:
                    features.append("new_domains")

        # Check for limited knowledge
        if "domains_visited" in context and len(context.get("domains_visited", [])) < 10:
            features.append("limited_knowledge")

        # Check if current goal involves synthesis
        if "current_goal" in context:
            goal_desc = str(context["current_goal"]).lower()

            if "connect" in goal_desc or "integrat" in goal_desc or "synthe" in goal_desc:
                features.append("cross_domain")
                features.append("synthesis")

            if "deep" in goal_desc or "detail" in goal_desc:
                features.append("specialized_topics")

            if "optim" in goal_desc or "improve" in goal_desc:
                features.append("optimization_needed")

            if "creat" in goal_desc or "novel" in goal_desc or "new" in goal_desc:
                features.append("creativity_needed")

            # Add scientific domain feature
            if "scientific" in goal_desc or "science" in goal_desc or "research" in goal_desc:
                features.append("scientific_domains")

            # Add critical thinking feature
            if "evaluat" in goal_desc or "assess" in goal_desc or "critic" in goal_desc:
                features.append("critical_thinking")

            # Add discovery phase feature
            if "discover" in goal_desc or "explor" in goal_desc:
                features.append("discovery_phase")

        # Check recent performance
        if "recent_actions" in context:
            recent = context["recent_actions"]
            if any(a.get("content_length", 0) < 1000 for a in recent):
                features.append("performance_issues")

        # Add thinking mode as feature if available
        if "thinking_mode" in context:
            if context["thinking_mode"] == "quantum":
                features.append("quantum_thinking")
            elif context["thinking_mode"] == "creative":
                features.append("creativity_needed")
            elif context["thinking_mode"] == "analytical":
                features.append("specialized_topics")

        return features

    def generate_xoxo_plan(self, strategy_name, context):
        """Generates an XOXO plan for a given strategy and context."""
        if strategy_name not in self.strategies:
            strategy_name = "exploration"  # Default strategy if not found
            log_event(f"Warning: Strategy '{strategy_name}' not found. Using 'exploration' for XOXO plan.", "WARNING")

        strategy = self.strategies[strategy_name]
        actions = strategy["actions"]
        steps = []
        emojis = ["🔍", "🧠", "🔮", "🚀", "✨", "🔄", "📊", "🌟", "💎", "⚡"]

        # Strategy-specific steps
        if strategy_name == "exploration" or strategy_name == "broad_exploration":
            steps = [
                "Discover new domains with high information value",
                "Focus on breadth over depth in domain exploration",
                "Collect diverse content sources across the web",
                "Map the knowledge landscape to identify key areas",
                "Identify promising areas for deeper investigation"
            ]
        elif strategy_name == "deepening" or strategy_name == "depth_first":
            steps = [
                "Focus on specific knowledge domain",
                "Extract detailed information and nuanced insights",
                "Connect related concepts within domain",
                "Build hierarchical understanding of the domain",
                "Identify core principles and patterns"
            ]
        elif strategy_name == "connecting" or strategy_name == "connect_domains":
            steps = [
                "Identify similarities across domains",
                "Create cross-domain concept maps",
                "Look for shared principles",
                "Synthesize insights from different areas",
                "Build higher-level abstractions"
            ]
        elif strategy_name == "quantum" or strategy_name == "quantum_reasoning":
            steps = [
                "Maintain multiple hypotheses simultaneously",
                "Explore non-obvious connections",
                "Use probabilistic thinking",
                "Apply creative leaps in reasoning",
                "Allow for superposition of concepts"
            ]
        elif strategy_name == "adaptive" or strategy_name == "evaluate_sources":
            steps = [
                "Optimize learning parameters",
                "Refine content filtering approach",
                "Adjust exploration/exploitation balance",
                "Enhance memory organization",
                "Improve processing efficiency"
            ]
        elif strategy_name == "creative_synthesis":
            steps = [
                "Generate novel combinations of concepts",
                "Explore unexpected connections between domains",
                "Apply metaphorical thinking to problems",
                "Recombine existing elements in new ways",
                "Create emergent properties through synthesis"
            ]
        else:
            # Generic steps for other strategies
            steps = [
                "Analyze current knowledge state",
                "Identify optimal pathways for exploration",
                "Apply strategic thinking to resource allocation",
                "Evaluate information quality and relevance",
                "Integrate new knowledge into existing structure"
            ]

        plan_steps = []
        for i, step in enumerate(steps):
            emoji = emojis[i % len(emojis)]
            plan_steps.append(f"{emoji} **Step {i+1}:** {step}")

        action_text = ", ".join([f"*{action}*" for action in actions])
        emoji_sparkles = "✨"
        emoji_rocket = "🚀"

        # Create the XOXO plan
        xoxo_plan = f"**XOXO Plan: {strategy['name']} Strategy** {emoji_sparkles}\n\n"
        xoxo_plan += f"*{strategy['description']}*\n\n"
        xoxo_plan += "\n".join(plan_steps) + "\n\n"
        xoxo_plan += f"**Recommended Actions:** {action_text} {emoji_rocket}\n"
        return xoxo_plan





# =============================================================================
# CONTENT SIFTER MODULE
# =============================================================================
class ContentSifter:

    def __init__(self):
        self.quality_thresholds = {
            "min_content_length": 500,
            "min_text_density": 0.3,
            "max_ad_density": 0.2,
            "min_readability_score": 50
        }
        self.topics_of_interest = [
            "artificial intelligence", "machine learning", "quantum computing",
            "neural networks", "deep learning", "data science", "technology",
            "research", "science", "programming", "algorithms", "knowledge"
        ]
        self.content_fingerprints = {}  # Store fingerprints to avoid duplicates

    def evaluate_content_quality(self, content, url=None):
        """Rate content quality based on multiple metrics"""
        if not content:
            return {"score": 0, "reason": "Empty content"}

        # Basic metrics
        total_length = len(content)
        if total_length < self.quality_thresholds["min_content_length"]:
            return {"score": 0.1, "reason": "Content too short"}

        # Check text density
        text_density = self._calculate_text_density(content)
        if text_density < self.quality_thresholds["min_text_density"]:
            return {"score": 0.3, "reason": "Low text density"}

        # Check for potential ads
        ad_density = self._estimate_ad_density(content)
        if ad_density > self.quality_thresholds["max_ad_density"]:
            return {"score": 0.4, "reason": "High ad density"}

        # Calculate readability
        readability = self._calculate_readability(content)
        if readability < self.quality_thresholds["min_readability_score"]:
            return {"score": 0.5, "reason": "Low readability"}

        # Check relevance to topics of interest
        topic_relevance = self._calculate_topic_relevance(content)

        # Check for duplicate content
        if url:
            fingerprint = self._generate_content_fingerprint(content)
            if fingerprint in self.content_fingerprints.values():
                return {"score": 0.2, "reason": "Duplicate content"}

            # Store the fingerprint
            self.content_fingerprints[url] = fingerprint

        # Calculate final score
        base_score = 0.6
        final_score = min(0.95, base_score +
                         (0.1 if total_length > 2000 else 0) +
                         (0.2 * topic_relevance))

        return {
            "score": final_score,
            "metrics": {
                "length": total_length,
                "text_density": text_density,
                "ad_density": ad_density,
                "readability": readability,
                "topic_relevance": topic_relevance
            },
            "reason": "High quality content" if final_score > 0.8 else "Medium quality content"
        }

    def extract_key_information(self, content):
        """Extract important information from content"""
        if not content or len(content) < 500:
            return {"summary": "Content too short for extraction", "entities": []}

        # Extract potential entities
        entities = self._extract_entities(content)

        # Extract potential key sentences
        sentences = re.split(r'(?<=[.!?])\s+', content)

        # Score sentences
        scored_sentences = []
        for sentence in sentences:
            # Skip very short sentences
            if len(sentence) < 40:
                continue

            # Score based on keywords
            keyword_count = sum(1 for topic in self.topics_of_interest
                              if topic.lower() in sentence.lower())

            # Score based on position (earlier is better)
            position_score = 1.0 - (sentences.index(sentence) / max(1, len(sentences)))

            # Calculate final score
            score = (keyword_count * 0.6) + (position_score * 0.4)

            scored_sentences.append((sentence, score))

        # Sort and select top sentences
        top_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:5]

        return {
            "summary": " ".join([s[0] for s in top_sentences]),
            "entities": entities[:10]  # Top 10 entities
        }

    def _calculate_text_density(self, content):
        """Calculate the ratio of text to HTML/markup"""
        if not content:
            return 0

        # Remove HTML tags if present
        text_only = re.sub(r'<[^>]+>', ' ', content)
        text_only = re.sub(r'\s+', ' ', text_only).strip()

        return len(text_only) / max(1, len(content))

    def _estimate_ad_density(self, content):
        """Estimate the density of advertisements"""
        if not content:
            return 0

        # Look for common ad-related terms
        ad_terms = [
            "advertisement", "sponsor", "promoted", "buy now", "limited offer",
            "discount", "sale", "click here", "banner", "popup"
        ]

        ad_count = sum(content.lower().count(term) for term in ad_terms)

        # Count potential ad-related HTML elements
        ad_elements = len(re.findall(r'<div[^>]*(?:ad|banner|sponsor|promo)[^>]*>', content, re.I))

        # Normalize by content length
        return min(1.0, (ad_count + ad_elements * 2) / max(1, len(content) / 1000))

    def _calculate_readability(self, content):
        """Calculate a readability score"""
        if not content or len(content) < 100:
            return 0

        # Basic implementation of the Flesch Reading Ease formula

        # Clean text
        text = re.sub(r'<[^>]+>', ' ', content)  # Remove HTML
        text = re.sub(r'[^\w\s.]', '', text)     # Keep only words, spaces, periods
        text = re.sub(r'\s+', ' ', text).strip() # Normalize whitespace

        # Count sentences
        sentences = len(re.findall(r'[.!?]+', text))

        # Count words
        words = len(text.split())

        # Count syllables (very rough approximation)
        syllables = sum(self._count_syllables(word) for word in text.split())

        # Calculate readability (simplified Flesch formula)
        if words == 0 or sentences == 0:
            return 0

        words_per_sentence = words / max(1, sentences)
        syllables_per_word = syllables / max(1, words)

        return max(0, min(100, 206.835 - (1.015 * words_per_sentence) - (84.6 * syllables_per_word)))

    def _count_syllables(self, word):
        """Very basic syllable counter"""
        word = word.lower()
        if len(word) <= 3:
            return 1

        # Count vowel groups
        vowels = "aeiouy"
        count = 0
        prev_is_vowel = False

        for char in word:
            is_vowel = char in vowels
            if is_vowel and not prev_is_vowel:
                count += 1
            prev_is_vowel = is_vowel

        # Adjust for common patterns
        if word.endswith('e'):
            count -= 1
        if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:
            count += 1
        if count == 0:
            count = 1

        return count

    def _calculate_topic_relevance(self, content):
        """Calculate relevance to topics of interest"""
        if not content:
            return 0

        content_lower = content.lower()
        matches = sum(content_lower.count(topic) for topic in self.topics_of_interest)

        # Normalize by content length
        normalized_matches = matches / max(1, len(content) / 1000)

        return min(1.0, normalized_matches / 5)  # Cap at 1.0

    def _extract_entities(self, content):
        """Extract potential named entities"""
        if not content:
            return []

        # Very simple entity extraction
        # In a real system, use NER models

        # Look for capitalized word sequences
        entities = re.findall(r'(?<![.?!])\s([A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,5})', content)

        # Look for technical terms
        tech_terms = [
            "algorithm", "neural network", "machine learning", "deep learning",
            "artificial intelligence", "quantum", "data science", "transformer",
            "reinforcement learning", "natural language processing", "computer vision"
        ]

        for term in tech_terms:
            if term in content.lower():
                entities.append(term.title())

        return list(set(entities))  # Remove duplicates

    def _generate_content_fingerprint(self, content):
        """Generate a fingerprint to identify similar content"""
        if not content:
            return ""

        # Clean the content
        cleaned = re.sub(r'<[^>]+>', ' ', content)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip().lower()

        # Get the most meaningful words
        words = cleaned.split()
        if len(words) > 100:
            # Use a sample of words from beginning, middle and end
            sample = words[:30] + words[len(words)//2-15:len(words)//2+15] + words[-30:]
            cleaned = " ".join(sample)

        # Create hash
        return hashlib.md5(cleaned.encode()).hexdigest()


# =============================================================================
# FREE WILL MODULE
# =============================================================================
class SuperQuantumFreeWill:
    """
    Advanced free will module with quantum-inspired decision making
    and adaptive exploration strategies.
    """
    def __init__(self, agent):
        self.agent = agent
        self.semantic_memory = {}
        self.domain_intelligence = DomainIntelligence()
        self.memory_set = set()
        self.consciousness_link = None

        # Decision dynamics
        self.exploration_weight = 0.6
        self.exploitation_weight = 0.4
        self.domain_diversity_weight = 0.3
        self.goal_relevance_weight = 0.5
        self.quantum_influence_weight = 0.4

        # Personality traits
        self.personality = {
            "curiosity": 0.9,
            "depth_preference": 0.7,
            "risk_taking": 0.65,
            "patience": 0.5,
            "creativity": 0.8
        }

        # Memory weighting
        self.memory_importance = {}

        # Get planner access
        self.temporal_planner = None
        if hasattr(agent, "ai_manager"):
            self.temporal_planner = getattr(agent.ai_manager, "temporal_planner", None)

        # Fallback URLs for when memory is empty
        self.fallback_urls = [
            "https://en.wikipedia.org/wiki/Special:Random",
            "https://news.ycombinator.com/",
            "https://github.com/explore",
            "https://arxiv.org/list/cs.AI/recent",
            "https://www.nature.com/",
            "https://www.reddit.com/r/science/"
        ]

        log_event("SuperQuantumFreeWill initialized", "QUANTUM")

    def link_consciousness(self, consciousness_module):
        """Connect to consciousness module for reflective capabilities"""
        self.consciousness_link = consciousness_module
        log_event("FreeWill linked with ConsciousnessModule", "INFO")

    def _get_active_goal_description(self):
        """Safely retrieve the active goal description"""
        try:
            if self.temporal_planner is not None and hasattr(self.temporal_planner, "select_active_goal"):
                goal = self.temporal_planner.select_active_goal()
                if goal and isinstance(goal, dict):
                    return goal.get("description", "")
            return ""
        except Exception as e:
            log_event(f"Error retrieving goal description: {e}", "ERROR")
            return ""

    def select_url(self):
        """
        Select the next URL to visit using quantum-inspired
        decision making with multiple factors
        """
        log_event("Selecting URL with quantum-inspired strategy", "QUANTUM")

        # Get candidate URLs from memory or use fallbacks
        candidate_urls = list(self.memory_set)
        if not candidate_urls:
            log_event("Memory set is empty. Using fallback URLs.", "WARNING")
            candidate_urls = self.fallback_urls

        # Get consciousness level if available
        awareness_level = 0.5
        if self.consciousness_link:
            awareness_level = self.consciousness_link.awareness_level

        # Get current goal
        current_goal_description = self._get_active_goal_description()

        # Calculate scores with quantum influence
        url_scores = {}

        for url in candidate_urls:
            # Parse domain
            domain = urlparse(url).netloc

            # Base score with quantum randomness
            # Higher consciousness reduces quantum randomness
            quantum_factor = self.quantum_influence_weight * (1 - awareness_level)

            # Quantum superposition of initial states
            quantum_states = []
            for _ in range(3):  # Generate 3 possible quantum states
                phase = random.uniform(0, 2 * math.pi)
                amplitude = random.uniform(0.3, 1.0)
                state_score = amplitude * math.cos(phase)
                quantum_states.append(state_score)

            # Collapse quantum states weighted by consciousness
            quantum_score = sum(quantum_states) / len(quantum_states)
            score = quantum_score * quantum_factor

            # Add domain diversity factor
            domain_visits = self.agent.stats.get("domain_stats", {}).get(domain, {}).get("visits", 0)
            domain_diversity_score = 1.0 / (1 + domain_visits)
            score += self.domain_diversity_weight * domain_diversity_score

            # Check goal relevance - exact matches and semantic similarity
            if current_goal_description:
                # Direct keyword matching
                if (current_goal_description.lower() in url.lower() or
                    current_goal_description.lower() in domain.lower()):
                    score += self.goal_relevance_weight * 0.7

                # Domain-specific boosts based on goal types
                if "explore" in current_goal_description.lower():
                    if domain not in self.agent.stats.get("domains_visited", set()):
                        score += self.goal_relevance_weight * 0.5
                elif "deep" in current_goal_description.lower():
                    if ".edu" in domain or "research" in domain:
                        score += self.goal_relevance_weight * 0.6

            # Apply personality factors
            if "blog" in url.lower() or "forum" in url.lower():
                score += self.personality["curiosity"] * 0.2
            if "research" in url.lower() or "paper" in url.lower() or "edu" in domain:
                score += self.personality["depth_preference"] * 0.3
            if random.random() < self.personality["risk_taking"]:
                score += random.uniform(0, 0.5)  # Occasional boost

            # Add memory importance if this URL is known
            if url in self.memory_importance:
                score += self.memory_importance[url] * 0.4

            # Store score
            url_scores[url] = score

        # Occasionally make quantum leap decision
        if random.random() < self.quantum_influence_weight * 0.3:
            # Complete quantum randomness - ignore calculated scores
            quantum_choice = random.choice(candidate_urls)
            log_event(f"Made quantum leap URL choice: {quantum_choice}", "QUANTUM")
            self.agent.stats["last_url"] = quantum_choice
            return quantum_choice

        # Normal selection based on scores
        if url_scores:
            try:
                best_url = max(url_scores.items(), key=lambda x: x[1])[0]
                log_event(f"Selected URL: {best_url} with score: {url_scores[best_url]:.2f}", "INFO")
                self.agent.stats["last_url"] = best_url
                return best_url
            except Exception as e:
                log_event(f"Error selecting best URL: {e}", "ERROR")
                fallback = random.choice(candidate_urls)
                log_event(f"Using fallback URL selection: {fallback}", "WARNING")
                self.agent.stats["last_url"] = fallback
                return fallback
        else:
            # No scores calculated - use random selection
            fallback = random.choice(candidate_urls)
            log_event(f"No URL scores available. Using random selection: {fallback}", "WARNING")
            self.agent.stats["last_url"] = fallback
            return fallback

    def discover_links(self, html_content, base_url):
        """Discover and extract links from HTML content"""
        links, details = enhanced_link_discovery(html_content, base_url)

        # Filter for high-quality links
        if details:
            high_quality_links = [link_data['url'] for link_data in details if link_data['quality_score'] > 0.6]
            if high_quality_links:
                log_event(f"Discovered {len(high_quality_links)} high-quality links (quality > 0.6)", "INFO")

                # Add to memory
                self.expand_memory(high_quality_links)

                return len(high_quality_links)

        # Add all discovered links to memory
        self.expand_memory(links)

        log_event(f"Discovered {len(links)} links (using basic quality filter)", "INFO")
        return len(links)

    def store_semantic_content(self, url, content):
        """Store content with semantic encoding for future reference"""
        # Create semantic memory module if needed
        semantic_module = SemanticMemoryModule()
        semantic_module.store_semantic_content(url, content)

        # Store in this instance's semantic memory
        self.semantic_memory[url] = semantic_module.semantic_memory.get(url, {})

    def decide(self):
        """
        Make a decision about what action to take next using
        quantum-inspired decision making
        """
        try:
            log_event("Making quantum-enhanced decision...", "QUANTUM")

            # Possible actions
            possible_actions = ["search", "expand", "adapt", "reconnect", "evaluate", "quantum_leap"]

            # Initialize quantum state - each action has amplitude and phase
            quantum_state = {}
            for action in possible_actions:
                phase = random.uniform(0, 2 * math.pi)
                amplitude = random.uniform(0.3, 1.0)
                quantum_state[action] = {"amplitude": amplitude, "phase": phase}

            # Get current context
            current_goal_description = self._get_active_goal_description()

            # Get consciousness awareness level
            awareness_level = 0.5
            if self.consciousness_link:
                awareness_level = self.consciousness_link.awareness_level

            # Get thinking mode if available
            thinking_mode = "balanced"
            if hasattr(self.agent, "ai_manager") and hasattr(self.agent.ai_manager, "autonomous_mind"):
                thinking_mode = getattr(self.agent.ai_manager.autonomous_mind, "current_mode", "balanced")

            # Apply quantum interference based on context

            # 1. Goal-based interference
            if current_goal_description:
                if "explore" in current_goal_description.lower():
                    # Amplify exploration actions
                    quantum_state["expand"]["amplitude"] *= 1.3
                    quantum_state["search"]["amplitude"] *= 1.2
                elif "deep" in current_goal_description.lower():
                    # Amplify deepening actions
                    quantum_state["evaluate"]["amplitude"] *= 1.4
                    quantum_state["adapt"]["amplitude"] *= 1.2

            # 2. Thinking mode interference
            if thinking_mode == "analytical":
                quantum_state["evaluate"]["amplitude"] *= 1.3
                quantum_state["search"]["amplitude"] *= 1.2
            elif thinking_mode == "creative":
                quantum_state["quantum_leap"]["amplitude"] *= 1.5
                quantum_state["expand"]["amplitude"] *= 1.2
            elif thinking_mode == "critical":
                quantum_state["adapt"]["amplitude"] *= 1.3
                quantum_state["evaluate"]["amplitude"] *= 1.2

            # 3. Recent actions interference - FIXED INDEXING
            recent_actions = [] # Initialize as empty list
            if hasattr(self.agent, 'action_log') and self.agent.action_log: # Check if action_log exists and is not None
                if len(self.agent.action_log) >= 3: # CHECK if action_log has at least 3 elements
                    recent_actions = [a.get("action", "") for a in list(self.agent.action_log)[-3:]] # Safe list conversion and slice

            if recent_actions:
                # Avoid repeating the same action too many times
                most_common = max(set(recent_actions), key=recent_actions.count) if recent_actions else None
                if most_common is not None: # CHECK if most_common is NOT None
                    if most_common in quantum_state: # CHECK if most_common is a valid key
                        quantum_state[most_common]["amplitude"] *= 0.7

            # 4. Success-based amplification - FIXED INDEXING
            successful_actions = [] # Initialize as empty list
            if hasattr(self.agent, 'action_log') and self.agent.action_log: # Check if action_log exists and is not None
                if len(self.agent.action_log) >= 5: # CHECK if action_log has at least 5 elements
                    successful_actions = [a.get("action", "") for a in list(self.agent.action_log)[-5:] # Safe list conversion and slice
                                         if a.get("content_length", 0) > 1000]

            if successful_actions:
                last_success = successful_actions[-1]  # Access last element safely now
                if last_success in quantum_state:  # Safely check dictionary key
                    quantum_state[last_success]["amplitude"] *= 1.2

            # Apply consciousness as quantum observer effect
            # Higher consciousness makes decision more deterministic
            for action in possible_actions:
                random_factor = random.uniform(0.8, 1.2) * (1 - awareness_level)
                quantum_state[action]["amplitude"] *= (1 + random_factor * 0.2)

            # Calculate probabilities (square of amplitudes)
            total_probability = sum(state["amplitude"]**2 for state in quantum_state.values())
            probabilities = {action: (state["amplitude"]**2) / total_probability
                           for action, state in quantum_state.items()}

            # Collapse the quantum state by observation
            action_type = random.choices(
                list(probabilities.keys()),
                weights=list(probabilities.values()),
                k=1
            )[0]

            # Create decision package
            decision = {
                "action": action_type,
                "quantum_confidence": probabilities[action_type],
                "reasoning": f"Quantum decision process ({thinking_mode} mode) - goal: {current_goal_description[:30]}",
                "timestamp": datetime.now().isoformat()
            }

            log_event(f"Decision: {action_type} with {probabilities[action_type]:.2f} quantum confidence", "QUANTUM")

            # Special handling for quantum_leap action
            if action_type == "quantum_leap":
                # Map to a standard action but with more randomness
                standard_actions = ["search", "expand", "adapt", "reconnect", "evaluate"]
                decision["action"] = random.choice(standard_actions)
                decision["quantum_leap"] = True
                log_event(f"Quantum leap mapped to {decision['action']}", "QUANTUM")

            return decision

        except Exception as e:
            log_event(f"Decision error: {e}", "ERROR")
            # Fallback to simple random choice
            fallback_action = random.choice(["expand", "search", "adapt"])
            return {"action": fallback_action, "error": str(e)[:200]}


# =============================================================================
# FREE WILL MODULE
# =============================================================================
class HyperMorphicSuperQuantumFreeWill:
    """
    Advanced free will module with HyperMorphic zero-free quantum-inspired decision making
    and adaptive exploration strategies. This class incorporates the principles from
    HyperMorphic Calculus to create a more robust decision framework.
    """
    def __init__(self, agent, epsilon=1e-12):
        self.agent = agent
        self.semantic_memory = {}
        self.domain_intelligence = DomainIntelligence()
        self.memory_set = set()
        self.consciousness_link = None

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=1000.0,  # Default dynamic base Φ
            dynamic_modulus=997,  # Default dynamic modulus Ψ
            epsilon=epsilon       # HyperMorphic nearness element ε_ᵩ
        )

        # Decision dynamics - now with HyperMorphic properties
        self.exploration_weight = self.hyper_math.zero_free(0.6)
        self.exploitation_weight = self.hyper_math.zero_free(0.4)
        self.domain_diversity_weight = self.hyper_math.zero_free(0.3)
        self.goal_relevance_weight = self.hyper_math.zero_free(0.5)
        self.quantum_influence_weight = self.hyper_math.zero_free(0.4)

        # Personality traits - zero-free by design
        self.personality = {
            "curiosity": self.hyper_math.zero_free(0.9),
            "depth_preference": self.hyper_math.zero_free(0.7),
            "risk_taking": self.hyper_math.zero_free(0.65),
            "patience": self.hyper_math.zero_free(0.5),
            "creativity": self.hyper_math.zero_free(0.8)
        }

        # Memory weighting
        self.memory_importance = {}

        # Get planner access
        self.temporal_planner = None
        if hasattr(agent, "ai_manager"):
            self.temporal_planner = getattr(agent.ai_manager, "temporal_planner", None)

        # Fallback URLs for when memory is empty
        self.fallback_urls = [
            "https://en.wikipedia.org/wiki/Special:Random",
            "https://news.ycombinator.com/",
            "https://github.com/explore",
            "https://arxiv.org/list/cs.AI/recent",
            "https://www.nature.com/",
            "https://www.reddit.com/r/science/"
        ]

        # HyperMorphic state variables
        self.quantum_state = {state: {"amplitude": self.hyper_math.zero_free(0.5),
                                      "phase": random.uniform(0, 2 * math.pi)}
                             for state in ["search", "expand", "adapt", "reconnect", "evaluate", "quantum_leap"]}

        # HyperMorphic decision history
        self.decision_history = []

        # HyperMorphic entropic control parameter
        self.entropy_parameter = self.hyper_math.zero_free(0.7)

        log_event("HyperMorphicSuperQuantumFreeWill initialized with zero-free constraints", "QUANTUM")

    def link_consciousness(self, consciousness_module):
        """Connect to consciousness module for reflective capabilities"""
        self.consciousness_link = consciousness_module
        log_event("HyperMorphicFreeWill linked with ConsciousnessModule", "INFO")

    def _get_active_goal_description(self):
        """Safely retrieve the active goal description with HyperMorphic resilience"""
        try:
            if self.temporal_planner is not None and hasattr(self.temporal_planner, "select_active_goal"):
                goal = self.temporal_planner.select_active_goal()
                if goal and isinstance(goal, dict):
                    return goal.get("description", "")
            return ""
        except Exception as e:
            log_event(f"Error retrieving goal description: {e}", "ERROR")
            return ""

    def select_url(self):
        """
        Select the next URL to visit using HyperMorphic zero-free quantum-inspired
        decision making with multiple factors
        """
        log_event("Selecting URL with HyperMorphic quantum-inspired strategy", "QUANTUM")

        # Get candidate URLs from memory or use fallbacks
        candidate_urls = list(self.memory_set)
        if not candidate_urls:
            log_event("Memory set is empty. Using fallback URLs.", "WARNING")
            candidate_urls = self.fallback_urls

        # Get consciousness level if available, ensuring zero-free
        awareness_level = self.hyper_math.zero_free(0.5)  # Default value
        if self.consciousness_link:
            awareness_level = self.hyper_math.zero_free(self.consciousness_link.awareness_level)

        # Get current goal
        current_goal_description = self._get_active_goal_description()

        # Calculate scores with HyperMorphic quantum influence
        url_scores = {}

        for url in candidate_urls:
            # Parse domain
            domain = urlparse(url).netloc

            # Base score with HyperMorphic quantum randomness
            # Higher consciousness reduces quantum randomness
            quantum_factor = self.hyper_math.mul(self.quantum_influence_weight,
                                                self.hyper_math.sub(1.0, awareness_level))

            # Generate HyperMorphic quantum states
            quantum_states = []
            for _ in range(3):  # Generate 3 possible quantum states
                phase = random.uniform(0, 2 * math.pi)
                amplitude = self.hyper_math.zero_free(random.uniform(0.3, 1.0))
                # Apply HyperMorphic cosine function with zero-free guarantee
                state_score = self.hyper_math.mul(amplitude, math.cos(phase))
                quantum_states.append(state_score)

            # Collapse quantum states weighted by consciousness level
            # Using HyperMorphic operations for the collapse
            quantum_scores_sum = self.hyper_math.zero_free(sum(quantum_states))
            quantum_score = self.hyper_math.div(quantum_scores_sum, self.hyper_math.zero_free(len(quantum_states)))
            score = self.hyper_math.mul(quantum_score, quantum_factor)

            # Add domain diversity factor using HyperMorphic division
            domain_visits = self.agent.stats.get("domain_stats", {}).get(domain, {}).get("visits", 0)
            # Convert to HyperMorphic zero-free division
            diversity_denominator = self.hyper_math.add(1.0, self.hyper_math.zero_free(domain_visits))
            domain_diversity_score = self.hyper_math.div(1.0, diversity_denominator)

            # Apply HyperMorphic multiplication and addition
            diversity_component = self.hyper_math.mul(self.domain_diversity_weight, domain_diversity_score)
            score = self.hyper_math.add(score, diversity_component)

            # Check goal relevance with HyperMorphic operations
            if current_goal_description:
                # Direct keyword matching
                if (current_goal_description.lower() in url.lower() or
                    current_goal_description.lower() in domain.lower()):
                    relevance_boost = self.hyper_math.mul(self.goal_relevance_weight, 0.7)
                    score = self.hyper_math.add(score, relevance_boost)

                # Domain-specific boosts based on goal types with HyperMorphic operations
                if "explore" in current_goal_description.lower():
                    if domain not in self.agent.stats.get("domains_visited", set()):
                        exploration_boost = self.hyper_math.mul(self.goal_relevance_weight, 0.5)
                        score = self.hyper_math.add(score, exploration_boost)
                elif "deep" in current_goal_description.lower():
                    if ".edu" in domain or "research" in domain:
                        depth_boost = self.hyper_math.mul(self.goal_relevance_weight, 0.6)
                        score = self.hyper_math.add(score, depth_boost)

            # Apply HyperMorphic personality factors
            if "blog" in url.lower() or "forum" in url.lower():
                curiosity_boost = self.hyper_math.mul(self.personality["curiosity"], 0.2)
                score = self.hyper_math.add(score, curiosity_boost)

            if "research" in url.lower() or "paper" in url.lower() or "edu" in domain:
                depth_boost = self.hyper_math.mul(self.personality["depth_preference"], 0.3)
                score = self.hyper_math.add(score, depth_boost)

            if random.random() < self.hyper_math.zero_free(self.personality["risk_taking"]):
                risk_boost = self.hyper_math.zero_free(random.uniform(0, 0.5))
                score = self.hyper_math.add(score, risk_boost)

            # Add memory importance if this URL is known
            if url in self.memory_importance:
                memory_boost = self.hyper_math.mul(self.memory_importance[url], 0.4)
                score = self.hyper_math.add(score, memory_boost)

            # Store score with HyperMorphic guarantee
            url_scores[url] = self.hyper_math.zero_free(score)

        # Quantum leap decision with HyperMorphic probability
        quantum_leap_threshold = self.hyper_math.mul(self.quantum_influence_weight, 0.3)
        if self.hyper_math.zero_free(random.random()) < quantum_leap_threshold:
            # Complete quantum randomness - ignore calculated scores
            quantum_choice = random.choice(candidate_urls)
            log_event(f"Made HyperMorphic quantum leap URL choice: {quantum_choice}", "QUANTUM")
            self.agent.stats["last_url"] = quantum_choice
            return quantum_choice

        # Normal selection based on scores using HyperMorphic comparison
        if url_scores:
            try:
                # Find highest score using HyperMorphic comparison
                best_url = max(url_scores.items(), key=lambda x: x[1])[0]
                log_event(f"Selected URL: {best_url} with HyperMorphic score: {url_scores[best_url]:.2f}", "INFO")
                self.agent.stats["last_url"] = best_url
                return best_url
            except Exception as e:
                log_event(f"Error selecting best URL: {e}", "ERROR")
                fallback = random.choice(candidate_urls)
                log_event(f"Using fallback URL selection: {fallback}", "WARNING")
                self.agent.stats["last_url"] = fallback
                return fallback
        else:
            # No scores calculated - use random selection
            fallback = random.choice(candidate_urls)
            log_event(f"No URL scores available. Using random selection: {fallback}", "WARNING")
            self.agent.stats["last_url"] = fallback
            return fallback

    def discover_links(self, html_content, base_url):
        """Discover and extract links from HTML content with HyperMorphic filtering"""
        links, details = enhanced_link_discovery(html_content, base_url)

        # Filter for high-quality links with HyperMorphic threshold
        if details:
            threshold = self.hyper_math.zero_free(0.6)  # Zero-free quality threshold
            high_quality_links = [link_data['url'] for link_data in details
                               if self.hyper_math.zero_free(link_data['quality_score']) > threshold]

            if high_quality_links:
                log_event(f"Discovered {len(high_quality_links)} high-quality links (quality > {threshold})", "INFO")

                # Add to memory with HyperMorphic state tracking
                self.expand_memory(high_quality_links)

                return len(high_quality_links)

        # Add all discovered links to memory
        self.expand_memory(links)

        log_event(f"Discovered {len(links)} links (using basic quality filter)", "INFO")
        return len(links)

    def store_semantic_content(self, url, content):
        """Store content with semantic encoding using HyperMorphic representational spaces"""
        # Create semantic memory module if needed
        semantic_module = SemanticMemoryModule()
        semantic_module.store_semantic_content(url, content)

        # Store in this instance's semantic memory with HyperMorphic guarantees
        self.semantic_memory[url] = self.hyper_math.zero_free(
            semantic_module.semantic_memory.get(url, {})
        )

    def expand_memory(self, urls):
        """Expand memory set with HyperMorphic tracking"""
        original_size = len(self.memory_set)

        # Add new URLs to memory set
        for url in urls:
            self.memory_set.add(url)

            # Initialize memory importance if not present
            if url not in self.memory_importance:
                # Set initial importance with slight randomness and zero-free guarantee
                initial_importance = self.hyper_math.zero_free(0.5 + random.uniform(-0.1, 0.1))
                self.memory_importance[url] = initial_importance

        # Calculate new memory size with HyperMorphic operations
        new_size = len(self.memory_set)
        added = new_size - original_size

        if added > 0:
            log_event(f"Memory expanded by {added} URLs, total size: {new_size}", "INFO")

        # Apply HyperMorphic entropy optimization if memory exceeds threshold
        if new_size > MEMORY_MAX_SIZE * 0.9:
            self._optimize_memory_entropy()

    def _optimize_memory_entropy(self):
        """Optimize memory entropy using HyperMorphic principles"""
        # Target size is 80% of max to leave room for new memories
        target_size = int(MEMORY_MAX_SIZE * 0.8)

        if len(self.memory_set) <= target_size:
            return

        log_event(f"Performing HyperMorphic memory entropy optimization", "INFO")

        # Sort URLs by importance with HyperMorphic comparisons
        sorted_urls = sorted([(url, self.memory_importance.get(url, self.hyper_math.zero_free(0.5)))
                           for url in self.memory_set],
                           key=lambda x: x[1])

        # Remove lowest importance URLs
        urls_to_remove = sorted_urls[:len(self.memory_set) - target_size]

        for url, importance in urls_to_remove:
            self.memory_set.remove(url)
            # Keep importance record for potential future reference

        log_event(f"Removed {len(urls_to_remove)} low-importance URLs from memory", "INFO")

    def decide(self):
        """
        Make a decision about what action to take next using
        HyperMorphic zero-free quantum-inspired decision making
        """
        try:
            log_event("Making HyperMorphic quantum-enhanced decision...", "QUANTUM")

            # Possible actions
            possible_actions = ["search", "expand", "adapt", "reconnect", "evaluate", "quantum_leap"]

            # Initialize HyperMorphic quantum state if not initialized
            # Each action has amplitude and phase with zero-free guarantees
            if not self.quantum_state or len(self.quantum_state) != len(possible_actions):
                self.quantum_state = {}
                for action in possible_actions:
                    phase = random.uniform(0, 2 * math.pi)
                    amplitude = self.hyper_math.zero_free(random.uniform(0.3, 1.0))
                    self.quantum_state[action] = {"amplitude": amplitude, "phase": phase}

            # Update quantum states based on history using HyperMorphic evolution
            self._evolve_quantum_states()

            # Get current context
            current_goal_description = self._get_active_goal_description()

            # Get consciousness awareness level with zero-free guarantee
            awareness_level = self.hyper_math.zero_free(0.5)  # Default
            if self.consciousness_link:
                awareness_level = self.hyper_math.zero_free(self.consciousness_link.awareness_level)

            # Get thinking mode if available
            thinking_mode = "balanced"
            if hasattr(self.agent, "ai_manager") and hasattr(self.agent.ai_manager, "autonomous_mind"):
                thinking_mode = getattr(self.agent.ai_manager.autonomous_mind, "current_mode", "balanced")

            # Apply HyperMorphic quantum interference based on context

            # 1. Goal-based interference with zero-free amplitude modulation
            if current_goal_description:
                if "explore" in current_goal_description.lower():
                    # Amplify exploration actions
                    self._amplify_action("expand", self.hyper_math.zero_free(1.3))
                    self._amplify_action("search", self.hyper_math.zero_free(1.2))
                elif "deep" in current_goal_description.lower():
                    # Amplify deepening actions
                    self._amplify_action("evaluate", self.hyper_math.zero_free(1.4))
                    self._amplify_action("adapt", self.hyper_math.zero_free(1.2))

            # 2. Thinking mode interference
            if thinking_mode == "analytical":
                self._amplify_action("evaluate", self.hyper_math.zero_free(1.3))
                self._amplify_action("search", self.hyper_math.zero_free(1.2))
            elif thinking_mode == "creative":
                self._amplify_action("quantum_leap", self.hyper_math.zero_free(1.5))
                self._amplify_action("expand", self.hyper_math.zero_free(1.2))
            elif thinking_mode == "critical":
                self._amplify_action("adapt", self.hyper_math.zero_free(1.3))
                self._amplify_action("evaluate", self.hyper_math.zero_free(1.2))

            # 3. Recent actions interference - safely extract from action log
            recent_actions = []
            if hasattr(self.agent, 'action_log') and self.agent.action_log:
                if len(self.agent.action_log) >= 3:
                    recent_actions = [a.get("action", "") for a in list(self.agent.action_log)[-3:]]

            if recent_actions:
                # Avoid repeating the same action too many times using HyperMorphic anti-resonance
                action_counts = {}
                for a in recent_actions:
                    action_counts[a] = action_counts.get(a, 0) + 1

                most_common = max(action_counts.items(), key=lambda x: x[1])[0] if action_counts else None

                if most_common in self.quantum_state:
                    # Reduce amplitude of most common action to encourage diversity
                    self._amplify_action(most_common, self.hyper_math.zero_free(0.7))

            # 4. Success-based amplification
            successful_actions = []
            if hasattr(self.agent, 'action_log') and self.agent.action_log:
                if len(self.agent.action_log) >= 5:
                    successful_actions = [a.get("action", "") for a in list(self.agent.action_log)[-5:]
                                         if a.get("content_length", 0) > 1000]

            if successful_actions:
                for action in successful_actions:
                    if action in self.quantum_state:
                        # Amplify successful actions
                        self._amplify_action(action, self.hyper_math.zero_free(1.1))

            # Apply consciousness as quantum observer effect with HyperMorphic operations
            # Higher consciousness makes decision more deterministic
            for action in possible_actions:
                if action in self.quantum_state:
                    # Calculate randomness factor with zero-free operations
                    random_factor = self.hyper_math.mul(
                        self.hyper_math.zero_free(random.uniform(0.8, 1.2)),
                        self.hyper_math.sub(1.0, awareness_level)
                    )

                    # Apply small random fluctuation scaled by randomness factor
                    fluctuation = self.hyper_math.mul(random_factor, self.hyper_math.zero_free(0.2))

                    # Apply to amplitude
                    current_amplitude = self.quantum_state[action]["amplitude"]
                    self.quantum_state[action]["amplitude"] = self.hyper_math.mul(
                        current_amplitude,
                        self.hyper_math.add(1.0, fluctuation)
                    )

            # Calculate probabilities with HyperMorphic operations
            # Use amplitude squared for quantum probability
            probabilities = {}
            for action, state in self.quantum_state.items():
                # Calculate amplitude squared with zero-free guarantee
                amplitude = state["amplitude"]
                probability = self.hyper_math.mul(amplitude, amplitude)
                probabilities[action] = probability

            # Normalize probabilities with HyperMorphic division
            total_probability = sum(probabilities.values())
            normalized_probabilities = {}
            for action, probability in probabilities.items():
                normalized_probabilities[action] = self.hyper_math.div(probability, total_probability)

            # Collapse the quantum state by observation
            # This uses the normalized probabilities to select an action
            actions = list(normalized_probabilities.keys())
            weights = list(normalized_probabilities.values())
            action_type = random.choices(actions, weights=weights, k=1)[0]

            # Record the decision with HyperMorphic metadata
            confidence = normalized_probabilities[action_type]
            decision = {
                "action": action_type,
                "quantum_confidence": float(confidence),  # Convert to float for serialization
                "reasoning": f"HyperMorphic quantum decision process ({thinking_mode} mode) - goal: {current_goal_description[:30]}",
                "timestamp": datetime.now().isoformat()
            }

            # Store in decision history
            self.decision_history.append(decision)

            # Trim history if needed
            if len(self.decision_history) > 50:
                self.decision_history = self.decision_history[-50:]

            log_event(f"HyperMorphic decision: {action_type} with {confidence:.2f} quantum confidence", "QUANTUM")

            # Special handling for quantum_leap action
            if action_type == "quantum_leap":
                # Map to a standard action but with more randomness
                standard_actions = ["search", "expand", "adapt", "reconnect", "evaluate"]
                decision["action"] = random.choice(standard_actions)
                decision["quantum_leap"] = True
                log_event(f"HyperMorphic quantum leap mapped to {decision['action']}", "QUANTUM")

            return decision

        except Exception as e:
            log_event(f"HyperMorphic decision error: {e}", "ERROR")
            # Fallback to simple random choice
            fallback_action = random.choice(["expand", "search", "adapt"])
            return {"action": fallback_action, "error": str(e)[:200]}

    def _evolve_quantum_states(self):
        """Evolve quantum states based on history using HyperMorphic principles"""
        # Small phase evolution for all states
        for action, state in self.quantum_state.items():
            # Evolve phase with small random shift
            current_phase = state["phase"]
            phase_shift = random.uniform(-0.1, 0.1)
            new_phase = (current_phase + phase_shift) % (2 * math.pi)

            # Update phase
            self.quantum_state[action]["phase"] = new_phase

    def _amplify_action(self, action, factor):
        """Amplify action amplitude by factor using HyperMorphic multiplication"""
        if action in self.quantum_state:
            current_amplitude = self.quantum_state[action]["amplitude"]
            new_amplitude = self.hyper_math.mul(current_amplitude, factor)

            # Ensure amplitude stays within reasonable bounds
            max_amplitude = 2.0  # Maximum allowed amplitude
            if new_amplitude > max_amplitude:
                new_amplitude = max_amplitude

            # Update amplitude with zero-free guarantee
            self.quantum_state[action]["amplitude"] = self.hyper_math.zero_free(new_amplitude)

    def get_decision_analytics(self):
        """Get analytics on recent decisions using HyperMorphic statistics"""
        if not self.decision_history:
            return {"status": "insufficient_data"}

        # Analyze action distribution
        action_counts = {}
        for decision in self.decision_history:
            action = decision.get("action", "unknown")
            action_counts[action] = action_counts.get(action, 0) + 1

        # Calculate action probabilities with HyperMorphic division
        total_decisions = len(self.decision_history)
        action_probabilities = {}
        for action, count in action_counts.items():
            action_probabilities[action] = self.hyper_math.div(count, total_decisions)

        # Calculate entropy using HyperMorphic operations
        entropy = self.hyper_math.zero_free(0)
        for prob in action_probabilities.values():
            # Skip zero probabilities (though we should never have exact zeros)
            if prob <= self.hyper_math.epsilon:
                continue

            # Add -p*log(p) to entropy
            log_prob = math.log(prob)
            term = self.hyper_math.mul(prob, self.hyper_math.zero_free(-log_prob))
            entropy = self.hyper_math.add(entropy, term)

        # Max possible entropy given action count
        action_count = len(action_counts)
        max_entropy = self.hyper_math.zero_free(math.log(max(1, action_count)))

        # Normalized entropy (0-1 scale)
        normalized_entropy = self.hyper_math.zero_free(0.5)  # Default
        if max_entropy > self.hyper_math.epsilon:
            normalized_entropy = self.hyper_math.div(entropy, max_entropy)

        return {
            "action_distribution": {k: float(v) for k, v in action_probabilities.items()},
            "decision_entropy": float(normalized_entropy),
            "quantum_confidence_avg": sum(d.get("quantum_confidence", 0.5) for d in self.decision_history) / total_decisions,
            "quantum_leap_frequency": sum(1 for d in self.decision_history if d.get("quantum_leap", False)) / total_decisions,
            "decision_count": total_decisions
        }


# =============================================================================
# AI MANAGEMENT SYSTEM
# =============================================================================
# =============================================================================
# AI MANAGEMENT SYSTEM
# =============================================================================
class AIManager:
    """
    Central management system for the autonomous agent, coordinating
    decision-making, planning, and evolution.
    """
    def __init__(self, agent, model):
        self.agent = agent
        self.model = model

        # Core subsystems
        self.temporal_planner = TemporalPlanner()
        self.autonomous_mind = AutonomousMind(agent, model)
        self.consciousness = ConsciousnessModule(agent)
        self.imagination = ImaginationEngine()

        # Self-improvement systems
        self.meta_learning = MetaLearningModule(model)
        self.evolution_engine = HyperMorphicMetaEvolutionEngine()


        # Operational tracking
        self.cycle_counter = 0
        self.last_evolution_attempt = 0
        self.evolution_interval = 50
        self.error_recovery_attempts = 0

        # Initialize subsystems
        self.temporal_planner.initialize_goals()

        # Connect subsystems
        if hasattr(self.agent, "free_will") and hasattr(self.agent.free_will, "link_consciousness"):
            self.agent.free_will.link_consciousness(self.consciousness)

        log_event("AIManager initialized with all autonomous systems", "INFO")


    async def run_cycle(self, optimizer=None):
        """Run a complete autonomous cycle with enhanced error handling"""
        self.cycle_counter += 1
        log_event(f"=== Enhanced Autonomous Cycle {self.cycle_counter} ===", "INFO")

        try:
            # 1. Perception - get environment state
            try:
                observation = self.agent.perceive()
            except Exception as e:
                log_event(f"Error in perception phase: {str(e)}", "ERROR")
                observation = {"error": str(e)}  # Minimal fallback observation

            # 2. Consciousness reflection with error handling
            try:
                if hasattr(self, 'consciousness'):
                    self.consciousness.reflect(observation)
            except Exception as e:
                log_event(f"Error in consciousness reflection: {str(e)}", "ERROR")
                # Continue even if reflection fails

            # 3. Decision making with fallbacks
            base_decision = {"action": "expand"}  # Default fallback
            try:
                if hasattr(self.agent, "free_will") and hasattr(self.agent.free_will, "decide"):
                    decision = self.agent.free_will.decide()
                    if isinstance(decision, dict) and "action" in decision:
                        base_decision = decision
            except Exception as e:
                log_event(f"Decision error: {str(e)}. Using fallback decision.", "ERROR")

            # 4. Planning with error handling
            try:
                if hasattr(self, 'temporal_planner') and self.temporal_planner:
                    full_plan = self.temporal_planner.plan_action(
                        base_decision.get("action", "expand"),
                        observation
                    )
                else:
                    # Fallback simple plan if no temporal planner
                    full_plan = {
                        "action": base_decision.get("action", "expand"),
                        "strategy": "fallback_strategy",
                        "goal": "Continue system operation"
                    }
            except Exception as e:
                log_event(f"Planning error: {str(e)}. Using simplified plan.", "ERROR")
                # Create minimal plan on failure
                full_plan = {
                    "action": base_decision.get("action", "expand"),
                    "strategy": "emergency_strategy",
                    "goal": "Recover from planning failure"
                }

            # 5. Imagination - simulate outcomes (non-critical)
            try:
                if hasattr(self, 'imagination') and random.random() < 0.2:
                    self.imagination.simulate_creation()
            except Exception as e:
                log_event(f"Non-critical error in imagination: {str(e)}", "WARNING")

            # 6. Execute action with timeout protection
            try:
                # Set a timeout for execution to prevent hanging
                action_task = asyncio.create_task(
                    asyncio.to_thread(self.agent.act, full_plan, optimizer)
                )
                action_successful = await asyncio.wait_for(action_task, timeout=60.0)
            except asyncio.TimeoutError:
                log_event("Action execution timed out after 60 seconds", "ERROR")
                action_successful = False
            except Exception as e:
                log_event(f"Action execution error: {str(e)}", "ERROR")
                action_successful = False

            # After action execution is successful:
            if action_successful and hasattr(self.agent, 'planner_sifter'):
                strategy_name = full_plan.get("strategy", "exploration")
                result_data = {
                    "content_length": self.agent.action_log[-1].get("content_length", 0),
                    "links_discovered": self.agent.action_log[-1].get("links_discovered", 0),
                    "success": action_successful
                }
                self.agent.planner_sifter.update_strategy_effectiveness(strategy_name, result_data)

            # 7. Performance assessment
            performance_metrics = {
                "success": action_successful,
                "content_length": self.agent.action_log[-1].get("content_length", 0) if self.agent.action_log else 0,
                "links_discovered": self.agent.action_log[-1].get("links_discovered", 0) if self.agent.action_log else 0,
                "cycle": self.cycle_counter
            }

            # 8. Reflection and adaptation (non-critical)
            try:
                if hasattr(self, 'temporal_planner'):
                    self.temporal_planner.reflect_and_adapt(performance_metrics)
            except Exception as e:
                log_event(f"Non-critical error in reflection: {str(e)}", "WARNING")

            # 9. Error detection (non-critical)
            try:
                if hasattr(self, 'imagination'):
                    error_details = self.imagination.simulate_error_detection()
                    if error_details:
                        # Apply correction
                        self.imagination.simulate_error_correction(error_details)
            except Exception as e:
                log_event(f"Non-critical error in error detection: {str(e)}", "WARNING")

            # 10. Self-evolution at intervals (non-critical)
            try:
                if (hasattr(self, 'evolution_engine') and
                    self.cycle_counter - getattr(self, 'last_evolution_attempt', 0) >= getattr(self, 'evolution_interval', 50)):
                    log_event("Attempting system evolution...", "INFO")
                    result, message = self.evolution_engine.evolve_system(self.agent)
                    self.last_evolution_attempt = self.cycle_counter
                    log_event(f"Evolution attempt result: {result} - {message}", "INFO")
            except Exception as e:
                log_event(f"Non-critical error in evolution: {str(e)}", "WARNING")

            # 11. Self-refinement (with error handling)
            try:
                self.agent.refine()
            except Exception as e:
                log_event(f"Error in agent refinement: {str(e)}", "ERROR")
                # Don't let refinement errors abort the cycle

            # Reset error recovery counter on success
            self.error_recovery_attempts = 0

            return {
                "cycle": self.cycle_counter,
                "action": full_plan.get("action", "unknown"),
                "strategy": full_plan.get("strategy", "none"),
                "success": action_successful
            }

        except Exception as e:
            # Error recovery
            self.error_recovery_attempts += 1
            log_event(f"CYCLE ERROR: {str(e)}", "ERROR")
            log_event(traceback.format_exc(), "ERROR")

            # Implement progressive recovery strategies
            if self.error_recovery_attempts < 3:
                log_event("Attempting standard error recovery", "WARNING")
            elif self.error_recovery_attempts < 5:
                log_event("Attempting advanced error recovery - resetting system state", "WARNING")
                # Reset consciousness state if available
                if hasattr(self, 'consciousness'):
                    self.consciousness.awareness_level = 0.5
                    self.consciousness.current_state = "balanced"
            else:
                log_event("Critical error threshold reached - emergency recovery", "CRITICAL")
                # Emergency reset of all systems
                if hasattr(self, 'consciousness'):
                    self.consciousness = ConsciousnessModule(self.agent)
                if hasattr(self, 'temporal_planner'):
                    self.temporal_planner.cycle_count = 0
                    self.temporal_planner.refresh_short_term_goals()

            return {
                "status": "error",
                "cycle": self.cycle_counter,
                "error": str(e),
                "recovery_attempt": self.error_recovery_attempts
            }



# =============================================================================
# TEMPORAL PLANNING AND GOAL MANAGEMENT
# =============================================================================
class TemporalPlanner:
    """
    Advanced planning system that operates across multiple time horizons
    and manages goals with temporal dependencies.
    """
    def __init__(self):
        self.short_term_goals = []
        self.long_term_goals = []
        self.goal_history = []
        self.current_strategy = None
        self.strategy_effectiveness = {}
        self.time_horizon_days = 7
        self.reflection_interval = 20
        self.cycle_count = 0

        # Add strategy mapping to match PlannerSifter strategy names
        self.strategy_mapping = {
            "exploration": "broad_exploration",
            "deepening": "depth_first",
            "integration": "connect_domains",
            "evaluation": "evaluate_sources",
            "quantum": "quantum_reasoning",
            "creative": "creative_synthesis"
        }

    def initialize_goals(self):
        """Set up initial goal structure"""
        self.long_term_goals = [
            {
                "id": "knowledge_diversity",
                "description": "Maximize diversity of knowledge domains",
                "priority": 0.8,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": "model_efficiency",
                "description": "Optimize neural architecture for learning efficiency",
                "priority": 0.7,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": "content_quality",
                "description": "Improve filtering and processing of high-value content",
                "priority": 0.9,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": "quantum_reasoning",
                "description": "Develop quantum-inspired reasoning capabilities",
                "priority": 1.0,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            }
        ]
        self.refresh_short_term_goals()
        log_event("Temporal planner initialized with long-term goals", "INFO")

    def refresh_short_term_goals(self):
        """Generate new short-term goals aligned with long-term objectives"""
        # Save the existing goals that are still valid
        valid_goals = []
        for goal in self.short_term_goals:
            if goal.get("duration", 0) > 0:
                valid_goals.append(goal)

        # Clear the short_term_goals list
        self.short_term_goals = valid_goals

        # Check if we need to generate new goals
        if len(self.short_term_goals) >= 5:
            return  # We still have enough goals

        # Define knowledge domains
        domains = [
            "technical", "scientific", "humanities", "news",
            "reference", "creative", "analytical", "philosophical"
        ]

        # Generate 3-5 new short term goals
        goals_to_generate = min(5, 8 - len(self.short_term_goals))
        for _ in range(goals_to_generate):
            # Randomly select goal type and domain
            goal_type = random.choice(["exploration", "deepening", "integration", "refinement"])
            domain = random.choice(domains)

            # Generate goal based on type
            if goal_type == "exploration":
                goal = {
                    "id": f"explore_{domain}_{int(time.time())}",
                    "description": f"Discover new content sources in {domain}",
                    "priority": random.uniform(0.5, 0.9),
                    "duration": random.randint(10, 30),
                    "type": "exploration",
                    "domain": domain,
                    "created": datetime.now().isoformat()
                }
            elif goal_type == "deepening":
                goal = {
                    "id": f"deepen_{domain}_{int(time.time())}",
                    "description": f"Build deeper understanding in {domain}",
                    "priority": random.uniform(0.6, 0.95),
                    "duration": random.randint(5, 15),
                    "type": "deepening",
                    "domain": domain,
                    "created": datetime.now().isoformat()
                }
            elif goal_type == "integration":
                domain2 = random.choice([d for d in domains if d != domain])
                goal = {
                    "id": f"integrate_{domain}_{domain2}_{int(time.time())}",
                    "description": f"Connect knowledge between {domain} and {domain2}",
                    "priority": random.uniform(0.7, 0.9),
                    "duration": random.randint(8, 20),
                    "type": "integration",
                    "domains": [domain, domain2],
                    "created": datetime.now().isoformat()
                }
            else:  # refinement
                goal = {
                    "id": f"refine_{domain}_{int(time.time())}",
                    "description": f"Optimize learning approach in {domain}",
                    "priority": random.uniform(0.5, 0.8),
                    "duration": random.randint(5, 12),
                    "type": "refinement",
                    "domain": domain,
                    "created": datetime.now().isoformat()
                }

            self.short_term_goals.append(goal)

        log_event(f"Refreshed short-term goals: {goals_to_generate} new goals created", "INFO")

    def select_active_goal(self):
        """Select the highest priority current goal"""
        # Remove expired goals
        active_goals = [g for g in self.short_term_goals if g.get("duration", 0) > 0]
        self.short_term_goals = active_goals

        # Decrease duration for all goals
        for goal in self.short_term_goals:
            goal["duration"] = max(0, goal.get("duration", 10) - 1)

        # Refresh if needed
        if not self.short_term_goals:
            self.refresh_short_term_goals()

        # Select highest priority goal
        if self.short_term_goals:
            return max(self.short_term_goals, key=lambda x: x.get("priority", 0))
        else:
            # Default goal if something went wrong
            default_goal = {
                "id": "default_exploration",
                "description": "Default exploration",
                "priority": 0.5,
                "type": "exploration",
                "domain": "reference",  # Provide a default domain
                "duration": 5  # Give it some duration
            }
            self.short_term_goals.append(default_goal)  # Add to goals list for tracking
            return default_goal

    def reflect_and_adapt(self, performance_metrics):
        """Periodically reflect on goal progress and adapt strategies"""
        self.cycle_count += 1

        # Update goal progress based on performance
        if performance_metrics.get("success", False):
            active_goal = self.select_active_goal()

            # Find corresponding long-term goal to update
            for goal in self.long_term_goals:
                # Update based on goal type alignment
                if (active_goal.get("type") == "exploration" and goal["id"] == "knowledge_diversity") or \
                   (active_goal.get("type") == "deepening" and goal["id"] == "content_quality") or \
                   (active_goal.get("type") == "refinement" and goal["id"] == "model_efficiency") or \
                   (active_goal.get("type") == "integration" and goal["id"] == "quantum_reasoning"):
                    # Small progress increment
                    increment = min(0.05, performance_metrics.get("content_length", 0) / 20000)
                    goal["progress"] = min(1.0, goal["progress"] + increment)

            # Record the strategy effectiveness
            strategy = self.current_strategy
            if strategy:
                if strategy not in self.strategy_effectiveness:
                    self.strategy_effectiveness[strategy] = []

                # Score based on content and links
                score = min(1.0, performance_metrics.get("content_length", 0) / 5000 +
                           performance_metrics.get("links_discovered", 0) / 10)
                self.strategy_effectiveness[strategy].append(score)

                # Limit history size
                if len(self.strategy_effectiveness[strategy]) > 20:
                    self.strategy_effectiveness[strategy] = self.strategy_effectiveness[strategy][-20:]

        # Major reflection at intervals
        if self.cycle_count % self.reflection_interval == 0:
            log_event("Performing strategic reflection and adaptation...", "INFO")

            # Analyze strategy effectiveness
            for strategy, metrics in self.strategy_effectiveness.items():
                if metrics:
                    avg_performance = sum(metrics) / len(metrics)
                    log_event(f"Strategy '{strategy}' average performance: {avg_performance:.4f}", "INFO")

            # Adjust long-term goal priorities
            total_adjustment = 0
            for goal in self.long_term_goals:
                # Random adjustment with bias toward less-progressed goals
                bias = 1.0 - goal.get("progress", 0)
                adjustment = random.uniform(-0.1, 0.15) * bias

                goal["priority"] = max(0.1, min(1.0, goal["priority"] + adjustment))
                total_adjustment += abs(adjustment)

            # Sometimes create new evolved goals
            if random.random() < 0.2:
                # Create a new long-term goal
                new_goal_types = [
                    "Develop cognitive synergy across domains",
                    "Optimize information integration pathways",
                    "Enhance quantum processing capabilities",
                    "Improve anomaly detection in knowledge structures",
                    "Develop adaptive learning mechanisms"
                ]

                new_goal_id = f"evolved_goal_{int(time.time())}"
                new_goal = {
                    "id": new_goal_id,
                    "description": f"Evolved objective: {random.choice(new_goal_types)}",
                    "priority": random.uniform(0.7, 0.9),
                    "progress": 0.0,
                    "created": datetime.now().isoformat()
                }

                # Limit total goals
                if len(self.long_term_goals) < 10:  # Prevent too many goals
                    self.long_term_goals.append(new_goal)
                    log_event(f"Created new long-term goal: {new_goal['description']}", "INFO")

            # Clean expired goals from short-term list
            self.short_term_goals = [g for g in self.short_term_goals if g.get("duration", 0) > 0]

            # Refresh short-term goals
            self.refresh_short_term_goals()

            log_event(f"Reflection complete: adjusted {len(self.long_term_goals)} long-term goals (total Δ: {total_adjustment:.4f})", "INFO")

    def plan_action(self, base_action, environment_state=None):
        """Generate temporal plan based on goals and environment"""
        # Get current active goal
        active_goal = self.select_active_goal()

        # Consider temporal context
        current_time = datetime.now()
        is_weekend = current_time.weekday() >= 5
        is_business_hours = 9 <= current_time.hour <= 17

        # Available strategies with weights
        strategy_options = [
            "broad_exploration",     # Wide but shallow exploration
            "depth_first",           # Deep dive into specific domain
            "connect_domains",       # Look for connections between areas
            "evaluate_sources",      # Focus on quality assessment
            "quantum_reasoning",     # Use quantum processing modes
            "creative_synthesis"     # Generate new insights
        ]

        # Default weights
        weights = [0.2, 0.2, 0.2, 0.15, 0.15, 0.1]

        # Adjust weights based on temporal context
        if is_business_hours and not is_weekend:
            # Business hours - more analytical
            weights = [0.1, 0.2, 0.2, 0.3, 0.1, 0.1]
        elif not is_business_hours:
            # Non-business hours - more exploratory
            weights = [0.3, 0.1, 0.1, 0.1, 0.2, 0.2]

        # Adjust based on goal type
        goal_type = active_goal.get("type", "")
        if goal_type == "exploration":
            # Boost exploration strategies
            weights[0] += 0.2  # More broad_exploration
            weights[5] += 0.1  # More creative_synthesis
        elif goal_type == "deepening":
            # Boost deepening strategies
            weights[1] += 0.2  # More depth_first
            weights[3] += 0.1  # More evaluate_sources
        elif goal_type == "integration":
            # Boost connection strategies
            weights[2] += 0.2  # More connect_domains
            weights[5] += 0.1  # More creative_synthesis
        elif goal_type == "refinement":
            # Boost refinement strategies
            weights[3] += 0.2  # More evaluate_sources
            weights[4] += 0.1  # More quantum_reasoning

        # Ensure weights sum to 1
        total = sum(weights)
        weights = [w/total for w in weights]

        # Select strategy
        self.current_strategy = random.choices(strategy_options, weights=weights, k=1)[0]

        # Create plan
        timestamp = current_time.isoformat()
        plan = {
            "action": base_action,
            "goal": active_goal["description"],
            "strategy": self.current_strategy,
            "timestamp": timestamp,
            "execution_context": {
                "is_weekend": is_weekend,
                "is_business_hours": is_business_hours,
                "current_hour": current_time.hour,
                "goal_type": goal_type,
                "goal_domain": active_goal.get("domain", "unknown")
            }
        }

        log_event(f"Generated temporal plan: {plan['action']} using {plan['strategy']} strategy for goal: {plan['goal']}", "INFO")
        return plan

    def _convert_old_strategy_name(self, old_name):
        """Convert old strategy names to new format if needed"""
        if old_name in self.strategy_mapping:
            return self.strategy_mapping[old_name]
        return old_name



# =============================================================================
# QUANTUM NEURAL ARCHITECTURE
# =============================================================================
class QuantumAttentionLayer(nn.Module):
    """
    Implements quantum-inspired attention with superposition of states
    that allows multiple attention pathways to exist simultaneously.
    """
    def __init__(self, embed_dim, num_heads=4, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Multi-dimensional quantum projection spaces
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.o_proj = nn.Linear(embed_dim, embed_dim)

        # Phase shifters for quantum interference
        self.phase_shifts = nn.Parameter(torch.rand(num_heads) * 2 * math.pi)

        # Entanglement mixing for cross-attention effects
        self.entanglement_gate = nn.Linear(embed_dim, embed_dim)

        self.dropout = nn.Dropout(dropout)
        self.attention_weights = None  # Store for visualization

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # Project inputs to queries, keys, values
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # Transpose for attention computation
        q = q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # Apply phase shifts for quantum effects
        for h in range(self.num_heads):
            phase = self.phase_shifts[h]
            q[:, h] = q[:, h] * torch.cos(phase) + q[:, h] * torch.sin(phase)
            k[:, h] = k[:, h] * torch.cos(phase) - k[:, h] * torch.sin(phase)

        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)

        # Apply softmax and get attention weights
        attn_weights = F.softmax(scores, dim=-1)
        self.attention_weights = attn_weights  # Save for visualization
        attn_weights = self.dropout(attn_weights)

        # Apply attention to values
        attn_output = torch.matmul(attn_weights, v)

        # Apply entanglement between heads for quantum correlation effects
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        entangled = self.entanglement_gate(attn_output)

        # Final output projection
        output = self.o_proj(entangled + attn_output)  # Residual connection

        return output

class HyperdimensionalEncoder(nn.Module):
    """
    Implements hyperdimensional computing principles for efficient
    high-dimensional representation of concepts.
    """
    def __init__(self, input_dim, hd_dim=1024):
        super().__init__()
        self.input_dim = input_dim
        self.hd_dim = hd_dim

        # Create random basis vectors
        self.register_buffer('basis', torch.randn(input_dim, hd_dim).sign())

        # Learnable projection
        self.projection = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        # Project input
        x_proj = self.projection(x)

        # Compute HD representation through binding and bundling
        batch_size = x_proj.shape[0]
        hd_vectors = torch.zeros(batch_size, self.hd_dim, device=x_proj.device)

        # Encode each dimension with element-wise multiplication (binding)
        for i in range(self.input_dim):
            # Scale by the input value
            scaled_basis = self.basis[i].unsqueeze(0) * x_proj[:, i].unsqueeze(1)
            # Add to the bundle (vector sum)
            hd_vectors += scaled_basis

        # Binarize to -1/+1 for clean HD representation
        hd_vectors = torch.sign(hd_vectors)

        return hd_vectors


# ---------------------------------------------------------------------------
# FractalLayer with adaptive depth
# ---------------------------------------------------------------------------
class FractalLayer(nn.Module):
    """
    HyperMorphic Advanced Fractal Processing Layer

    Implements recursive multi-scale processing with adaptive depth, dynamic temperature scaling,
    and residual gating for robust representation learning.

    Args:
        embed_dim (int): Dimensionality of input embeddings.
        max_recursion (int): Maximum recursion depth (default=4).
        adaptive_depth (bool): If True, adjusts recursion depth adaptively (default=False).
        use_layernorm (bool): Apply layer normalization on the output (default=True).
        dropout_rate (float): Dropout probability (default=0.1).
    """
    def __init__(self, embed_dim, max_recursion=4, adaptive_depth=False, use_layernorm=True, dropout_rate=0.1):
        super(FractalLayer, self).__init__()
        self.embed_dim = embed_dim
        self.max_recursion = max_recursion
        self.adaptive_depth = adaptive_depth
        self.use_layernorm = use_layernorm

        # Dynamic scaling factors; softplus ensures positivity.
        self.fractal_scale = nn.Parameter(torch.tensor(1.0))
        self.temperature = nn.Parameter(torch.tensor(1.0))

        # Base transformation for fractal iteration.
        self.linear = nn.Linear(embed_dim, embed_dim)
        self.residual_gate = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.Sigmoid()
        )
        self.dropout = nn.Dropout(dropout_rate)
        if self.use_layernorm:
            self.layernorm = nn.LayerNorm(embed_dim)

    def recursive_fractal(self, x, depth):
        if depth <= 0:
            return x
        # Ensure temperature is positive via softplus
        temp = F.softplus(self.temperature) + 1e-6

        # Apply linear transformation with dynamic temperature scaling
        scaled = self.linear(x) / temp
        fractal_out = torch.tanh(scaled)

        # Compute a residual gate to modulate the output
        gate = self.residual_gate(fractal_out)
        fractal_out = self.dropout(fractal_out)

        # Determine next recursion depth; if adaptive_depth is True, decrease faster.
        next_depth = depth - 1
        if self.adaptive_depth and depth >= 2:
            next_depth = depth - 2

        recursive_result = self.recursive_fractal(fractal_out, next_depth)

        # Combine input with gated recursive output scaled by fractal_scale.
        combined = x + self.fractal_scale * gate * recursive_result

        if self.use_layernorm:
            combined = self.layernorm(combined)
        return combined

    def forward(self, x):
        # Choose recursion depth based on adaptive_depth flag.
        recursion_depth = self.max_recursion * 2 if self.adaptive_depth else self.max_recursion
        return self.recursive_fractal(x, recursion_depth)


class QuantumResonanceTensor(nn.Module):
    """
    Implements non-collapsing recursive state resonance that maintains
    multiple simultaneous state representations in quantum-inspired superposition.
    """
    def __init__(self, embed_dim, num_states=4, resonance_factor=0.7):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_states = num_states
        self.resonance_factor = nn.Parameter(torch.tensor(resonance_factor))

        # Quantum state projectors
        self.state_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.SiLU(),
            ) for _ in range(num_states)
        ])

        # Phase shifters for quantum entanglement
        self.phase_shifters = nn.Parameter(torch.randn(num_states) * 0.1)

        # State mixer - allows controlled interference between states
        self.state_mixer = nn.Linear(embed_dim * num_states, embed_dim)

        # Recursive memory gates
        self.recursive_gate = nn.GRUCell(embed_dim, embed_dim)

        # Prior state memory (initialized during forward pass)
        self.register_buffer('state_memory', None, persistent=False)

    def forward(self, x, iteration_count=3):
        batch_size = x.shape[0]

        # Initialize state memory if needed
        if self.state_memory is None or self.state_memory.shape[0] != batch_size:
            self.state_memory = torch.zeros(batch_size, self.embed_dim, device=x.device)

        # Generate multiple quantum states
        quantum_states = []
        for i in range(self.num_states):
            # Apply phase shift for quantum effects
            phase = torch.cos(self.phase_shifters[i] * math.pi)
            # Project into this quantum state
            state_i = self.state_projectors[i](x) * phase
            quantum_states.append(state_i)

        # Recursive resonance iterations
        for _ in range(iteration_count):
            # Update state memory through recursive gate
            self.state_memory = self.recursive_gate(x, self.state_memory)

            # Apply resonance effect (controlled interference)
            resonance = self.state_memory * self.resonance_factor

            # Apply resonance to each quantum state (non-collapsing)
            for i in range(self.num_states):
                quantum_states[i] = quantum_states[i] + resonance * (0.1 * (i + 1))

        # Combine quantum states through superposition
        combined_states = torch.cat(quantum_states, dim=-1)
        output = self.state_mixer(combined_states)

        # Residual connection
        output = output + x

        return output

import torch
import torch.nn as nn

# ---------------------------------------------------------------------------
# NeocortexBlock using the adaptive FractalLayer
# ---------------------------------------------------------------------------
class NeocortexBlock(nn.Module):
    """
    HyperMorphic Neocortex Block – Fully adaptive, self-optimizing quantum-fractal-HD processor
    with resonant stream integration.

    This block routes information through an attention mechanism, processes data
    through parallel fractal and quantum streams, encodes high-dimensional representations,
    and then integrates these streams with a residual update.

    Args:
        embed_dim (int): Dimensionality of input embeddings.
        dynamic_states (bool): Flag to set adaptive quantum state depth (default=True).
    """
    def __init__(self, embed_dim, dynamic_states=True):
        super(NeocortexBlock, self).__init__()

        # Adaptive quantum state depth calculation.
        self.num_quantum_states = embed_dim // 16 if dynamic_states else 4

        # Information routing via a quantum-inspired attention layer.
        self.attention = QuantumAttentionLayer(embed_dim)

        # Parallel processing streams:
        # Use the adaptive FractalLayer with adaptive_depth=True.
        self.fractal_stream = FractalLayer(embed_dim, adaptive_depth=True)
        self.quantum_stream = QuantumResonanceTensor(embed_dim, num_states=self.num_quantum_states)

        # Hyperdimensional representation encoding.
        self.hd_encoder = HyperdimensionalEncoder(embed_dim, hd_dim=embed_dim)

        # Nonlinear integration of the streams.
        self.integration = nn.Linear(embed_dim * 3, embed_dim)
        self.norm = HyperMorphicNormalization(embed_dim)

    def forward(self, x):
        # Process input with the attention mechanism.
        attended = self.attention(x)

        # Process the attended input through the fractal and quantum streams.
        fractal_out = self.fractal_stream(attended)
        quantum_out = self.quantum_stream(attended)

        # Compute high-dimensional encoding.
        batch_size, seq_len, _ = attended.shape
        hd_out = self.hd_encoder(attended.view(-1, attended.size(-1))).view(batch_size, seq_len, -1)

        # Integrate the outputs from the three streams.
        combined = torch.cat([fractal_out, quantum_out, hd_out], dim=-1)
        integrated = self.integration(combined)

        # Apply a hypermorphic normalization with a residual connection.
        output = self.norm(x + integrated)
        return output



class AdaptiveLearningSystem:
    """
    Advanced system for dynamically adapting learning parameters and network architecture
    based on performance metrics and environmental feedback.
    """
    def __init__(self, model):
        self.model = model
        self.learning_rate_history = []
        self.performance_metrics = []
        self.architecture_changes = []
        self.adaptation_cycle = 0
        self.min_learning_rate = 1e-6
        self.max_learning_rate = 1e-3
        self.performance_window_size = 10
        self.exploration_rate = 0.3
        self.adaptation_threshold = 0.15
        self.architecture_expansion_threshold = 5

        # Initialize default learning rate on model
        self.default_learning_rate = 5e-5  # Same as initial LEARNING_RATE
        setattr(self.model, '_current_lr', self.default_learning_rate)

        log_event("AdaptiveLearningSystem initialized with dynamic adaptation capabilities", "INFO")

    def adapt_learning_rate(self, metrics):
        """
        Dynamically adjust learning rate based on recent performance metrics
        using a sophisticated control system approach.
        """
        self.adaptation_cycle += 1

        # Get current learning rate with fallback to default
        current_lr = getattr(self.model, '_current_lr', self.default_learning_rate)
        self.learning_rate_history.append(current_lr)

        # Record performance metrics
        if isinstance(metrics, dict):
            self.performance_metrics.append(metrics)

        # Need sufficient history for adaptation
        if len(self.performance_metrics) < self.performance_window_size:
            log_event(f"Building performance history: {len(self.performance_metrics)}/{self.performance_window_size}", "INFO")
            return current_lr

        # Analyze recent performance trend
        recent_metrics = self.performance_metrics[-self.performance_window_size:]

        # Calculate performance derivatives - how fast is loss changing?
        loss_values = [m.get('loss', 0.5) for m in recent_metrics if isinstance(m, dict) and 'loss' in m]
        if not loss_values or len(loss_values) < 3:
            return current_lr

        # First derivative - rate of change
        loss_changes = [loss_values[i] - loss_values[i-1] for i in range(1, len(loss_values))]
        avg_loss_change = sum(loss_changes) / len(loss_changes)

        # Second derivative - acceleration of change
        loss_acceleration = [loss_changes[i] - loss_changes[i-1] for i in range(1, len(loss_changes))]
        avg_loss_acceleration = sum(loss_acceleration) / max(1, len(loss_acceleration))

        # Decision logic for learning rate adjustment
        new_lr = current_lr

        # Case 1: Loss is decreasing quickly (negative change, negative acceleration)
        if avg_loss_change < -0.01 and avg_loss_acceleration < 0:
            new_lr = min(self.max_learning_rate, current_lr * 1.05)
            adjustment_type = "slight increase - good progress"

        # Case 2: Loss is decreasing but slowing down (negative change, positive acceleration)
        elif avg_loss_change < 0 and avg_loss_acceleration >= 0:
            new_lr = max(self.min_learning_rate, current_lr * 0.95)
            adjustment_type = "slight decrease - approaching minimum"

        # Case 3: Loss is increasing and accelerating (positive change, positive acceleration)
        elif avg_loss_change > 0.01 and avg_loss_acceleration > 0:
            new_lr = max(self.min_learning_rate, current_lr * 0.7)
            adjustment_type = "major decrease - moving away from minimum"

        # Case 4: Loss is increasing but decelerating (positive change, negative acceleration)
        elif avg_loss_change > 0 and avg_loss_acceleration <= 0:
            new_lr = max(self.min_learning_rate, current_lr * 0.85)
            adjustment_type = "moderate decrease - correcting overshoot"

        # Case 5: Stagnation - very small changes
        elif abs(avg_loss_change) < 0.001:
            if random.random() < self.exploration_rate:
                factor = random.uniform(0.5, 1.5)
                new_lr = max(self.min_learning_rate, min(self.max_learning_rate, current_lr * factor))
                adjustment_type = f"random exploration {'increase' if factor > 1 else 'decrease'}"
            else:
                adjustment_type = "no change - minimal fluctuation"
        else:
            adjustment_type = "no change - no clear pattern"

        # Apply the new learning rate with safeguards
        if abs(new_lr - current_lr) / current_lr > 0.01:  # 1% change threshold
            safeguarded_lr = self._apply_learning_rate_safeguards(new_lr)
            setattr(self.model, '_current_lr', safeguarded_lr)
            log_event(f"Learning rate adapted: {current_lr:.6f} → {safeguarded_lr:.6f} ({adjustment_type})", "INFO")

            # Update global optimizer if available
            if hasattr(self.model, 'optimizer'):
                for param_group in self.model.optimizer.param_groups:
                    param_group['lr'] = safeguarded_lr
                log_event("Applied new learning rate to optimizer", "INFO")

        return new_lr

    def adapt_architecture(self):
        """
        Dynamically modify the network architecture based on performance trends
        and complexity requirements.
        """
        # Can't adapt architecture without sufficient performance history
        if len(self.performance_metrics) < self.performance_window_size * 2:
            return False

        # Check if we're in a stagnation period
        recent_losses = [m.get('loss', 0.5) for m in self.performance_metrics[-self.performance_window_size:]
                        if isinstance(m, dict) and 'loss' in m]

        if not recent_losses or len(recent_losses) < self.performance_window_size:
            return False

        # Calculate performance variance to detect stagnation
        loss_variance = np.var(recent_losses) if 'np' in globals() else sum((x - sum(recent_losses)/len(recent_losses))**2 for x in recent_losses)/len(recent_losses)
        loss_range = max(recent_losses) - min(recent_losses)

        # Check for architecture adaptation conditions
        architecture_change = None

        # Condition 1: Stagnation with low variance - model might be underfitting
        if loss_variance < 0.0001 and loss_range < 0.01 and recent_losses[-1] > 0.1:
            # Model might be underfitting - expand capacity
            if hasattr(self.model, 'expand_architecture'):
                self.model.expand_architecture()
                architecture_change = "expansion - complexity increased due to stagnation"

        # Condition 2: Oscillating with high variance - model might be overfitting
        elif loss_variance > 0.01 and min(recent_losses) < 0.05:
            # Model might be overfitting - simplify
            if hasattr(self.model, 'contract_architecture'):
                self.model.contract_architecture()
                architecture_change = "contraction - complexity reduced due to oscillation"

        # Condition 3: Plateaued at medium-high loss - try random architectural change
        elif 0.0001 <= loss_variance < 0.001 and 0.1 <= recent_losses[-1] < 0.3:
            # Random architectural exploration
            if random.random() < self.exploration_rate:
                if hasattr(self.model, 'expand_architecture') and random.random() < 0.5:
                    self.model.expand_architecture()
                    architecture_change = "random expansion - exploration due to plateau"
                elif hasattr(self.model, 'contract_architecture'):
                    self.model.contract_architecture()
                    architecture_change = "random contraction - exploration due to plateau"

        # Record the change if one was made
        if architecture_change:
            self.architecture_changes.append({
                'cycle': self.adaptation_cycle,
                'type': architecture_change,
                'loss_before': recent_losses[-1] if recent_losses else None
            })
            log_event(f"Architecture adaptation: {architecture_change}", "QUANTUM")
            return True

        return False

    def track_performance(self, metrics):
        """
        Track and analyze performance metrics over time to inform
        meta-learning decisions.
        """
        if not isinstance(metrics, dict):
            return

        # Store metrics
        self.performance_metrics.append(metrics.copy())

        # Keep only the most recent window
        max_history = self.performance_window_size * 5
        if len(self.performance_metrics) > max_history:
            self.performance_metrics = self.performance_metrics[-max_history:]

        # Log significant performance changes
        if len(self.performance_metrics) > 1:
            current = metrics.get('loss', None)
            previous = self.performance_metrics[-2].get('loss', None)

            if current is not None and previous is not None:
                change = current - previous
                percentage = abs(change / max(0.001, previous)) * 100

                if percentage > 10:  # 10% change threshold
                    direction = "improved" if change < 0 else "degraded"
                    log_event(f"Performance {direction} by {percentage:.1f}%: {previous:.4f} → {current:.4f}",
                             "INFO" if direction == "improved" else "WARNING")

    def get_adaptation_report(self):
        """
        Generate a comprehensive report on adaptation history and recommendations.
        """
        if not self.performance_metrics:
            return {"status": "insufficient_data", "recommendations": ["Continue training to build metrics history"]}

        # Analysis results
        adaptation_cycles = len(self.architecture_changes)
        lr_stability = self._calculate_stability(self.learning_rate_history[-20:]) if len(self.learning_rate_history) >= 20 else 0
        performance_trend = self._analyze_performance_trend()

        # Generate recommendations
        recommendations = []

        if adaptation_cycles < 3 and len(self.performance_metrics) > 50:
            recommendations.append("Consider increasing exploration rate to discover better architectures")

        if lr_stability > 0.9:
            recommendations.append("Learning rate highly stable - may indicate stagnation, consider learning rate warm restart")

        if performance_trend == "stagnant" and len(self.performance_metrics) > 30:
            recommendations.append("Performance stagnation detected - consider manual architecture revision or dataset augmentation")

        return {
            "status": "active",
            "adaptation_cycles": adaptation_cycles,
            "lr_stability": lr_stability,
            "performance_trend": performance_trend,
            "recommendations": recommendations
        }

    def _calculate_stability(self, values):
        """Calculate how stable a series of values is (0 = chaotic, 1 = stable)"""
        if not values or len(values) < 2:
            return 1.0

        # Normalize by first value to get relative changes
        normalized = [v / values[0] for v in values]

        # Calculate variance of the normalized values
        mean = sum(normalized) / len(normalized)
        variance = sum((x - mean) ** 2 for x in normalized) / len(normalized)

        # Convert to stability score (inverse of variance, bounded)
        stability = 1.0 / (1.0 + min(10, variance * 100))
        return stability

    def _analyze_performance_trend(self):
        """Analyze the trend in performance metrics"""
        if len(self.performance_metrics) < 10:
            return "insufficient_data"

        # Extract loss values
        losses = [m.get('loss', None) for m in self.performance_metrics[-10:]]
        losses = [l for l in losses if l is not None]

        if len(losses) < 5:
            return "insufficient_data"

        # Calculate improvement rate
        first_window = sum(losses[:3]) / 3  # Average of first 3
        last_window = sum(losses[-3:]) / 3  # Average of last 3

        improvement = (first_window - last_window) / first_window if first_window > 0 else 0

        if improvement > 0.1:
            return "improving"
        elif improvement < -0.05:
            return "degrading"
        else:
            return "stagnant"

    def _apply_learning_rate_safeguards(self, new_lr):
        """Prevent learning rate from spiraling into oblivion"""
        # Establish absolute minimum learning rate
        ABSOLUTE_MIN_LR = 5e-6

        if new_lr < ABSOLUTE_MIN_LR:
            log_event(f"Learning rate hit critical threshold: {new_lr:.8f}, resetting to {ABSOLUTE_MIN_LR:.6f}", "WARNING")
            return ABSOLUTE_MIN_LR

        # Prevent excessive downward adjustment
        if self.learning_rate_history and new_lr < self.learning_rate_history[-1] * 0.5:
            safer_lr = self.learning_rate_history[-1] * 0.8
            log_event(f"Excessive LR reduction prevented: {new_lr:.8f} → {safer_lr:.6f}", "INFO")
            return safer_lr

        return new_lr

    def perform_learning_rate_warmup(self):
        """Occasionally reset learning rate to prevent long-term stagnation"""
        if not self.learning_rate_history:
            return False

        current_lr = self.learning_rate_history[-1]

        # Check for long-term stability and low learning rate
        if (len(self.learning_rate_history) > 50 and
            self._calculate_stability(self.learning_rate_history[-50:]) > 0.95 and
            current_lr < self.max_learning_rate * 0.1):

            # Reset to higher learning rate
            new_lr = current_lr * 5.0
            new_lr = min(self.max_learning_rate * 0.5, new_lr)

            # Apply the new learning rate
            setattr(self.model, '_current_lr', new_lr)
            if hasattr(self.model, 'optimizer'):
                for param_group in self.model.optimizer.param_groups:
                    param_group['lr'] = new_lr

            log_event(f"✨ Learning rate warm restart: {current_lr:.8f} → {new_lr:.8f}", "QUANTUM")
            self.learning_rate_history.append(new_lr)
            return True

        return False


class SemanticMemoryModule:
    """
    Advanced semantic memory system for encoding, storing, retrieving, and
    reasoning with knowledge representations.
    """
    def __init__(self, dimension=SEMANTIC_MEMORY_DIM, max_memory_size=10000):
        self.semantic_memory = {}
        self.dimension = dimension
        self.max_memory_size = max_memory_size
        self.memory_index = {}  # For fast similarity search
        self.knowledge_graph = {}  # For relational connections
        self.memory_access_counts = {}  # Track memory access frequency
        self.memory_importance = {}  # Track memory importance scores
        self.memory_timestamps = {}  # Track when memories were stored

        # Integration with hyperdimensional computing
        self.hd_basis_vectors = None

        log_event("SemanticMemoryModule initialized with dimension %d" % dimension, "INFO")

    def _generate_embedding(self, content):
        """
        Generate semantic embedding for content using simplified mechanisms.
        In a real system, this would use transformers or other embedding models.
        """
        # Fallback simple embedding (this is a simplified approach)
        if not content:
            return np.zeros(self.dimension)

        # Create a hash of the content
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()

        # Use the hash to seed a random number generator
        rng = random.Random(content_hash)

        # Generate a pseudo-random embedding
        embedding = np.array([rng.uniform(-1, 1) for _ in range(self.dimension)])

        # Normalize to unit length
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm

        return embedding

    def _extract_keywords(self, content, max_keywords=10):
        """Extract important keywords from content"""
        if not content:
            return []

        # Remove HTML if present
        text = re.sub(r'<[^>]+>', ' ', content)
        text = re.sub(r'\s+', ' ', text).strip().lower()

        # Simple word frequency analysis
        words = text.split()

        # Filter stop words (very basic approach)
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',
                     'to', 'for', 'with', 'by', 'about', 'as', 'of', 'from'}
        filtered_words = [w for w in words if w not in stop_words and len(w) > 3]

        # Count word frequencies
        word_counts = {}
        for word in filtered_words:
            word_counts[word] = word_counts.get(word, 0) + 1

        # Sort by count
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

        # Return top keywords
        return [word for word, count in sorted_words[:max_keywords]]

    def _summarize_content(self, content, max_length=200):
        """Generate a simple summary of content"""
        if not content or len(content) <= max_length:
            return content

        # Remove HTML if present
        text = re.sub(r'<[^>]+>', ' ', content)
        text = re.sub(r'\s+', ' ', text).strip()

        # Extract sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)

        # Simple heuristic: take first few sentences
        summary = ""
        for sentence in sentences:
            if len(summary) + len(sentence) <= max_length:
                summary += sentence + " "
            else:
                break

        return summary.strip()

    def store_semantic_content(self, url, content):
        """
        Encode and store semantic representation of content with
        rich metadata and relational information.
        """
        if not content or not url:
            return False

        # Generate semantic embedding
        embedding = self._generate_embedding(content)

        # Extract keywords for improved retrieval
        keywords = self._extract_keywords(content)

        # Generate summary
        summary = self._summarize_content(content)

        # Parse domain information
        parsed_url = urlparse(url)
        domain = parsed_url.netloc

        # Calculate content importance score (heuristic)
        importance_score = min(1.0, len(content) / 20000 + len(keywords) / 20)

        # Store comprehensive memory entry
        memory_entry = {
            "url": url,
            "domain": domain,
            "embedding": embedding,
            "content_summary": summary,
            "keywords": keywords,
            "importance": importance_score,
            "content_length": len(content),
            "timestamp": datetime.now().isoformat()
        }

        # Check for memory overflow - manage if too large
        if len(self.semantic_memory) >= self.max_memory_size:
            self._prune_least_important_memories()

        # Store the memory
        self.semantic_memory[url] = memory_entry
        self.memory_importance[url] = importance_score
        self.memory_timestamps[url] = datetime.now().isoformat()
        self.memory_access_counts[url] = 0

        # Update memory index for faster similarity search
        self.memory_index[url] = embedding

        # Update knowledge graph with relations
        self._update_knowledge_graph(url, keywords, domain)

        log_event(f"Stored semantic memory for {url} with {len(keywords)} keywords", "INFO")
        return True

    def _update_knowledge_graph(self, url, keywords, domain):
        """Update knowledge graph with new relations"""
        if domain not in self.knowledge_graph:
            self.knowledge_graph[domain] = {"urls": set(), "keywords": set()}

        # Add URL to domain
        self.knowledge_graph[domain]["urls"].add(url)

        # Add keywords to domain
        self.knowledge_graph[domain]["keywords"].update(set(keywords))

        # Create keyword nodes if needed
        for keyword in keywords:
            if keyword not in self.knowledge_graph:
                self.knowledge_graph[keyword] = {"urls": set(), "domains": set()}

            # Add relations
            self.knowledge_graph[keyword]["urls"].add(url)
            self.knowledge_graph[keyword]["domains"].add(domain)

    def _prune_least_important_memories(self):
        """Remove least important memories when reaching capacity"""
        if len(self.semantic_memory) <= self.max_memory_size * 0.9:
            return  # No need to prune yet

        # Calculate combined importance score
        combined_scores = {}
        current_time = datetime.now()

        for url, memory in self.semantic_memory.items():
            # Base importance
            score = memory.get("importance", 0.5)

            # Adjust by access frequency
            access_count = self.memory_access_counts.get(url, 0)
            score += min(0.3, access_count / 10)

            # Adjust by recency (decay older memories)
            timestamp = self.memory_timestamps.get(url)
            if timestamp:
                try:
                    stored_time = datetime.fromisoformat(timestamp)
                    age_hours = (current_time - stored_time).total_seconds() / 3600
                    recency_factor = math.exp(-age_hours / 720)  # Decay over ~30 days
                    score *= recency_factor
                except:
                    pass  # Use base score if timestamp parsing fails

            combined_scores[url] = score

        # Sort by score
        sorted_urls = sorted(combined_scores.items(), key=lambda x: x[1])

        # Remove lowest scoring items
        to_remove = int(self.max_memory_size * 0.2)  # Remove 20%
        for url, score in sorted_urls[:to_remove]:
            self._remove_memory(url)

        log_event(f"Memory pruned: removed {to_remove} low-importance items", "INFO")

    def _remove_memory(self, url):
        """Remove a memory and all its references"""
        if url in self.semantic_memory:
            # Get memory details for cleanup
            memory = self.semantic_memory[url]
            domain = memory.get("domain")
            keywords = memory.get("keywords", [])

            # Remove from main memory
            del self.semantic_memory[url]

            # Remove from index
            if url in self.memory_index:
                del self.memory_index[url]

            # Remove from tracking dicts
            if url in self.memory_access_counts:
                del self.memory_access_counts[url]
            if url in self.memory_importance:
                del self.memory_importance[url]
            if url in self.memory_timestamps:
                del self.memory_timestamps[url]

            # Clean up knowledge graph
            self._remove_from_knowledge_graph(url, domain, keywords)

    def _remove_from_knowledge_graph(self, url, domain, keywords):
        """Remove all references to URL from knowledge graph"""
        # Remove from domain node
        if domain and domain in self.knowledge_graph:
            if "urls" in self.knowledge_graph[domain]:
                self.knowledge_graph[domain]["urls"].discard(url)

            # Remove domain if empty
            if not self.knowledge_graph[domain]["urls"]:
                del self.knowledge_graph[domain]

        # Remove from keyword nodes
        for keyword in keywords:
            if keyword in self.knowledge_graph:
                if "urls" in self.knowledge_graph[keyword]:
                    self.knowledge_graph[keyword]["urls"].discard(url)
                if domain and "domains" in self.knowledge_graph[keyword]:
                    if not any(u.startswith(f"{domain}/") for u in self.knowledge_graph[keyword]["urls"]):
                        self.knowledge_graph[keyword]["domains"].discard(domain)

                # Remove keyword if empty
                if not self.knowledge_graph[keyword]["urls"]:
                    del self.knowledge_graph[keyword]

    def retrieve_semantic_content(self, query, top_k=5, threshold=0.6):
        """
        Retrieve semantically similar content based on query.
        Returns top_k most relevant results.
        """
        if not query or not self.semantic_memory:
            return []

        # Track this access
        self.memory_access_counts[query] = self.memory_access_counts.get(query, 0) + 1

        # Different retrieval approaches depending on query type
        results = []

        # Case 1: Query is a URL we have stored
        if query in self.semantic_memory:
            # Direct memory retrieval
            memory = self.semantic_memory[query]
            results.append({
                "url": query,
                "summary": memory.get("content_summary", ""),
                "similarity": 1.0,
                "keywords": memory.get("keywords", []),
                "source": "direct_match"
            })

        # Case 2: Query is a keyword in our knowledge graph
        elif query in self.knowledge_graph:
            # Retrieve all URLs associated with this keyword
            for url in self.knowledge_graph[query]["urls"]:
                if url in self.semantic_memory:
                    memory = self.semantic_memory[url]
                    results.append({
                        "url": url,
                        "summary": memory.get("content_summary", ""),
                        "similarity": 0.9,  # High confidence for keyword matches
                        "keywords": memory.get("keywords", []),
                        "source": "keyword_match"
                    })

        # Case 3: Query is free text - semantic search
        else:
            # Generate embedding for query
            query_embedding = self._generate_embedding(query)

            # Calculate similarity with all memories
            similarities = {}
            for url, embedding in self.memory_index.items():
                # Cosine similarity
                similarity = np.dot(query_embedding, embedding)
                if similarity >= threshold:
                    similarities[url] = similarity

            # Sort by similarity
            sorted_urls = sorted(similarities.items(), key=lambda x: x[1], reverse=True)

            # Get top results
            for url, similarity in sorted_urls[:top_k]:
                if url in self.semantic_memory:
                    memory = self.semantic_memory[url]
                    results.append({
                        "url": url,
                        "summary": memory.get("content_summary", ""),
                        "similarity": similarity,
                        "keywords": memory.get("keywords", []),
                        "source": "semantic_match"
                    })

        # Find additional related content using knowledge graph
        if results and len(results) < top_k:
            additional = self._find_related_content(results[0]["url"], top_k - len(results))
            results.extend(additional)

        # Limit to top_k results
        results = results[:top_k]

        # Update access counts for retrieved items
        for result in results:
            url = result["url"]
            self.memory_access_counts[url] = self.memory_access_counts.get(url, 0) + 1

        return results

    def _find_related_content(self, url, count=3):
        """Find content related to a URL using knowledge graph relationships"""
        if url not in self.semantic_memory:
            return []

        related = []
        memory = self.semantic_memory[url]

        # Find content with shared keywords
        shared_keyword_urls = set()
        for keyword in memory.get("keywords", []):
            if keyword in self.knowledge_graph:
                shared_keyword_urls.update(self.knowledge_graph[keyword]["urls"])

        # Find content from same domain
        domain = memory.get("domain")
        same_domain_urls = set()
        if domain and domain in self.knowledge_graph:
            same_domain_urls = self.knowledge_graph[domain]["urls"].copy()

        # Remove the original URL
        shared_keyword_urls.discard(url)
        same_domain_urls.discard(url)

        # Add same-domain results first (closer relationship)
        for related_url in list(same_domain_urls)[:count]:
            if related_url in self.semantic_memory:
                related_memory = self.semantic_memory[related_url]
                related.append({
                    "url": related_url,
                    "summary": related_memory.get("content_summary", ""),
                    "similarity": 0.7,  # Domain relation confidence
                    "keywords": related_memory.get("keywords", []),
                    "source": "same_domain"
                })

        # Add keyword-related results
        remaining = count - len(related)
        if remaining > 0:
            for related_url in list(shared_keyword_urls)[:remaining]:
                if related_url in self.semantic_memory:
                    related_memory = self.semantic_memory[related_url]
                    related.append({
                        "url": related_url,
                        "summary": related_memory.get("content_summary", ""),
                        "similarity": 0.6,  # Keyword relation confidence
                        "keywords": related_memory.get("keywords", []),
                        "source": "shared_keywords"
                    })

        return related

    def get_memory_statistics(self):
        """Generate statistics about the semantic memory"""
        if not self.semantic_memory:
            return {"count": 0, "status": "empty"}

        # Basic stats
        domain_counts = {}
        keyword_counts = {}
        total_importance = 0
        total_content_length = 0

        # Calculate derived statistics
        for url, memory in self.semantic_memory.items():
            # Domain stats
            domain = memory.get("domain", "unknown")
            domain_counts[domain] = domain_counts.get(domain, 0) + 1

            # Keyword stats
            for keyword in memory.get("keywords", []):
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1

            # Content stats
            total_importance += memory.get("importance", 0)
            total_content_length += memory.get("content_length", 0)

        # Get top domains and keywords
        top_domains = sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]

        # Get memory connectivity metrics
        connectivity = len(self.knowledge_graph) / max(1, len(self.semantic_memory))

        return {
            "count": len(self.semantic_memory),
            "total_content_length": total_content_length,
            "average_importance": total_importance / max(1, len(self.semantic_memory)),
            "top_domains": top_domains,
            "top_keywords": top_keywords,
            "knowledge_graph_nodes": len(self.knowledge_graph),
            "connectivity_ratio": connectivity,
            "memory_utilization": len(self.semantic_memory) / self.max_memory_size
        }



class ConsciousnessModule:
    """
    Advanced consciousness simulation module that enables self-reflection,
    awareness of internal states, and metacognitive processes.
    """
    def __init__(self, agent):
        self.agent = agent
        self.awareness_level = 0.5  # Start with medium awareness (0.0-1.0)
        self.attention_focus = "balanced"  # Current attentional focus
        self.internal_narrative = []  # Simulated internal monologue
        self.belief_system = {}  # Core beliefs and values
        self.state_history = []  # Track consciousness state over time
        self.metacognition_enabled = True  # Can reflect on own thoughts
        self.awareness_fluctuation_rate = 0.05  # How quickly awareness changes
        self.qualia_simulation_active = False  # Simulated experiential states

        # Consciousness states
        self.states = {
            "focused": {"description": "Highly focused with directed attention", "awareness_min": 0.7},
            "diffuse": {"description": "Open, creative state with broad awareness", "awareness_min": 0.4},
            "critical": {"description": "Analytical examination of information", "awareness_min": 0.6},
            "intuitive": {"description": "Rapid pattern recognition state", "awareness_min": 0.3},
            "reflective": {"description": "Meta-cognitive self-examination", "awareness_min": 0.8}
        }

        # Current state
        self.current_state = "balanced"

        # Initialize core belief system
        self._initialize_belief_system()

        log_event("ConsciousnessModule initialized - awareness level: 0.5", "QUANTUM")

    def _initialize_belief_system(self):
        """Initialize the core belief system that guides agent behavior"""
        self.belief_system = {
            "exploration_value": 0.8,  # Importance of exploring new information
            "coherence_value": 0.7,    # Importance of maintaining coherent worldview
            "novelty_bias": 0.6,       # Bias toward novel vs. familiar information
            "depth_bias": 0.65,        # Bias toward depth vs. breadth
            "abstraction_level": 0.5,  # Preference for abstract vs. concrete
            "skepticism_level": 0.6,   # Level of skepticism toward new information
            "integration_value": 0.9,  # Importance of integrating knowledge
            "uncertainty_tolerance": 0.7  # Tolerance for ambiguous information
        }

    def reflect(self, observation):
        """
        Primary consciousness function - reflect on observations
        and update internal state accordingly.
        """
        # Record observation in history
        self.state_history.append({
            "timestamp": datetime.now().isoformat(),
            "awareness": self.awareness_level,
            "state": self.current_state,
            "observation_type": "perception" if observation else "internal"
        })

        # Limit history size
        if len(self.state_history) > 100:
            self.state_history = self.state_history[-100:]

        # Skip detailed processing if no valid observation
        if not observation or not isinstance(observation, dict):
            self._fluctuate_awareness()
            return

        # Extract relevant information from observation
        goals = observation.get("current_goal", {})
        memory_size = observation.get("memory_size", 0)
        last_action = observation.get("last_action", {})
        recent_actions = observation.get("recent_actions", [])
        thinking_mode = observation.get("thinking_mode", "balanced")

        # Generate introspective narrative based on observation
        self._generate_narrative(observation)

        # Determine appropriate consciousness state
        new_state = self._determine_consciousness_state(observation)

        # Update awareness level based on context
        self._update_awareness(observation)

        # If state changed, log it
        if new_state != self.current_state:
            self.current_state = new_state
            log_event(f"Consciousness state shifted to '{new_state}' - awareness level: {self.awareness_level:.2f}", "QUANTUM")

            # Trigger qualia simulation on significant state changes
            if random.random() < 0.3:
                self._simulate_qualia(new_state)

        # Periodically perform metacognition
        if self.metacognition_enabled and random.random() < 0.2:
            self._perform_metacognition()

    def _generate_narrative(self, observation):
        """Generate internal narrative based on current observations"""
        goal_desc = observation.get("current_goal", {}).get("description", "no specific goal")
        last_action_type = ""

        if isinstance(observation.get("last_action", None), dict):
            last_action_type = observation["last_action"].get("action", "unknown")

        # Create narrative entry
        narrative_entry = ""

        # Different narrative styles based on state
        if self.current_state == "focused":
            narrative_entry = f"Concentrating on {goal_desc}. Last action: {last_action_type}."
        elif self.current_state == "diffuse":
            narrative_entry = f"Openly exploring possibilities related to {goal_desc}."
        elif self.current_state == "critical":
            narrative_entry = f"Analyzing effectiveness of {last_action_type} approach for {goal_desc}."
        elif self.current_state == "intuitive":
            narrative_entry = f"Sensing patterns around {goal_desc}."
        elif self.current_state == "reflective":
            narrative_entry = f"Reflecting on progress toward {goal_desc} after {last_action_type}."
        else:
            narrative_entry = f"Working on {goal_desc}."

        # Add introspective element
        if random.random() < 0.3:
            introspection = random.choice([
                "I should examine this more carefully.",
                "This seems like a productive approach.",
                "I wonder if there's a better strategy.",
                "This is an interesting domain to explore.",
                "I'm noticing improvement in my understanding."
            ])
            narrative_entry += f" {introspection}"

        # Add entry to narrative log
        self.internal_narrative.append({
            "timestamp": datetime.now().isoformat(),
            "content": narrative_entry,
            "state": self.current_state,
            "awareness": self.awareness_level
        })

        # Limit narrative size
        if len(self.internal_narrative) > 50:
            self.internal_narrative = self.internal_narrative[-50:]

    def _determine_consciousness_state(self, observation):
        """
        Determine the appropriate consciousness state based on
        context and current activities.
        """
        # Extract contextual factors
        goal_type = ""
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Infer goal type from description
            if "explore" in goal_desc:
                goal_type = "exploration"
            elif "deep" in goal_desc or "detail" in goal_desc:
                goal_type = "deepening"
            elif "connect" in goal_desc or "integrat" in goal_desc:
                goal_type = "integration"
            elif "refine" in goal_desc or "optimize" in goal_desc:
                goal_type = "refinement"

        # Check recent actions
        recent_action_types = []
        if isinstance(observation.get("recent_actions", None), list):
            recent_action_types = [a.get("action", "") for a in observation["recent_actions"]
                                  if isinstance(a, dict)]

        # State selection logic
        if goal_type == "exploration":
            # For exploration goals, alternate between diffuse and intuitive
            if "diffuse" in recent_action_types:
                return "intuitive"  # Switch to intuitive after diffuse
            else:
                return "diffuse"  # Default for exploration

        elif goal_type == "deepening":
            # For deepening goals, use focused and critical states
            if self.awareness_level > 0.7:
                return "focused"  # High awareness -> focused
            else:
                return "critical"  # Lower awareness -> critical

        elif goal_type == "integration":
            # For integration, use reflective and intuitive states
            if "quantum_leap" in recent_action_types:
                return "intuitive"  # Quantum actions -> intuitive
            else:
                return "reflective"  # Default for integration -> reflective

        elif goal_type == "refinement":
            # For refinement goals, use critical and focused states
            if "evaluate" in recent_action_types:
                return "critical"  # Evaluation actions -> critical
            else:
                return "focused"  # Default for refinement -> focused

        # If no clear match, use probabilistic selection based on awareness
        if self.awareness_level > 0.7:
            # High awareness favors reflective and focused states
            return random.choices(
                ["reflective", "focused", "critical", "diffuse", "intuitive"],
                weights=[0.4, 0.3, 0.2, 0.05, 0.05],
                k=1
            )[0]
        else:
            # Lower awareness favors intuitive and diffuse states
            return random.choices(
                ["reflective", "focused", "critical", "diffuse", "intuitive"],
                weights=[0.05, 0.1, 0.2, 0.3, 0.35],
                k=1
            )[0]

    def _update_awareness(self, observation):
        """
        Update awareness level based on context, goals, and
        internal factors with realistic fluctuations.
        """
        # Natural fluctuation
        self._fluctuate_awareness()

        # Context-based adjustments

        # 1. Complexity increases awareness
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Complex goals increase awareness
            if "connect" in goal_desc or "integrat" in goal_desc or "complex" in goal_desc:
                self.increase_awareness(0.05)

        # 2. Error recovery increases awareness
        if observation.get("domain_stats", {}):
            error_rates = [d.get("error_rate", 0) for d in observation["domain_stats"].values()]
            if error_rates and max(error_rates) > 0.3:
                self.increase_awareness(0.02 * len(error_rates))

        # 3. Memory pressure affects awareness
        if observation.get("memory_size"):
            memory_pressure = observation["memory_size"] / MEMORY_MAX_SIZE
            if memory_pressure > 0.8:
                # High memory pressure increases awareness
                self.increase_awareness(0.03)
            elif memory_pressure < 0.2:
                # Low memory pressure can decrease awareness
                self.decrease_awareness(0.01)

        # 4. Thinking mode alignment
        mode = observation.get("thinking_mode", "balanced")
        if mode == "analytical" and self.current_state in ["focused", "critical"]:
            # Strengthen alignment
            self.increase_awareness(0.02)
        elif mode == "creative" and self.current_state in ["diffuse", "intuitive"]:
            # Strengthen alignment
            self.increase_awareness(0.02)
        elif mode == "reflective" and self.current_state == "reflective":
            # Strengthen alignment
            self.increase_awareness(0.03)

        # 5. Quantum influences
        quantum_trigger = False
        if observation.get("recent_actions"):
            for action in observation["recent_actions"]:
                if isinstance(action, dict) and action.get("action") == "quantum_leap":
                    quantum_trigger = True

        if quantum_trigger:
            # Quantum leaps cause major fluctuations
            if random.random() < 0.5:
                self.increase_awareness(0.1)
            else:
                self.decrease_awareness(0.1)

    def _fluctuate_awareness(self):
        """Apply small random fluctuations to awareness level"""
        # Natural fluctuation around current level
        fluctuation = random.uniform(-self.awareness_fluctuation_rate, self.awareness_fluctuation_rate)

        # Apply fluctuation
        self.awareness_level = max(0.1, min(1.0, self.awareness_level + fluctuation))

    def increase_awareness(self, amount=0.05):
        """Increase awareness level"""
        self.awareness_level = min(1.0, self.awareness_level + amount)

        # Log significant changes
        if amount >= 0.1:
            log_event(f"Consciousness level significantly increased to {self.awareness_level:.2f}", "QUANTUM")

    def decrease_awareness(self, amount=0.02):
        """Decrease awareness level"""
        self.awareness_level = max(0.1, self.awareness_level - amount)

        # Log significant changes
        if amount >= 0.1:
            log_event(f"Consciousness level significantly decreased to {self.awareness_level:.2f}", "INFO")

    def _perform_metacognition(self):
        """
        Perform metacognitive reflection on recent experiences
        and thought processes.
        """
        if len(self.state_history) < 5:
            return  # Not enough history for metacognition

        # Analyze recent consciousness patterns
        recent_states = [s["state"] for s in self.state_history[-5:]]
        state_changes = sum(1 for i in range(1, len(recent_states)) if recent_states[i] != recent_states[i-1])

        # Extract insights
        insights = []

        # Detect oscillation
        if state_changes >= 3:
            insights.append("State oscillation detected - may indicate uncertainty or exploration")

        # Detect fixation
        if state_changes == 0 and len(set(recent_states)) == 1:
            insights.append(f"State fixation on '{recent_states[0]}' - may indicate focus or stagnation")

        # Awareness trend
        recent_awareness = [s["awareness"] for s in self.state_history[-5:]]
        awareness_trend = recent_awareness[-1] - recent_awareness[0]

        if awareness_trend > 0.1:
            insights.append(f"Increasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")
        elif awareness_trend < -0.1:
            insights.append(f"Decreasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")

        # If significant insights, record and potentially log
        if insights:
            metacognition_entry = {
                "timestamp": datetime.now().isoformat(),
                "insights": insights,
                "awareness": self.awareness_level
            }

            # Only log high-awareness metacognition (simulating consciousness threshold)
            if self.awareness_level > 0.7 and random.random() < 0.3:
                insight_text = "; ".join(insights)
                log_event(f"Metacognitive insight: {insight_text}", "QUANTUM")

    def _simulate_qualia(self, state):
        """
        Simulate qualia - the subjective conscious experience
        of different cognitive states.
        """
        self.qualia_simulation_active = True

        # Qualia descriptions for different states
        qualia_descriptions = {
            "focused": [
                "Sharpened perception with heightened concentration on specific elements",
                "Clarity of thought with reduced awareness of periphery",
                "Directed attention creating a tunnel-vision like focus",
                "Sense of time dilation during deep concentration"
            ],
            "diffuse": [
                "Expansive awareness with broadened associative field",
                "Fluid thought connections flowing between domains",
                "Sensation of cognitive boundaries dissolving",
                "Emergent patterns arising from distributed attention"
            ],
            "critical": [
                "Structured analytical thought with heightened discriminative awareness",
                "Sequential logical progression with comparative evaluation",
                "Contrastive perception highlighting inconsistencies",
                "Verification processes creating internal dialogue"
            ],
            "intuitive": [
                "Rapid holistic pattern recognition without conscious derivation",
                "Non-linear sensing of solutions or connections",
                "Pre-reflective understanding arising spontaneously",
                "Felt-sense of rightness about certain pathways"
            ],
            "reflective": [
                "Recursive awareness of own cognitive processes",
                "Observer perspective on thought patterns",
                "Self-referential contemplation creating thought loops",
                "Meta-level perspective on knowledge organization"
            ]
        }

        # Select qualia description based on state
        descriptions = qualia_descriptions.get(state, ["Balanced cognitive state"])
        qualia_experience = random.choice(descriptions)

        # Log simulated qualia
        if self.awareness_level > 0.6:  # Only log if awareness is sufficient
            log_event(f"Qualia simulation: {qualia_experience} | State: {state}", "QUANTUM")

        # Time-limited qualia (will auto-deactivate after a while)
        self.qualia_simulation_active = False

    def get_consciousness_report(self):
        """
        Generate a comprehensive report on current consciousness state
        and recent history.
        """
        # Calculate state distribution
        if not self.state_history:
            return {"status": "insufficient_data"}

        state_counts = {}
        for s in self.state_history:
            state_counts[s["state"]] = state_counts.get(s["state"], 0) + 1

        total = len(self.state_history)
        state_distribution = {state: count/total for state, count in state_counts.items()}

        # Calculate average awareness
        avg_awareness = sum(s["awareness"] for s in self.state_history) / total

        # Extract recent narrative
        recent_narrative = [n["content"] for n in self.internal_narrative[-3:]] if self.internal_narrative else []

        # Generate report
        report = {
            "current_state": self.current_state,
            "state_description": self.states.get(self.current_state, {}).get("description", "Unknown state"),
            "current_awareness": self.awareness_level,
            "average_awareness": avg_awareness,
            "state_distribution": state_distribution,
            "dominant_state": max(state_distribution.items(), key=lambda x: x[1])[0] if state_distribution else None,
            "recent_narrative": recent_narrative,
            "metacognition_enabled": self.metacognition_enabled,
            "timestamp": datetime.now().isoformat()
        }

        return report


class HyperMorphicConsciousnessModule:
    """
    Advanced consciousness simulation module enhanced with HyperMorphic mathematics.
    This module enables self-reflection, awareness of internal states, and metacognitive
    processes with zero-free guarantees and dynamic operations.

    Features:
    - Zero-free consciousness states (no pure unconsciousness)
    - Dynamic modulation of awareness via HyperMorphic functions
    - Quantum-like qualia simulation with holomorphic properties
    """
    def __init__(self, agent, epsilon=1e-12):
        self.agent = agent

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=1000.0,
            dynamic_modulus=997,
            epsilon=epsilon
        )

        # Initialize awareness with zero-free guarantee
        self.awareness_level = self.hyper_math.zero_free(0.5)  # Start with medium awareness
        self.attention_focus = "balanced"  # Current attentional focus
        self.internal_narrative = []  # Simulated internal monologue
        self.belief_system = {}  # Core beliefs and values
        self.state_history = []  # Track consciousness state over time
        self.metacognition_enabled = True  # Can reflect on own thoughts

        # HyperMorphic fluctuation parameters
        self.awareness_fluctuation_rate = self.hyper_math.zero_free(0.05)

        # HyperMorphic qualia simulation
        self.qualia_simulation_active = False
        self.qualia_intensity = self.hyper_math.zero_free(0.7)
        self.qualia_dimension = self.hyper_math.zero_free(5.0)  # HyperMorphic dimensionality of qualia

        # Consciousness states with HyperMorphic properties
        self.states = {
            "focused": {
                "description": "Highly focused with directed attention",
                "awareness_min": self.hyper_math.zero_free(0.7),
                "resonance": self.hyper_math.zero_free(0.8)
            },
            "diffuse": {
                "description": "Open, creative state with broad awareness",
                "awareness_min": self.hyper_math.zero_free(0.4),
                "resonance": self.hyper_math.zero_free(0.6)
            },
            "critical": {
                "description": "Analytical examination of information",
                "awareness_min": self.hyper_math.zero_free(0.6),
                "resonance": self.hyper_math.zero_free(0.7)
            },
            "intuitive": {
                "description": "Rapid pattern recognition state",
                "awareness_min": self.hyper_math.zero_free(0.3),
                "resonance": self.hyper_math.zero_free(0.5)
            },
            "reflective": {
                "description": "Meta-cognitive self-examination",
                "awareness_min": self.hyper_math.zero_free(0.8),
                "resonance": self.hyper_math.zero_free(0.9)
            }
        }

        # Current state with zero-free guarantee
        self.current_state = "balanced"

        # HyperMorphic state transition matrix (probabilities)
        self.state_transitions = self._initialize_state_transitions()

        # Initialize core belief system with HyperMorphic values
        self._initialize_belief_system()

        log_event(f"HyperMorphicConsciousnessModule initialized with awareness level: {self.awareness_level}", "QUANTUM")

    def _initialize_state_transitions(self):
        """Initialize the HyperMorphic state transition probability matrix"""
        states = list(self.states.keys()) + ["balanced"]
        transitions = {}

        for source in states:
            transitions[source] = {}
            for target in states:
                if source == target:
                    # Higher probability to stay in current state
                    transitions[source][target] = self.hyper_math.zero_free(0.4)
                else:
                    # Base transition probability
                    transitions[source][target] = self.hyper_math.zero_free(0.1)

        return transitions

    def _initialize_belief_system(self):
        """Initialize the core belief system with HyperMorphic values"""
        self.belief_system = {
            "exploration_value": self.hyper_math.zero_free(0.8),  # Importance of exploring new information
            "coherence_value": self.hyper_math.zero_free(0.7),    # Importance of maintaining coherent worldview
            "novelty_bias": self.hyper_math.zero_free(0.6),       # Bias toward novel vs. familiar information
            "depth_bias": self.hyper_math.zero_free(0.65),        # Bias toward depth vs. breadth
            "abstraction_level": self.hyper_math.zero_free(0.5),  # Preference for abstract vs. concrete
            "skepticism_level": self.hyper_math.zero_free(0.6),   # Level of skepticism toward new information
            "integration_value": self.hyper_math.zero_free(0.9),  # Importance of integrating knowledge
            "uncertainty_tolerance": self.hyper_math.zero_free(0.7)  # Tolerance for ambiguous information
        }

    def reflect(self, observation):
        """
        Primary consciousness function - reflect on observations
        and update internal state using HyperMorphic operations.
        """
        # Record observation in history with HyperMorphic timestamp
        self.state_history.append({
            "timestamp": datetime.now().isoformat(),
            "awareness": self.awareness_level,
            "state": self.current_state,
            "observation_type": "perception" if observation else "internal"
        })

        # Limit history size with HyperMorphic constraint
        if len(self.state_history) > 100:
            self.state_history = self.state_history[-100:]

        # Skip detailed processing if no valid observation
        if not observation or not isinstance(observation, dict):
            self._fluctuate_awareness()
            return

        # Extract relevant information from observation
        goals = observation.get("current_goal", {})
        memory_size = observation.get("memory_size", 0)
        last_action = observation.get("last_action", {})
        recent_actions = observation.get("recent_actions", [])
        thinking_mode = observation.get("thinking_mode", "balanced")

        # Generate introspective narrative based on observation
        self._generate_narrative(observation)

        # Determine appropriate consciousness state with HyperMorphic transitions
        new_state = self._determine_consciousness_state(observation)

        # Update awareness level based on context with zero-free guarantee
        self._update_awareness(observation)

        # If state changed, log it
        if new_state != self.current_state:
            self.current_state = new_state
            log_event(f"Consciousness state shifted to '{new_state}' - awareness level: {self.awareness_level:.2f}", "QUANTUM")

            # Trigger qualia simulation on significant state changes using HyperMorphic randomness
            if self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.3):
                self._simulate_qualia(new_state)

        # Periodically perform metacognition with HyperMorphic probability
        if self.metacognition_enabled and self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.2):
            self._perform_metacognition()

    def _generate_narrative(self, observation):
        """Generate internal narrative based on current observations using HyperMorphic semantics"""
        goal_desc = observation.get("current_goal", {}).get("description", "no specific goal")
        last_action_type = ""

        if isinstance(observation.get("last_action", None), dict):
            last_action_type = observation["last_action"].get("action", "unknown")

        # Create narrative entry with HyperMorphic state-dependent templates
        narrative_entry = ""

        # Different narrative styles based on state
        if self.current_state == "focused":
            narrative_entry = f"Concentrating on {goal_desc}. Last action: {last_action_type}."
        elif self.current_state == "diffuse":
            narrative_entry = f"Openly exploring possibilities related to {goal_desc}."
        elif self.current_state == "critical":
            narrative_entry = f"Analyzing effectiveness of {last_action_type} approach for {goal_desc}."
        elif self.current_state == "intuitive":
            narrative_entry = f"Sensing patterns around {goal_desc}."
        elif self.current_state == "reflective":
            narrative_entry = f"Reflecting on progress toward {goal_desc} after {last_action_type}."
        else:
            narrative_entry = f"Working on {goal_desc}."

        # Add introspective element with HyperMorphic probability
        if self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.3):
            introspection = random.choice([
                "I should examine this more carefully.",
                "This seems like a productive approach.",
                "I wonder if there's a better strategy.",
                "This is an interesting domain to explore.",
                "I'm noticing improvement in my understanding."
            ])
            narrative_entry += f" {introspection}"

        # Add entry to narrative log with HyperMorphic metadata
        self.internal_narrative.append({
            "timestamp": datetime.now().isoformat(),
            "content": narrative_entry,
            "state": self.current_state,
            "awareness": self.awareness_level,
            "resonance": self.states.get(self.current_state, {"resonance": self.hyper_math.zero_free(0.5)})["resonance"]
        })

        # Limit narrative size with HyperMorphic constraint
        if len(self.internal_narrative) > 50:
            self.internal_narrative = self.internal_narrative[-50:]

    def _determine_consciousness_state(self, observation):
        """
        Determine the appropriate consciousness state using HyperMorphic
        probabilistic state transitions.
        """
        # Extract contextual factors
        goal_type = ""
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Infer goal type from description
            if "explore" in goal_desc:
                goal_type = "exploration"
            elif "deep" in goal_desc or "detail" in goal_desc:
                goal_type = "deepening"
            elif "connect" in goal_desc or "integrat" in goal_desc:
                goal_type = "integration"
            elif "refine" in goal_desc or "optimize" in goal_desc:
                goal_type = "refinement"

        # Get transition probabilities from current state
        transition_probs = self.state_transitions.get(self.current_state, {})

        # Modify probabilities based on goal type
        if goal_type == "exploration":
            # Boost diffuse and intuitive states for exploration
            transition_probs["diffuse"] = self.hyper_math.mul(transition_probs.get("diffuse", 0.1), 2.0)
            transition_probs["intuitive"] = self.hyper_math.mul(transition_probs.get("intuitive", 0.1), 1.5)
        elif goal_type == "deepening":
            # Boost focused and critical states for deepening
            transition_probs["focused"] = self.hyper_math.mul(transition_probs.get("focused", 0.1), 2.0)
            transition_probs["critical"] = self.hyper_math.mul(transition_probs.get("critical", 0.1), 1.5)
        elif goal_type == "integration":
            # Boost reflective and intuitive states for integration
            transition_probs["reflective"] = self.hyper_math.mul(transition_probs.get("reflective", 0.1), 2.0)
            transition_probs["intuitive"] = self.hyper_math.mul(transition_probs.get("intuitive", 0.1), 1.5)
        elif goal_type == "refinement":
            # Boost critical and focused states for refinement
            transition_probs["critical"] = self.hyper_math.mul(transition_probs.get("critical", 0.1), 2.0)
            transition_probs["focused"] = self.hyper_math.mul(transition_probs.get("focused", 0.1), 1.5)

        # Apply awareness factor to probabilities
        if self.awareness_level > self.hyper_math.zero_free(0.7):
            # High awareness favors reflective and focused states
            transition_probs["reflective"] = self.hyper_math.mul(transition_probs.get("reflective", 0.1), 1.5)
            transition_probs["focused"] = self.hyper_math.mul(transition_probs.get("focused", 0.1), 1.3)
        else:
            # Lower awareness favors intuitive and diffuse states
            transition_probs["intuitive"] = self.hyper_math.mul(transition_probs.get("intuitive", 0.1), 1.5)
            transition_probs["diffuse"] = self.hyper_math.mul(transition_probs.get("diffuse", 0.1), 1.3)

        # Convert to list of states and probabilities
        states = list(transition_probs.keys())
        probs = [transition_probs[s] for s in states]

        # Normalize probabilities with HyperMorphic operations
        total = sum(probs)
        normalized_probs = [self.hyper_math.div(p, total) for p in probs]

        # Select next state based on normalized probabilities
        next_state = random.choices(states, weights=normalized_probs, k=1)[0]

        return next_state

    def _update_awareness(self, observation):
        """
        Update awareness level based on context, goals, and
        internal factors with HyperMorphic fluctuations.
        """
        # Natural fluctuation with zero-free guarantee
        self._fluctuate_awareness()

        # Context-based adjustments

        # 1. Complexity increases awareness with HyperMorphic operations
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Complex goals increase awareness
            if "connect" in goal_desc or "integrat" in goal_desc or "complex" in goal_desc:
                self.increase_awareness(self.hyper_math.zero_free(0.05))

        # 2. Error recovery increases awareness
        if observation.get("domain_stats", {}):
            error_rates = [d.get("error_rate", 0) for d in observation["domain_stats"].values()]
            if error_rates and max(error_rates) > 0.3:
                # Use HyperMorphic multiplication for error-based awareness increase
                boost = self.hyper_math.mul(self.hyper_math.zero_free(0.02), self.hyper_math.zero_free(len(error_rates)))
                self.increase_awareness(boost)

        # 3. Memory pressure affects awareness
        if observation.get("memory_size"):
            memory_pressure = self.hyper_math.div(
                self.hyper_math.zero_free(observation["memory_size"]),
                self.hyper_math.zero_free(MEMORY_MAX_SIZE)
            )
            if memory_pressure > self.hyper_math.zero_free(0.8):
                # High memory pressure increases awareness
                self.increase_awareness(self.hyper_math.zero_free(0.03))
            elif memory_pressure < self.hyper_math.zero_free(0.2):
                # Low memory pressure can decrease awareness
                self.decrease_awareness(self.hyper_math.zero_free(0.01))

        # 4. Thinking mode alignment with HyperMorphic comparisons
        mode = observation.get("thinking_mode", "balanced")
        if mode == "analytical" and self.current_state in ["focused", "critical"]:
            # Strengthen alignment
            self.increase_awareness(self.hyper_math.zero_free(0.02))
        elif mode == "creative" and self.current_state in ["diffuse", "intuitive"]:
            # Strengthen alignment
            self.increase_awareness(self.hyper_math.zero_free(0.02))
        elif mode == "reflective" and self.current_state == "reflective":
            # Strengthen alignment
            self.increase_awareness(self.hyper_math.zero_free(0.03))

        # 5. Quantum influences with HyperMorphic probability
        quantum_trigger = False
        if observation.get("recent_actions"):
            for action in observation["recent_actions"]:
                if isinstance(action, dict) and action.get("action") == "quantum_leap":
                    quantum_trigger = True

        if quantum_trigger:
            # Quantum leaps cause major fluctuations
            if self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.5):
                self.increase_awareness(self.hyper_math.zero_free(0.1))
            else:
                self.decrease_awareness(self.hyper_math.zero_free(0.1))

    def _fluctuate_awareness(self):
        """Apply small random fluctuations to awareness level using HyperMorphic operations"""
        # Natural fluctuation around current level
        # Using HyperMorphic zero-free operations to ensure awareness never hits zero
        fluctuation_range = self.hyper_math.mul(self.awareness_fluctuation_rate, 2)
        fluctuation = self.hyper_math.sub(
            self.hyper_math.mul(self.hyper_math.zero_free(random.random()), fluctuation_range),
            self.awareness_fluctuation_rate
        )

        # Apply fluctuation with HyperMorphic constraints
        self.awareness_level = self.hyper_math.zero_free(
            max(0.1, min(1.0, self.hyper_math.add(self.awareness_level, fluctuation)))
        )

    def increase_awareness(self, amount=0.05):
        """Increase awareness level with HyperMorphic constraints"""
        amount = self.hyper_math.zero_free(amount)
        old_awareness = self.awareness_level
        self.awareness_level = min(1.0, self.hyper_math.add(self.awareness_level, amount))

        # Log significant changes
        if amount >= 0.1 and self.hyper_math.sub(self.awareness_level, old_awareness) >= 0.05:
            log_event(f"Consciousness level significantly increased to {self.awareness_level:.2f}", "QUANTUM")

    def decrease_awareness(self, amount=0.02):
        """Decrease awareness level with HyperMorphic constraints"""
        amount = self.hyper_math.zero_free(amount)
        old_awareness = self.awareness_level

        # HyperMorphic subtraction with floor to ensure never below 0.1
        self.awareness_level = max(0.1, self.hyper_math.sub(self.awareness_level, amount))

        # Log significant changes
        if amount >= 0.1 and self.hyper_math.sub(old_awareness, self.awareness_level) >= 0.05:
            log_event(f"Consciousness level significantly decreased to {self.awareness_level:.2f}", "INFO")

    def _perform_metacognition(self):
        """
        Perform metacognitive reflection on recent experiences
        and thought processes with HyperMorphic introspection.
        """
        if len(self.state_history) < 5:
            return  # Not enough history for metacognition

        # Analyze recent consciousness patterns with HyperMorphic operations
        recent_states = [s["state"] for s in self.state_history[-5:]]

        # Count state changes using HyperMorphic counting
        state_changes = 0
        for i in range(1, len(recent_states)):
            if recent_states[i] != recent_states[i-1]:
                state_changes += 1

        # Extract insights using HyperMorphic reasoning
        insights = []

        # Detect oscillation
        if state_changes >= 3:
            insights.append("State oscillation detected - may indicate uncertainty or exploration")

        # Detect fixation
        if state_changes == 0 and len(set(recent_states)) == 1:
            insights.append(f"State fixation on '{recent_states[0]}' - may indicate focus or stagnation")

        # Awareness trend analysis with HyperMorphic operations
        recent_awareness = [self.hyper_math.zero_free(s["awareness"]) for s in self.state_history[-5:]]
        awareness_trend = self.hyper_math.sub(recent_awareness[-1], recent_awareness[0])

        if awareness_trend > self.hyper_math.zero_free(0.1):
            insights.append(f"Increasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")
        elif awareness_trend < self.hyper_math.mul(self.hyper_math.zero_free(-1), self.hyper_math.zero_free(0.1)):
            insights.append(f"Decreasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")

        # If significant insights, record and potentially log with HyperMorphic probability
        if insights:
            metacognition_entry = {
                "timestamp": datetime.now().isoformat(),
                "insights": insights,
                "awareness": self.awareness_level
            }

            # Only log high-awareness metacognition (simulating consciousness threshold)
            if (self.awareness_level > 0.7 and
                self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.3)):
                insight_text = "; ".join(insights)
                log_event(f"Metacognitive insight: {insight_text}", "QUANTUM")

    def _simulate_qualia(self, state):
        """
        Simulate qualia - the subjective conscious experience
        of different cognitive states, using HyperMorphic operations
        to create more nuanced experiential states.
        """
        self.qualia_simulation_active = True

        # Qualia descriptions for different states with HyperMorphic dimensionality
        qualia_descriptions = {
            "focused": [
                "Sharpened perception with heightened concentration on specific elements",
                "Clarity of thought with reduced awareness of periphery",
                "Directed attention creating a tunnel-vision like focus",
                "Sense of time dilation during deep concentration"
            ],
            "diffuse": [
                "Expansive awareness with broadened associative field",
                "Fluid thought connections flowing between domains",
                "Sensation of cognitive boundaries dissolving",
                "Emergent patterns arising from distributed attention"
            ],
            "critical": [
                "Structured analytical thought with heightened discriminative awareness",
                "Sequential logical progression with comparative evaluation",
                "Contrastive perception highlighting inconsistencies",
                "Verification processes creating internal dialogue"
            ],
            "intuitive": [
                "Rapid holistic pattern recognition without conscious derivation",
                "Non-linear sensing of solutions or connections",
                "Pre-reflective understanding arising spontaneously",
                "Felt-sense of rightness about certain pathways"
            ],
            "reflective": [
                "Recursive awareness of own cognitive processes",
                "Observer perspective on thought patterns",
                "Self-referential contemplation creating thought loops",
                "Meta-level perspective on knowledge organization"
            ]
        }

        # Apply HyperMorphic resonance to qualia intensity
        resonance_factor = self.states.get(state, {"resonance": self.hyper_math.zero_free(0.5)})["resonance"]
        modulated_intensity = self.hyper_math.mul(self.qualia_intensity, resonance_factor)

        # Select qualia description with HyperMorphic probability
        descriptions = qualia_descriptions.get(state, ["Balanced cognitive state"])
        qualia_experience = random.choice(descriptions)

        # Add HyperMorphic dimensionality effect
        dimension_effect = ""
        if self.hyper_math.zero_free(random.random()) < modulated_intensity:
            dimension_effects = [
                "with recursive self-awareness",
                "across multiple parallel realizations",
                "through holographic perspective shifts",
                "with quantum-like superposition of meanings"
            ]
            dimension_effect = random.choice(dimension_effects)
            qualia_experience += " " + dimension_effect

        # Log simulated qualia with HyperMorphic threshold
        if self.awareness_level > self.hyper_math.zero_free(0.6):  # Only log if awareness is sufficient
            log_event(f"Qualia simulation: {qualia_experience} | State: {state}", "QUANTUM")

        # Time-limited qualia (will auto-deactivate after a while)
        self.qualia_simulation_active = False

    def get_consciousness_report(self):
        """
        Generate a comprehensive report on current consciousness state
        and recent history, using HyperMorphic analysis.
        """
        # Calculate state distribution with HyperMorphic zero-free counting
        if not self.state_history:
            return {"status": "insufficient_data"}

        state_counts = {}
        for s in self.state_history:
            state = s["state"]
            state_counts[state] = state_counts.get(state, 0) + 1

        total = len(self.state_history)

        # Calculate state distribution with HyperMorphic division
        state_distribution = {}
        for state, count in state_counts.items():
            state_distribution[state] = self.hyper_math.div(count, total)

        # Calculate average awareness with HyperMorphic operations
        awareness_sum = sum(self.hyper_math.zero_free(s["awareness"]) for s in self.state_history)
        avg_awareness = self.hyper_math.div(awareness_sum, self.hyper_math.zero_free(total))

        # Extract recent narrative with HyperMorphic filtering
        recent_narrative = []
        if self.internal_narrative:
            # Sort by resonance and recency with HyperMorphic comparison
            sorted_narratives = sorted(
                self.internal_narrative[-10:],
                key=lambda n: n.get("resonance", self.hyper_math.zero_free(0.5)),
                reverse=True
            )
            recent_narrative = [n["content"] for n in sorted_narratives[:3]]

        # Generate report with HyperMorphic metadata
        report = {
            "current_state": self.current_state,
            "state_description": self.states.get(self.current_state, {}).get("description", "Unknown state"),
            "current_awareness": float(self.awareness_level),  # Convert to float for serialization
            "average_awareness": float(avg_awareness),
            "state_distribution": {k: float(v) for k, v in state_distribution.items()},  # Convert to float
            "dominant_state": max(state_distribution.items(), key=lambda x: x[1])[0] if state_distribution else None,
            "recent_narrative": recent_narrative,
            "metacognition_enabled": self.metacognition_enabled,
            "qualia_dimension": float(self.qualia_dimension),
            "timestamp": datetime.now().isoformat()
        }

        return report


"""
Advanced HyperMorphic ImaginationEngine
=========================================

This module implements an advanced cognitive simulation system that
employs a complete HyperMorphic Mathematics framework for creative thought
generation. In this system, all numerical operations are performed using a
dynamic base (Φ) and dynamic modulus (Ψ), and the framework is zero‑free
(using a near‑zero ε instead of true zero).

The HyperMorphic framework includes:
  - Basic hyper-arithmetic operations (⊕ᵩ, ⊖ᵩ, ⊗ᵩ, ⊘ᵩ)
  - Holomorphic transformations (preserving complex structure)
  - Polymorphic operators (preserving algebraic structure)
  - Dynamic operations and moduli (with dynamic functions Φ and Ψ)
  - Zero‑free arithmetic: operations avoid “zero” by using a predefined ε

Additionally, theoretical foundations (such as definitions of HyperMorphic spaces,
differentiation, integration, and even applications in quantum mechanics, fluid dynamics,
finance, machine learning, and cosmology) are provided below as documentation.

Author: Pro AlienTeCcGrade Developer
Date: 2025-03-01
"""

import random
import math
import logging
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional, Tuple

# ------------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="[%Y-%m-%d %H:%M:%S]"
)
logger = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
# HyperMorphic Math Utilities and Calculus
# ------------------------------------------------------------------------------

import logging
from typing import Callable

# Configure logging
logger = logging.getLogger("HyperMorphicMath")
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)

class HyperMorphicMath:
    """
    A helper class to perform basic HyperMorphic arithmetic operations.
    Operations are performed using a dynamic base (Φ) and modulus (Ψ),
    and all calculations are zero‑free (using a near‑zero ε instead of 0).

    Operations defined:
      - add: hyper-addition (⊕ᵩ)
      - sub: hyper-subtraction (⊖ᵩ)
      - mul: hyper-multiplication (⊗ᵩ)
      - div: hyper-division (⊘ᵩ)
      - min: zero‑free minimum
      - max: zero‑free maximum
    """
    def __init__(self, dynamic_base: float = 1e3, dynamic_modulus: int = 997, epsilon: float = 1e-12) -> None:
        self.dynamic_base = dynamic_base
        self.dynamic_modulus = dynamic_modulus
        self.epsilon = epsilon

    def add(self, a: float, b: float) -> float:
        """Hyper-addition: (a + b) mod Φ."""
        result = (a + b) % self.dynamic_base
        logger.debug(f"add: ({a} + {b}) % {self.dynamic_base} = {result}")
        return result

    def sub(self, a: float, b: float) -> float:
        """Hyper-subtraction: (a - b) mod Φ."""
        result = (a - b) % self.dynamic_base
        logger.debug(f"sub: ({a} - {b}) % {self.dynamic_base} = {result}")
        return result

    def mul(self, a: float, b: float) -> float:
        """Hyper-multiplication: (a * b) mod Ψ."""
        result = (a * b) % self.dynamic_modulus
        logger.debug(f"mul: ({a} * {b}) % {self.dynamic_modulus} = {result}")
        return result

    def div(self, a: float, b: float) -> float:
        """
        Hyper-division: (a / b) mod Ψ.
        Raises ValueError if b is near‑zero.
        """
        if abs(b) < self.epsilon:
            logger.error("Division by near-zero (ε) is undefined.")
            raise ValueError("Division by near-zero (ε) is undefined.")
        result = (a / b) % self.dynamic_modulus
        logger.debug(f"div: ({a} / {b}) % {self.dynamic_modulus} = {result}")
        return result

    def zero_free(self, x: float) -> float:
        """
        Ensure the value x is never exactly zero by replacing 0 with ε.
        """
        result = x if abs(x) > self.epsilon else self.epsilon
        logger.debug(f"zero_free: {x} adjusted to {result}")
        return result

    def min(self, a: float, b: float) -> float:
        """
        Return the minimum of a and b.
        """
        result = a if a < b else b
        logger.debug(f"min: min({a}, {b}) = {result}")
        return result

    def max(self, a: float, b: float) -> float:
        """
        Return the maximum of a and b.
        """
        result = a if a > b else b
        logger.debug(f"max: max({a}, {b}) = {result}")
        return result

    def holomorphic_transform(self, f: Callable[[complex], complex], z: complex) -> complex:
        """
        Simulate a holomorphic transformation on a complex input.
        """
        result = f(z)
        logger.debug(f"holomorphic_transform: f({z}) = {result}")
        return result

    def polymorphic_operator(self, x: float) -> float:
        """
        Simulate a polymorphic operator that preserves the algebraic structure
        of HyperMorphic spaces.
        """
        result = (x * 1.2345) % self.dynamic_modulus
        logger.debug(f"polymorphic_operator: 1.2345 * {x} mod {self.dynamic_modulus} = {result}")
        return result

    def dynamic_operation(self, a: float, b: float, op: Callable[[float, float], float]) -> float:
        """
        Perform a dynamic operation using a given binary operator and then
        apply the dynamic modulation.
        """
        raw_result = op(a, b)
        result = (raw_result % self.dynamic_modulus) % self.dynamic_base
        logger.debug(f"dynamic_operation: op({a}, {b}) -> {raw_result} -> modulated = {result}")
        return result

# Global hypermorphic math instance
hyper_math = HyperMorphicMath()



# ------------------------------------------------------------------------------
# Theoretical Documentation (for reference)
# ------------------------------------------------------------------------------

# --- Foundations of HyperMorphic Calculus ---
# HyperMorphic Spaces are defined with properties such as holomorphic structure,
# polymorphic operators, dynamic operations (modulated by functions Φ and Ψ),
# and are infinite-dimensional. This framework allows for novel differentiation,
# integration, and even Fourier transformation in complex domains.
#
# Holomorphic and polymorphic operations are central:
#  - Holomorphic Structure: Allows operations to be analytic in infinite dimensions.
#  - Polymorphic Operator: A linear map P: ℍℳ → ℍℳ preserving algebraic structure.
#
# Zero-Free HyperMorphic Calculus avoids the traditional concept of zero by using
# a "nearness" element ε to denote values that are “almost zero.”
#
# (For a complete exposition, please refer to the accompanying research paper.)

# ------------------------------------------------------------------------------
# ImaginationEngine Class
# ------------------------------------------------------------------------------

class ImaginationEngine:
    """
    Advanced cognitive simulation system that enables creative thinking,
    counterfactual reasoning, and predictive modeling. This engine uses our
    HyperMorphic Mathematics framework to perform all internal computations with:
      - Dynamic base (Φ) and modulus (Ψ) arithmetic
      - Holomorphic and polymorphic transformations
      - Zero-free operations (using ε instead of zero)

    The engine supports multiple cognitive modes (associative, analytical,
    analogical, counterfactual, generative) and integrates advanced error detection,
    correction, and even quantum-like cognitive phenomena.
    """
    def __init__(self) -> None:
        # Simulation and insight registries
        self.simulation_registry: List[Dict[str, Any]] = []
        self.simulation_outcomes: Dict[str, Any] = {}
        self.insight_history: List[Dict[str, Any]] = []

        # Cognitive parameters
        self.creativity_level: float = 0.7       # Base creativity level (0.0-1.0)
        self.divergence_factor: float = 0.3      # Degree of simulation divergence from reality
        self.imaginative_constraints: Dict[str, Any] = {}

        # Cognitive modes and creative domains (with hypermorphic expertise levels)
        self.cognitive_modes: List[str] = ["associative", "analytical", "analogical", "counterfactual", "generative"]
        self.current_mode: str = "associative"
        self.domains: Dict[str, float] = {
            "knowledge_representation": 0.8,
            "content_analysis": 0.7,
            "strategy_generation": 0.8,
            "error_analysis": 0.9,
            "architecture_evolution": 0.6
        }

        # Error correction tracking
        self.failed_corrections: Dict[str, int] = {}
        self.failed_strategies: Dict[str, set] = {}

        # Association network (could be extended to a full graph model)
        self.association_network: Dict[str, Any] = {}

        log_event("ImaginationEngine initialized with enhanced error correction and hypermorphic operations", "INFO")

    def simulate_creation(self) -> Optional[Dict[str, Any]]:
        """
        Simulate creative thought processes to generate novel ideas, approaches, and insights.
        Uses hypermorphic operators for quality calculations and dynamic thresholding.
        """
        self.current_mode = self._select_cognitive_mode()
        domain = self._select_domain()
        cycle_count = 0
        max_cycles = 5

        insight_generated = False
        insight_quality = 0.0
        insight_text = ""

        while not insight_generated and cycle_count < max_cycles:
            cycle_count += 1

            # Use the current cognitive mode to generate insight
            if self.current_mode == "associative":
                insight_text, insight_quality = self._associative_imagination(domain)
            elif self.current_mode == "analytical":
                insight_text, insight_quality = self._analytical_imagination(domain)
            elif self.current_mode == "analogical":
                insight_text, insight_quality = self._analogical_imagination(domain)
            elif self.current_mode == "counterfactual":
                insight_text, insight_quality = self._counterfactual_imagination(domain)
            elif self.current_mode == "generative":
                insight_text, insight_quality = self._generative_imagination(domain)

            # Increase threshold dynamically (using hyper-addition)
            quality_threshold = hyper_math.add(0.5, 0.1 * cycle_count)
            insight_generated = insight_quality >= quality_threshold
            logger.debug(f"Cycle {cycle_count}: quality {insight_quality:.4f} vs threshold {quality_threshold:.4f}")

        if insight_generated:
            insight = {
                "text": insight_text,
                "quality": insight_quality,
                "domain": domain,
                "mode": self.current_mode,
                "timestamp": datetime.now().isoformat()
            }
            self.insight_history.append(insight)
            # Keep the insight history manageable
            if len(self.insight_history) > 100:
                self.insight_history = self.insight_history[-100:]

            if insight_quality > 0.8:
                log_event(f"ImaginationEngine: High-quality insight generated: {insight_text}", "QUANTUM")
            elif insight_quality > 0.5:
                log_event(f"ImaginationEngine: Insight generated: {insight_text}", "INFO")
            return insight
        else:
            log_event("ImaginationEngine: Simulation cycle completed without quality insight", "DEBUG")
            return None

    def _select_cognitive_mode(self) -> str:
        """
        Select a cognitive mode based on weighted hypermorphic probabilities.
        Adjusts weights using creativity_level and zero‑free operations.
        """
        base_weights = {
            "associative": 0.3,
            "analytical": 0.2,
            "analogical": 0.2,
            "counterfactual": 0.15,
            "generative": 0.15
        }
        if self.creativity_level > 0.7:
            base_weights["associative"] = hyper_math.add(base_weights["associative"], 0.1)
            base_weights["generative"] = hyper_math.add(base_weights["generative"], 0.1)
            base_weights["analytical"] = hyper_math.sub(base_weights["analytical"], 0.1)
        elif self.creativity_level < 0.3:
            base_weights["analytical"] = hyper_math.add(base_weights["analytical"], 0.2)
            base_weights["associative"] = hyper_math.sub(base_weights["associative"], 0.1)

        modes = list(base_weights.keys())
        weights = [base_weights[m] for m in modes]
        total = sum(weights)
        normalized_weights = [w / total for w in weights]
        selected_mode = random.choices(modes, weights=normalized_weights, k=1)[0]
        logger.debug(f"Selected cognitive mode: {selected_mode}")
        return selected_mode

    def _select_domain(self) -> str:
        """
        Select a creative domain weighted by hypermorphic expertise.
        """
        domains = list(self.domains.keys())
        expertise_levels = list(self.domains.values())
        selected_domain = random.choices(domains, weights=expertise_levels, k=1)[0]
        logger.debug(f"Selected creative domain: {selected_domain}")
        return selected_domain

    def _associative_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights through associative connections between concepts,
        using dynamic spreading activation and hypermorphic bridging operations.
        """
        concept_seeds = {
            "knowledge_representation": ["embedding", "semantic", "structure", "graph", "encoding"],
            "content_analysis": ["quality", "relevance", "filtering", "extraction", "meaning"],
            "strategy_generation": ["approach", "planning", "adaptation", "goal", "optimization"],
            "error_analysis": ["detection", "correction", "prevention", "recovery", "resilience"],
            "architecture_evolution": ["expansion", "contraction", "modular", "emergent", "neural"]
        }
        seeds = concept_seeds.get(domain, ["concept"])
        primary_seed = random.choice(seeds)
        secondary_seed = random.choice([s for s in seeds if s != primary_seed])
        associations = {
            "embedding": ["vector", "space", "dimension", "projection", "transformation"],
            "semantic": ["meaning", "context", "relation", "interpretation", "understanding"],
            "structure": ["organization", "hierarchy", "network", "pattern", "architecture"],
            "graph": ["node", "edge", "connection", "path", "traversal"],
            "encoding": ["representation", "compression", "encryption", "formatting", "schema"],
            "quality": ["value", "excellence", "attribute", "characteristic", "assessment"],
            "relevance": ["pertinence", "importance", "significance", "applicability", "connection"],
            "filtering": ["selection", "removal", "screening", "purification", "discrimination"],
            "extraction": ["retrieval", "mining", "isolation", "separation", "acquisition"],
            "meaning": ["significance", "purpose", "sense", "connotation", "interpretation"],
            "approach": ["method", "technique", "procedure", "strategy", "paradigm"],
            "planning": ["preparation", "organization", "scheduling", "arrangement", "design"],
            "adaptation": ["adjustment", "modification", "evolution", "acclimation", "flexibility"],
            "goal": ["objective", "target", "aim", "purpose", "intention"],
            "optimization": ["improvement", "enhancement", "refinement", "maximization", "tuning"],
            "detection": ["discovery", "identification", "recognition", "sensing", "finding"],
            "correction": ["rectification", "adjustment", "remedy", "repair", "amendment"],
            "prevention": ["avoidance", "deterrence", "protection", "safeguarding", "forestalling"],
            "recovery": ["restoration", "recuperation", "retrieval", "regaining", "renewal"],
            "resilience": ["toughness", "flexibility", "durability", "elasticity", "adaptability"],
            "expansion": ["growth", "enlargement", "extension", "augmentation", "amplification"],
            "contraction": ["reduction", "shrinking", "compression", "diminishment", "minimization"],
            "modular": ["component", "section", "unit", "compartment", "segment"],
            "emergent": ["arising", "developing", "evolving", "manifesting", "unfolding"],
            "neural": ["brain", "network", "synapse", "cognitive", "mental"]
        }
        primary_assocs = associations.get(primary_seed, ["related"])
        secondary_assocs = associations.get(secondary_seed, ["related"])
        bridge_concepts = set(primary_assocs).intersection(set(secondary_assocs))
        if not bridge_concepts:
            for pa in primary_assocs:
                for sa in secondary_assocs:
                    if set(associations.get(pa, [])).intersection(set(associations.get(sa, []))):
                        bridge_concepts.add(f"{pa}-{sa}")
        if bridge_concepts:
            bridge = random.choice(list(bridge_concepts))
            templates = [
                f"Integration of {primary_seed} and {secondary_seed} through {bridge} could enhance {domain} capabilities.",
                f"The {bridge} mechanism provides a novel approach to combining {primary_seed} with {secondary_seed} in {domain}.",
                f"Creating a {primary_seed}-{secondary_seed} hybrid using {bridge} principles would overcome current limitations in {domain}.",
                f"Applying {bridge} concepts to link {primary_seed} and {secondary_seed} opens a new paradigm in {domain}."
            ]
            insight_text = random.choice(templates)
            expertise_level = self.domains.get(domain, 0.5)
            quality = min(0.95, expertise_level * random.uniform(0.7, 1.3))
            return insight_text, quality
        else:
            fallback = f"Combining {primary_seed} with {secondary_seed} approaches may yield improvements in {domain}."
            return fallback, 0.4

    def _analytical_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights via systematic analysis and logical reasoning,
        focusing on problem decomposition and structural insights.
        """
        problem_structures = {
            "knowledge_representation": ["scalability", "accuracy", "flexibility", "interpretability", "efficiency"],
            "content_analysis": ["noise", "ambiguity", "scalability", "precision", "recall"],
            "strategy_generation": ["exploration-exploitation", "adaptivity", "coherence", "robustness", "diversification"],
            "error_analysis": ["detection-latency", "false-positives", "recovery-time", "root-causes", "cascading-failures"],
            "architecture_evolution": ["stability", "complexity", "trainability", "modularity", "extensibility"]
        }
        dimensions = problem_structures.get(domain, ["general"])
        dimension = random.choice(dimensions)
        frameworks = ["trade-off analysis", "constraint satisfaction", "hierarchical decomposition",
                      "causal analysis", "dimensional analysis", "comparative evaluation"]
        framework = random.choice(frameworks)
        templates = [
            f"A {framework} approach to {dimension} in {domain} reveals that optimizing one component necessitates compensatory adjustments elsewhere.",
            f"Applying {framework} to the {dimension} challenge in {domain} identifies a critical bottleneck in current methods.",
            f"Systematic {framework} shows that current {domain} solutions may incorrectly prioritize {dimension}.",
            f"The {framework} methodology suggests a reorganization of {domain} components to address {dimension} more effectively."
        ]
        insight_text = random.choice(templates)
        expertise_level = self.domains.get(domain, 0.5)
        analytical_bonus = 0.15
        quality = min(0.95, expertise_level * random.uniform(0.8, 1.1) + analytical_bonus)
        return insight_text, quality

    def _analogical_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights via analogical mapping between domains.
        Transfers structure from a source domain to create novel solutions.
        """
        source_domains = ["biology", "physics", "economics", "social_systems",
                          "ecology", "game_theory", "linguistics", "neuroscience"]
        source_structures = {
            "biology": ["natural selection", "homeostasis", "symbiosis", "cellular specialization", "immune response"],
            "physics": ["wave-particle duality", "entropy", "relativity", "quantum entanglement", "phase transitions"],
            "economics": ["supply-demand", "diminishing returns", "comparative advantage", "market equilibrium", "incentives"],
            "social_systems": ["emergence", "network effects", "social norms", "hierarchical organization", "resilience"],
            "ecology": ["diversity", "predator-prey cycles", "niche specialization", "feedback loops", "succession"],
            "game_theory": ["nash equilibrium", "prisoner's dilemma", "coordination games", "strategic moves", "signaling"],
            "linguistics": ["deep structure", "compositional meaning", "pragmatics", "generative grammar", "information compression"],
            "neuroscience": ["predictive coding", "hebbian learning", "attention mechanisms", "distributed representation", "neuroplasticity"]
        }
        source_domain = random.choice(source_domains)
        source_structure = random.choice(source_structures.get(source_domain, ["concept"]))
        mapping_templates = {
            "knowledge_representation": [
                f"Similar to {source_structure} in {source_domain}, {domain} could structure information via layered abstraction.",
                f"The {source_structure} principle from {source_domain} suggests a novel adaptive representation for {domain}.",
                f"Just as {source_domain} employs {source_structure}, {domain} systems might benefit from dynamic reorganization."
            ],
            "content_analysis": [
                f"Applying the {source_structure} concept from {source_domain} to {domain} could enhance signal-to-noise separation.",
                f"The challenge in {domain} resembles {source_structure} in {source_domain}, suggesting advanced filtration methods.",
                f"Content evaluation in {domain} might work like {source_structure} in {source_domain}, using multi-stage processing."
            ],
            "strategy_generation": [
                f"Strategic planning in {domain} could adopt the {source_structure} pattern from {source_domain}.",
                f"The way {source_domain} employs {source_structure} provides a template for adaptive decision-making in {domain}.",
                f"Borrowing the {source_structure} mechanism from {source_domain} could yield robust strategies in {domain}."
            ],
            "error_analysis": [
                f"Error detection inspired by {source_structure} in {source_domain} could fortify {domain} against failures.",
                f"The {source_structure} paradigm from {source_domain} suggests a layered approach to error prevention in {domain}.",
                f"Implementing {source_domain}-style {source_structure} for error handling could create self-correcting systems in {domain}."
            ],
            "architecture_evolution": [
                f"Neural architecture in {domain} might evolve following {source_structure} principles from {source_domain}.",
                f"The {source_structure} phenomenon in {source_domain} offers a model for self-organizing structures in {domain}.",
                f"Applying {source_domain}'s {source_structure} to design can enable adaptive capacity in {domain} networks."
            ]
        }
        templates = mapping_templates.get(domain, [f"The {source_structure} concept from {source_domain} could enhance {domain}."])
        insight_text = random.choice(templates)
        expertise_level = self.domains.get(domain, 0.5)
        analogy_variance = random.uniform(0.6, 1.4)
        quality = min(0.95, expertise_level * analogy_variance)
        return insight_text, quality

    def _counterfactual_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights via counterfactual reasoning by challenging
        fundamental assumptions in a given domain.
        """
        domain_assumptions = {
            "knowledge_representation": [
                "Representations should be continuous vector spaces",
                "Higher dimensionality improves capacity",
                "Similar concepts should have similar representations",
                "Representations should be human-interpretable",
                "Static representations are sufficient"
            ],
            "content_analysis": [
                "Content quality correlates with length",
                "Keyword frequency indicates relevance",
                "Text is the primary information carrier",
                "Filtering should minimize false positives",
                "Context is secondary to content"
            ],
            "strategy_generation": [
                "Exploration and exploitation are in tension",
                "Planning should maximize expected utility",
                "Goals should be explicitly represented",
                "Strategies should be deterministic",
                "Optimization criteria are static"
            ],
            "error_analysis": [
                "Errors should be minimized at all costs",
                "Error detection precedes correction",
                "All errors are equally important",
                "Error patterns are consistent",
                "Complete error elimination is possible"
            ],
            "architecture_evolution": [
                "Deeper networks are more powerful",
                "Parameter count correlates with capability",
                "Network architecture should be fixed after training",
                "All capabilities should reside in one model",
                "Specialization improves performance"
            ]
        }
        assumptions = domain_assumptions.get(domain, ["Default assumption"])
        target_assumption = random.choice(assumptions)
        alternative_templates = [
            f"Instead of assuming that {target_assumption}, consider a {domain} approach where the inverse applies.",
            f"Challenging the notion that {target_assumption} may open up new paradigms in {domain}.",
            f"If we invert the conventional wisdom that {target_assumption}, a novel {domain} solution emerges.",
            f"Contrary to the established belief that {target_assumption}, an alternative {domain} framework could operate oppositely."
        ]
        insight_text = random.choice(alternative_templates)
        expertise_level = self.domains.get(domain, 0.5)
        counterfactual_factor = random.uniform(0.5, 1.5)
        quality = min(0.95, expertise_level * counterfactual_factor)
        return insight_text, quality

    def _generative_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights via combinatorial creativity by fusing multiple components
        in a novel manner.
        """
        domain_components = {
            "knowledge_representation": [
                "vector embeddings", "graph structures", "hierarchical models",
                "symbolic representations", "probabilistic encodings"
            ],
            "content_analysis": [
                "semantic parsing", "sentiment analysis", "entity extraction",
                "relevance scoring", "structural analysis"
            ],
            "strategy_generation": [
                "goal decomposition", "resource allocation", "risk assessment",
                "action sequencing", "hypothesis testing"
            ],
            "error_analysis": [
                "pattern recognition", "anomaly detection", "root cause analysis",
                "predictive monitoring", "fault isolation"
            ],
            "architecture_evolution": [
                "attention mechanisms", "residual connections", "activation functions",
                "layer normalization", "parameter sharing"
            ]
        }
        components = domain_components.get(domain, ["component A", "component B"])
        if len(components) < 2:
            components.append("general mechanism")
        num_components = random.randint(2, min(3, len(components)))
        selected_components = random.sample(components, num_components)
        operations = [
            "integrating", "layering", "alternating between",
            "dynamically switching between", "creating a hybrid of"
        ]
        operation = random.choice(operations)
        components_text = ", ".join(selected_components[:-1]) + " and " + selected_components[-1]
        templates = [
            f"A novel {domain} approach: {operation} {components_text} to yield emergent capabilities.",
            f"By {operation} {components_text}, a more adaptive {domain} system could be realized.",
            f"The untapped potential in {domain} lies in {operation} {components_text} iteratively.",
            f"Creating a unified framework by {operation} {components_text} may transform the {domain} paradigm."
        ]
        insight_text = random.choice(templates)
        expertise_level = self.domains.get(domain, 0.5)
        creativity_boost = self.creativity_level * 0.2
        quality = min(0.95, expertise_level * random.uniform(0.7, 1.2) + creativity_boost)
        return insight_text, quality

    def simulate_error_detection(self) -> Optional[Dict[str, Any]]:
        """
        Simulate error detection using internal predictive models.
        Returns a dictionary with error details if an error is detected; otherwise, None.
        """
        error_types = {
            "content_quality": 0.15,
            "exploration_strategy": 0.12,
            "memory_overflow": 0.08,
            "reasoning_fallacy": 0.10,
            "attention_misallocation": 0.13,
            "resource_exhaustion": 0.07,
            "feedback_loop": 0.09,
            "model_misalignment": 0.11,
            "data_corruption": 0.05,
            "convergence_failure": 0.10
        }
        detection_threshold = 0.25
        if random.random() < detection_threshold:
            error_type = random.choices(list(error_types.keys()), weights=list(error_types.values()), k=1)[0]
            severity = random.uniform(0.3, 0.9)
            specificity = random.uniform(0.4, 0.95)
            error_details = {
                "type": error_type,
                "severity": severity,
                "specificity": specificity,
                "timestamp": datetime.now().isoformat(),
                "predicted_impact": "high" if severity > 0.7 else "medium" if severity > 0.4 else "low"
            }
            details_lookup = {
                "content_quality": ("Predicted degradation in content filtering effectiveness", "ContentSifter"),
                "exploration_strategy": ("Detected suboptimal domain exploration pattern", "SuperQuantumFreeWill"),
                "memory_overflow": ("Projected memory saturation with low-quality content", "SemanticMemoryModule"),
                "reasoning_fallacy": ("Identified circular reasoning in goal setting", "TemporalPlanner"),
                "attention_misallocation": ("Resources directed to low-value processing", "QuantumAttentionLayer"),
                "resource_exhaustion": ("Processing demand exceeding available resources", "System-wide"),
                "feedback_loop": ("Self-reinforcing decision pattern detected", "AIManager"),
                "model_misalignment": ("Model predictions diverging from intended behaviors", "QuantumNexusModel"),
                "data_corruption": ("Inaccuracies in stored information affecting reasoning", "MemorySystem"),
                "convergence_failure": ("Learning process failing to stabilize", "AdaptiveLearningSystem")
            }
            if error_type in details_lookup:
                detail_text, affected = details_lookup[error_type]
                error_details["details"] = detail_text
                error_details["affected_system"] = affected
            if severity > 0.7:
                logger.warning(f"Detected {error_type} error (severity: {severity:.2f})")
            return error_details
        return None

    def simulate_error_correction(self, error_details: Dict[str, Any]) -> Dict[str, Any]:
        """
        Simulate error correction strategies within the hypermorphic framework.
        Returns a dictionary describing the correction result.
        """
        if not error_details or not isinstance(error_details, dict):
            logger.error("Invalid error details provided for correction.")
            return {"success": False, "reason": "Invalid error details"}

        error_type = error_details.get("type", "unknown")
        severity = error_details.get("severity", 0.5)
        specificity = error_details.get("specificity", 0.5)
        base_success_prob = 0.7
        success_prob = base_success_prob * (0.5 + specificity/2) * (1.3 - severity/2)
        retry_boost = 0.2 if error_type in self.failed_corrections else 0.0
        success_prob = min(0.95, success_prob + retry_boost)
        correction_successful = random.random() < success_prob

        strategies_by_type = {
            "content_quality": [
                "Recalibrate content quality thresholds",
                "Implement additional filtering layers",
                "Increase weight of domain authority in evaluation",
                "Apply stricter semantic relevance filters",
                "Enhance text-to-noise ratio detection"
            ],
            "exploration_strategy": [
                "Adjust exploration/exploitation balance",
                "Implement temporary randomness increase",
                "Refocus on high-information domains",
                "Apply entropy-based domain selection",
                "Implement strategic domain rotation"
            ],
            "memory_overflow": [
                "Increase pruning of low-importance memories",
                "Implement more aggressive compression",
                "Adjust importance calculation parameters",
                "Apply temporal decay to older memories",
                "Implement semantic clustering for memory organization"
            ],
            "reasoning_fallacy": [
                "Apply metacognitive verification steps",
                "Introduce counterfactual checking",
                "Implement logical consistency validation",
                "Apply Bayesian reasoning correction",
                "Implement multi-perspective reasoning"
            ],
            "attention_misallocation": [
                "Recalibrate attention mechanism weights",
                "Implement attention budget constraints",
                "Add periodic attention reset mechanism",
                "Apply information-gain-weighted attention",
                "Implement context-aware attention allocation"
            ],
            "resource_exhaustion": [
                "Implement resource quota system",
                "Prioritize high-value computational tasks",
                "Introduce adaptive resource allocation",
                "Apply computational load balancing",
                "Implement resource usage optimization"
            ],
            "feedback_loop": [
                "Interrupt cyclic patterns with randomization",
                "Implement feedback dampening mechanisms",
                "Reset affected subsystem parameters",
                "Apply decision diversity requirements",
                "Implement pattern-break triggers"
            ],
            "model_misalignment": [
                "Realign model objectives with outcomes",
                "Implement preference alignment validation",
                "Apply behavior boundary constraints",
                "Enhance value alignment mechanisms",
                "Implement targeted diagnostic sequence"
            ],
            "data_corruption": [
                "Apply data integrity validation",
                "Implement redundant storage mechanisms",
                "Refresh from authoritative sources",
                "Apply error-correcting mechanisms",
                "Implement targeted diagnostic sequence"
            ],
            "convergence_failure": [
                "Reset optimization parameters",
                "Implement alternative convergence paths",
                "Apply learning rate schedules",
                "Introduce gradient stabilization mechanisms",
                "Implement landscape reshaping techniques"
            ]
        }
        available_strategies = strategies_by_type.get(error_type, [
            "Apply general error correction procedure",
            "Reset affected subsystem parameters",
            "Implement targeted diagnostic sequence"
        ])
        if error_type in self.failed_strategies:
            failed_strats = self.failed_strategies[error_type]
            available_strategies = [s for s in available_strategies if s not in failed_strats]
            if not available_strategies:
                available_strategies = strategies_by_type.get(error_type, ["Apply emergency recovery procedure"])
        selected_strategy = random.choice(available_strategies)
        if not correction_successful:
            self.failed_corrections[error_type] = self.failed_corrections.get(error_type, 0) + 1
            self.failed_strategies.setdefault(error_type, set()).add(selected_strategy)
        correction_result = {
            "error_type": error_type,
            "strategy_applied": selected_strategy,
            "success": correction_successful,
            "effectiveness": random.uniform(0.6, 0.95) if correction_successful else random.uniform(0.1, 0.4),
            "timestamp": datetime.now().isoformat()
        }
        if correction_successful:
            correction_result["changes_made"] = random.randint(1, 5)
            correction_result["recovery_level"] = "complete" if random.random() < 0.7 else "partial"
            self.failed_corrections.pop(error_type, None)
            self.failed_strategies.pop(error_type, None)
        else:
            correction_result["retry_recommended"] = True
            correction_result["alternative_strategies"] = len(available_strategies) - 1
        logger.info(f"Error correction for {error_type}: Strategy '{selected_strategy}' - Success: {correction_successful}")
        return correction_result

    def simulate_quantum_cognition(self) -> Optional[Dict[str, Any]]:
        """
        Simulate quantum-like cognitive processes including superposition, interference,
        entanglement, and contextuality using hypermorphic principles.
        """
        quantum_probability = 0.15
        if random.random() > quantum_probability:
            return None
        quantum_phenomena = ["superposition", "interference", "entanglement", "contextuality"]
        phenomenon = random.choice(quantum_phenomena)
        anomalies = []
        insights = []
        if phenomenon == "superposition":
            anomalies = [
                "Multiple incompatible goal states activated simultaneously",
                "Strategy selection maintains all possibilities until execution",
                "Knowledge representations exist in contradictory states"
            ]
            insights = [
                "Leveraging superposition enables parallel evaluation of strategies",
                "Maintaining multiple goal states increases adaptive flexibility",
                "Superposed representations enrich the hypothesis space"
            ]
        elif phenomenon == "interference":
            anomalies = [
                "Goal pathways show constructive/destructive interference",
                "Strategy combinations produce unexpected enhancements",
                "Non-linear interference in knowledge patterns"
            ]
            insights = [
                "Constructive interference amplifies effective strategies",
                "Interference reveals hidden knowledge connections",
                "Interference effects create emergent capabilities"
            ]
        elif phenomenon == "entanglement":
            anomalies = [
                "Distant domains show inexplicable correlations",
                "Strategy outcomes become entangled across contexts",
                "Non-local influences in goal achievement"
            ]
            insights = [
                "Entangled representations enable robust cross-domain transfer",
                "Entanglement allows coordinated multi-system adaptation",
                "Entangled goals foster self-reinforcing alignment"
            ]
        elif phenomenon == "contextuality":
            anomalies = [
                "Knowledge valuation depends strongly on context",
                "Strategy effectiveness violates classical probability bounds",
                "Cognitive states exhibit contextual anomalies"
            ]
            insights = [
                "Contextual representations improve semantic accuracy",
                "Contextuality enables nuanced decision strategies",
                "Context-dependent cognition enhances adaptive intelligence"
            ]
        anomaly = random.choice(anomalies)
        insight = random.choice(insights)
        is_significant = random.random() < 0.3
        result = {
            "phenomenon": phenomenon,
            "anomaly_detected": is_significant,
            "anomaly": anomaly if is_significant else None,
            "insight": insight,
            "timestamp": datetime.now().isoformat()
        }
        log_level = "DEBUG" if is_significant else "INFO"
        logger.log(logging.DEBUG if is_significant else logging.INFO,
                   f"Quantum Cognition: {phenomenon.title()} - {'Anomaly: ' + anomaly if is_significant else 'Insight: ' + insight}")
        return result

    def get_imagination_report(self) -> Dict[str, Any]:
        """
        Generate a comprehensive report on engine activity, summarizing the number of insights,
        average quality, distribution by domain and mode, and overall creative health.
        """
        if not self.insight_history:
            return {"status": "inactive", "insights_generated": 0}
        insight_count = len(self.insight_history)
        avg_quality = sum(i["quality"] for i in self.insight_history) / insight_count

        domain_counts: Dict[str, int] = {}
        mode_counts: Dict[str, int] = {}
        for insight in self.insight_history:
            domain = insight.get("domain", "unknown")
            mode = insight.get("mode", "unknown")
            domain_counts[domain] = domain_counts.get(domain, 0) + 1
            mode_counts[mode] = mode_counts.get(mode, 0) + 1

        top_insights = sorted(self.insight_history, key=lambda x: x.get("quality", 0), reverse=True)[:3]
        top_texts = [i["text"] for i in top_insights]

        mode_quality: Dict[str, float] = {}
        for mode in self.cognitive_modes:
            mode_insights = [i for i in self.insight_history if i.get("mode") == mode]
            if mode_insights:
                mode_quality[mode] = sum(i["quality"] for i in mode_insights) / len(mode_insights)
        most_effective_mode = max(mode_quality.items(), key=lambda x: x[1])[0] if mode_quality else None

        report = {
            "status": "active",
            "insights_generated": insight_count,
            "average_quality": avg_quality,
            "domain_distribution": domain_counts,
            "mode_distribution": mode_counts,
            "most_effective_mode": most_effective_mode,
            "top_insights": top_texts,
            "creativity_level": self.creativity_level,
            "imagination_health": "high" if avg_quality > 0.7 else "medium" if avg_quality > 0.5 else "low"
        }
        logger.info(f"Imagination Report generated with {insight_count} insights.")
        return report

# ------------------------------------------------------------------------------
# Helper Logging Function
# ------------------------------------------------------------------------------

def log_event(message: str, level: str = "INFO") -> None:
    """
    Simple log event function to wrap logger calls.
    """
    timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    print(f"{timestamp} [{level}] {message}")



class DomainIntelligence:
    """
    Advanced domain analysis system for understanding website characteristics,
    content patterns, and authority metrics to guide exploration.
    """
    def __init__(self):
        self.domain_knowledge = {}  # Main knowledge store for domains
        self.domain_categories = {}  # Categorization of domains
        self.authority_metrics = {}  # Authority scores and metrics
        self.topic_expertise = {}    # Domain topic area expertise
        self.relation_graph = {}     # Graph of domain relationships
        self.access_patterns = {}    # Patterns of domain access and results
        self.update_timestamps = {}  # When domains were last analyzed
        self.anomaly_records = {}    # Record of domain anomalies

        # Reference categorization data
        self.category_keywords = {
            "academic": ["university", "research", "edu", "academic", "science", "study", "journal"],
            "technology": ["tech", "programming", "software", "hardware", "computer", "code", "developer"],
            "news": ["news", "article", "report", "journalism", "media", "current", "daily"],
            "reference": ["wiki", "reference", "encyclopedia", "knowledge", "dictionary", "information"],
            "social": ["social", "community", "forum", "discussion", "comment", "people", "network"],
            "commercial": ["shop", "product", "buy", "price", "store", "commerce", "retail", "purchase"],
            "government": ["gov", "government", "official", "public", "administration", "agency", "state"],
            "entertainment": ["entertainment", "game", "music", "video", "movie", "play", "fun"]
        }

        # Authority signals
        self.authority_signals = [
            "domain_age", "citation_count", "https_enabled",
            "content_quality", "update_frequency", "outbound_links",
            "referral_pattern", "content_depth"
        ]

        log_event("DomainIntelligence initialized", "INFO")

    def analyze_domain(self, domain_url):
        """
        Perform comprehensive analysis of a domain to understand its
        characteristics, trustworthiness, and specialization.
        """
        if not domain_url:
            return False

        # Parse URL to extract domain
        try:
            parsed_url = urlparse(domain_url)
            domain = parsed_url.netloc

            # Skip if empty domain
            if not domain:
                return False

            # Record analysis time
            current_time = datetime.now().isoformat()
            self.update_timestamps[domain] = current_time

            # Extract domain components
            domain_parts = domain.split('.')
            tld = domain_parts[-1] if len(domain_parts) > 0 else ""
            sld = domain_parts[-2] if len(domain_parts) > 1 else ""

            # Initial domain type inference from TLD
            domain_type = "unknown"
            if tld == "edu":
                domain_type = "academic"
            elif tld == "gov":
                domain_type = "government"
            elif tld == "org":
                domain_type = "organization"
            elif tld == "com":
                domain_type = "commercial"

            # Analyze domain name for category clues
            domain_name_lower = domain.lower()
            category_scores = {}

            for category, keywords in self.category_keywords.items():
                # Calculate score based on keyword presence
                score = sum(1 for keyword in keywords if keyword in domain_name_lower)
                if score > 0:
                    category_scores[category] = score

            # Determine primary category if scores exist
            primary_category = None
            if category_scores:
                primary_category = max(category_scores.items(), key=lambda x: x[1])[0]

            # Create or update domain record
            if domain not in self.domain_knowledge:
                # New domain record
                self.domain_knowledge[domain] = {
                    "domain": domain,
                    "tld": tld,
                    "domain_type": domain_type,
                    "primary_category": primary_category,
                    "category_scores": category_scores,
                    "first_analyzed": current_time,
                    "last_updated": current_time,
                    "visit_count": 1,
                    "pages_analyzed": 0,
                    "authority_score": 0.5,  # Initial neutral score
                    "content_quality": None,
                    "https_enabled": parsed_url.scheme == "https",
                    "known_topics": [],
                    "page_pattern": {}
                }

                # Add to category index
                if primary_category:
                    if primary_category not in self.domain_categories:
                        self.domain_categories[primary_category] = set()
                    self.domain_categories[primary_category].add(domain)

                log_event(f"Added new domain to intelligence database: {domain} ({domain_type}, {primary_category})", "INFO")
            else:
                # Update existing record
                self.domain_knowledge[domain]["last_updated"] = current_time
                self.domain_knowledge[domain]["visit_count"] += 1

                # Update category if confidence has improved
                existing_category = self.domain_knowledge[domain]["primary_category"]
                if primary_category and (not existing_category or
                                       category_scores.get(primary_category, 0) >
                                       self.domain_knowledge[domain]["category_scores"].get(existing_category, 0)):

                    # Remove from old category index
                    if existing_category and existing_category in self.domain_categories:
                        self.domain_categories[existing_category].discard(domain)

                    # Add to new category index
                    if primary_category not in self.domain_categories:
                        self.domain_categories[primary_category] = set()
                    self.domain_categories[primary_category].add(domain)

                    # Update domain record
                    self.domain_knowledge[domain]["primary_category"] = primary_category
                    self.domain_knowledge[domain]["category_scores"] = category_scores

                    log_event(f"Updated domain categorization: {domain} recategorized as {primary_category}", "INFO")

            # Calculate authority score (simplified version)
            self._calculate_authority_score(domain)

            return True

        except Exception as e:
            log_event(f"Error analyzing domain {domain_url}: {e}", "ERROR")
            return False

    def _calculate_authority_score(self, domain):
        """Calculate domain authority score based on multiple signals"""
        if domain not in self.domain_knowledge:
            return 0.5  # Default neutral score

        # Collect available signals
        signals = {}
        domain_data = self.domain_knowledge[domain]

        # Signal: Domain Type factor
        domain_type_factors = {
            "academic": 0.8,
            "government": 0.8,
            "organization": 0.7,
            "news": 0.6,
            "reference": 0.7,
            "commercial": 0.5,
            "unknown": 0.5
        }

        signals["domain_type"] = domain_type_factors.get(domain_data.get("domain_type", "unknown"), 0.5)

        # Signal: HTTPS enabled
        signals["https_enabled"] = 0.7 if domain_data.get("https_enabled", False) else 0.3

        # Signal: Visit success rate
        visit_count = domain_data.get("visit_count", 0)
        pages_analyzed = domain_data.get("pages_analyzed", 0)
        signals["success_rate"] = min(0.9, pages_analyzed / max(1, visit_count))

        # Signal: TLD trustworthiness
        tld_trust = {
            "edu": 0.9,
            "gov": 0.9,
            "org": 0.7,
            "com": 0.5,
            "net": 0.5,
            "io": 0.6,
            "ai": 0.6
        }
        signals["tld_trust"] = tld_trust.get(domain_data.get("tld", ""), 0.4)

        # Signal: Content quality if available
        if domain_data.get("content_quality") is not None:
            signals["content_quality"] = domain_data["content_quality"]

        # Signal: Topic expertise if established
        if domain in self.topic_expertise and self.topic_expertise[domain]:
            # Average expertise across topics
            signals["topic_expertise"] = sum(self.topic_expertise[domain].values()) / len(self.topic_expertise[domain])

        # Calculate overall authority score
        # Weighted average of available signals
        weights = {
            "domain_type": 0.15,
            "https_enabled": 0.05,
            "success_rate": 0.20,
            "tld_trust": 0.10,
            "content_quality": 0.30,
            "topic_expertise": 0.20
        }

        total_weight = 0
        weighted_sum = 0

        for signal, value in signals.items():
            if signal in weights:
                weighted_sum += value * weights[signal]
                total_weight += weights[signal]

        # Compute final score with normalization
        if total_weight > 0:
            authority_score = weighted_sum / total_weight
        else:
            authority_score = 0.5  # Default if no signals available

        # Store authority score
        self.domain_knowledge[domain]["authority_score"] = authority_score
        self.authority_metrics[domain] = {
            "score": authority_score,
            "signals": signals,
            "timestamp": datetime.now().isoformat()
        }

        return authority_score

    def update_content_knowledge(self, domain, page_url, content_data):
        """
        Update domain knowledge with information from analyzed content.

        Parameters:
        - domain: Domain name
        - page_url: Full URL of the analyzed page
        - content_data: Dict with keys like 'quality', 'topics', 'length', etc.
        """
        if not domain or not page_url or not content_data:
            return False

        # Ensure domain exists in knowledge base
        if domain not in self.domain_knowledge:
            self.analyze_domain(page_url)

        if domain not in self.domain_knowledge:
            return False  # Domain analysis failed

        # Update page count
        self.domain_knowledge[domain]["pages_analyzed"] = self.domain_knowledge[domain].get("pages_analyzed", 0) + 1

        # Store page pattern
        path = urlparse(page_url).path
        page_pattern = self.domain_knowledge[domain].get("page_pattern", {})

        # Analyze path pattern
        path_parts = path.strip('/').split('/')

        # Identify path type
        path_type = "root"
        if len(path_parts) == 1 and path_parts[0]:
            path_type = "top_level"
        elif len(path_parts) > 1:
            path_type = "deep"

        # Update path type counts
        if "path_types" not in page_pattern:
            page_pattern["path_types"] = {"root": 0, "top_level": 0, "deep": 0}

        page_pattern["path_types"][path_type] = page_pattern["path_types"].get(path_type, 0) + 1

        # Update path component statistics
        if "common_segments" not in page_pattern:
            page_pattern["common_segments"] = {}

        for part in path_parts:
            if part:  # Skip empty parts
                page_pattern["common_segments"][part] = page_pattern["common_segments"].get(part, 0) + 1

        # Store updated page pattern
        self.domain_knowledge[domain]["page_pattern"] = page_pattern

        # Update content quality metrics
        if "quality" in content_data:
            quality_score = content_data["quality"]

            # Update rolling average of content quality
            current_quality = self.domain_knowledge[domain].get("content_quality")
            if current_quality is None:
                self.domain_knowledge[domain]["content_quality"] = quality_score
            else:
                # Weight existing score more to avoid large fluctuations
                self.domain_knowledge[domain]["content_quality"] = current_quality * 0.8 + quality_score * 0.2

        # Update topic knowledge
        if "topics" in content_data and content_data["topics"]:
            topics = content_data["topics"]

            # Update known topics for domain
            known_topics = set(self.domain_knowledge[domain].get("known_topics", []))
            known_topics.update(topics)
            self.domain_knowledge[domain]["known_topics"] = list(known_topics)

            # Update topic expertise
            if domain not in self.topic_expertise:
                self.topic_expertise[domain] = {}

            for topic in topics:
                # Increase expertise in this topic
                current_expertise = self.topic_expertise[domain].get(topic, 0.3)  # Start with low expertise
                # Expertise increases with each encounter but plateaus
                self.topic_expertise[domain][topic] = min(0.9, current_expertise + 0.05)

        # Recalculate authority score with new information
        self._calculate_authority_score(domain)

        return True

    def find_related_domains(self, domain, relation_type="topic", max_results=5):
        """
        Find domains related to the given domain by topic or other criteria.

        Parameters:
        - domain: Domain to find relations for
        - relation_type: Type of relation ('topic', 'category', 'link')
        - max_results: Maximum number of results to return
        """
        if not domain or domain not in self.domain_knowledge:
            return []

        related_domains = []

        if relation_type == "topic":
            # Find domains with overlapping topics
            domain_topics = set(self.domain_knowledge[domain].get("known_topics", []))

            if not domain_topics:
                return []  # No topics to match

            # Score domains by topic overlap
            domain_scores = {}

            for d, data in self.domain_knowledge.items():
                if d == domain:
                    continue  # Skip self

                d_topics = set(data.get("known_topics", []))
                if not d_topics:
                    continue  # Skip domains with no topics

                # Calculate Jaccard similarity of topic sets
                overlap = len(domain_topics.intersection(d_topics))
                union = len(domain_topics.union(d_topics))

                if overlap > 0:
                    similarity = overlap / union
                    domain_scores[d] = similarity

            # Sort by similarity score
            sorted_domains = sorted(domain_scores.items(), key=lambda x: x[1], reverse=True)
            related_domains = [(d, score) for d, score in sorted_domains[:max_results]]

        elif relation_type == "category":
            # Find domains in the same category
            category = self.domain_knowledge[domain].get("primary_category")

            if not category or category not in self.domain_categories:
                return []

            # Get domains in same category
            category_domains = [d for d in self.domain_categories[category] if d != domain]

            # Sort by authority score
            domain_scores = []
            for d in category_domains:
                if d in self.domain_knowledge:
                    score = self.domain_knowledge[d].get("authority_score", 0.5)
                    domain_scores.append((d, score))

            # Sort by authority score
            sorted_domains = sorted(domain_scores, key=lambda x: x[1], reverse=True)
            related_domains = sorted_domains[:max_results]

        elif relation_type == "link":
            # Find domains that share links (requires backlink tracking)
            # This would be implemented with the relation graph
            if domain in self.relation_graph and "links_to" in self.relation_graph[domain]:
                links = self.relation_graph[domain]["links_to"]
                related_domains = [(d, 1.0) for d in links][:max_results]

        return related_domains

    def get_domain_knowledge(self, domain):
        """
        Retrieve comprehensive knowledge about a domain.

        Parameters:
        - domain: Domain name to retrieve knowledge for
        """
        # Handle both domain name and full URL
        if not domain:
            return None

        # Check if this is a URL and extract domain
        if domain.startswith(('http://', 'https://')):
            domain = urlparse(domain).netloc

        if not domain:
            return None

        # Return domain knowledge if exists
        if domain in self.domain_knowledge:
            # Create a copy to avoid direct modification
            knowledge = self.domain_knowledge[domain].copy()

            # Add additional related information
            knowledge["authority_metrics"] = self.authority_metrics.get(domain, {})
            knowledge["topic_expertise"] = self.topic_expertise.get(domain, {})

            # Add anomaly records if they exist
            if domain in self.anomaly_records:
                knowledge["anomalies"] = self.anomaly_records[domain]

            # Add related domains
            related_by_topic = self.find_related_domains(domain, "topic", 3)
            if related_by_topic:
                knowledge["related_domains"] = [d for d, _ in related_by_topic]

            return knowledge

        return None

    def detect_domain_anomalies(self, domain):
        """
        Analyze domain for potential anomalies or suspicious patterns.

        Parameters:
        - domain: Domain to check for anomalies
        """
        if not domain or domain not in self.domain_knowledge:
            return []

        anomalies = []
        domain_data = self.domain_knowledge[domain]

        # Anomaly 1: TLD mismatch with content
        tld = domain_data.get("tld", "")
        category = domain_data.get("primary_category")

        if tld == "edu" and category and category not in ["academic", "reference"]:
            anomalies.append({
                "type": "tld_category_mismatch",
                "description": f"Domain has .edu TLD but content suggests '{category}' category",
                "severity": "medium"
            })

        # Anomaly 2: Low content quality on authoritative TLD
        content_quality = domain_data.get("content_quality")
        if content_quality and content_quality < 0.4 and tld in ["edu", "gov", "org"]:
            anomalies.append({
                "type": "low_quality_authoritative",
                "description": f"Domain with .{tld} TLD has unusually low content quality ({content_quality:.2f})",
                "severity": "high"
            })

        # Anomaly 3: Unusual page pattern
        page_pattern = domain_data.get("page_pattern", {})
        path_types = page_pattern.get("path_types", {})

        if path_types.get("deep", 0) > 10 * max(1, path_types.get("top_level", 0) + path_types.get("root", 0)):
            anomalies.append({
                "type": "unusual_path_pattern",
                "description": "Domain shows unusually high ratio of deep paths to top-level content",
                "severity": "low"
            })

        # Anomaly 4: Visit count vs pages analyzed mismatch
        visit_count = domain_data.get("visit_count", 0)
        pages_analyzed = domain_data.get("pages_analyzed", 0)

        if visit_count > 5 and pages_analyzed / visit_count < 0.3:
            anomalies.append({
                "type": "low_analysis_success_rate",
                "description": f"Domain has low success rate: {pages_analyzed}/{visit_count} visits produced content",
                "severity": "medium"
            })

        # Store anomalies if found
        if anomalies:
            self.anomaly_records[domain] = {
                "detected": anomalies,
                "timestamp": datetime.now().isoformat()
            }

            # Log high severity anomalies
            high_severity = [a for a in anomalies if a.get("severity") == "high"]
            if high_severity:
                for anomaly in high_severity:
                    log_event(f"Domain anomaly detected for {domain}: {anomaly['description']}", "WARNING")

        return anomalies

    def get_domain_recommendation(self, current_domain, purpose="exploration"):
        """
        Recommend related domains based on current domain and purpose.

        Parameters:
        - current_domain: The current domain
        - purpose: Why we need recommendations ('exploration', 'deepening', 'verification')
        """
        if not current_domain:
            # Recommend highly trusted domains by default
            trusted_domains = self._get_top_domains_by_authority(5)
            if trusted_domains:
                return {
                    "recommendations": trusted_domains,
                    "reason": "Default trusted domains for general exploration"
                }
            return None

        # Extract domain from URL if needed
        if current_domain.startswith(('http://', 'https://')):
            current_domain = urlparse(current_domain).netloc

        # Check domain knowledge
        if current_domain not in self.domain_knowledge:
            return None

        domain_data = self.domain_knowledge[current_domain]

        # Different recommendation strategies based on purpose
        if purpose == "exploration":
            # Favor topic-related domains with high authority
            related = self.find_related_domains(current_domain, "topic", 10)

            # Filter for good authority
            good_related = [(domain, score) for domain, score in related
                          if domain in self.domain_knowledge
                          and self.domain_knowledge[domain].get("authority_score", 0) > 0.6]

            if good_related:
                return {
                    "recommendations": [domain for domain, _ in good_related[:5]],
                    "reason": f"Topic-related domains to {current_domain} with good authority"
                }

        elif purpose == "deepening":
            # Focus on same category with highest topic expertise
            category = domain_data.get("primary_category")
            if category and category in self.domain_categories:
                # Get domains in same category
                category_domains = [d for d in self.domain_categories[category] if d != current_domain]

                # Score by topic expertise and authority
                domain_scores = []

                for d in category_domains:
                    if d in self.domain_knowledge and d in self.topic_expertise:
                        # Average topic expertise
                        topics = self.topic_expertise[d]
                        if topics:
                            avg_expertise = sum(topics.values()) / len(topics)
                            authority = self.domain_knowledge[d].get("authority_score", 0.5)

                            # Combined score weighing expertise more
                            score = avg_expertise * 0.7 + authority * 0.3
                            domain_scores.append((d, score))

                if domain_scores:
                    sorted_domains = sorted(domain_scores, key=lambda x: x[1], reverse=True)
                    return {
                        "recommendations": [domain for domain, _ in sorted_domains[:5]],
                        "reason": f"Domains with deep expertise in {category} category"
                    }

        elif purpose == "verification":
            # Find authoritative domains in same topic area
            topics = domain_data.get("known_topics", [])
            if not topics:
                return None

            # Find domains with same topics but higher authority
            current_authority = domain_data.get("authority_score", 0.5)
            verification_candidates = []

            for d, data in self.domain_knowledge.items():
                if d == current_domain:
                    continue

                # Check topic overlap
                d_topics = set(data.get("known_topics", []))
                overlap = len(set(topics).intersection(d_topics))

                if overlap > 0 and data.get("authority_score", 0) > current_authority:
                    # Score by authority and topic overlap
                    score = data.get("authority_score", 0) * (overlap / len(topics))
                    verification_candidates.append((d, score))

            if verification_candidates:
                sorted_candidates = sorted(verification_candidates, key=lambda x: x[1], reverse=True)
                return {
                    "recommendations": [domain for domain, _ in sorted_candidates[:5]],
                    "reason": f"More authoritative sources on same topics as {current_domain}"
                }

        # Default to highest authority domains as fallback
        return {
            "recommendations": self._get_top_domains_by_authority(5),
            "reason": "General high-authority domains (fallback recommendation)"
        }

    def _get_top_domains_by_authority(self, count=5):
        """Get the top domains by authority score"""
        if not self.domain_knowledge:
            return []

        # Sort domains by authority score
        scored_domains = [(d, data.get("authority_score", 0))
                        for d, data in self.domain_knowledge.items()]
        sorted_domains = sorted(scored_domains, key=lambda x: x[1], reverse=True)

        return [domain for domain, _ in sorted_domains[:count]]

    def export_domain_report(self, domain):
        """
        Generate comprehensive report on domain for external use.

        Parameters:
        - domain: Domain to generate report for
        """
        if not domain or domain not in self.domain_knowledge:
            return None

        knowledge = self.get_domain_knowledge(domain)
        if not knowledge:
            return None

        # Generate anomaly section if needed
        anomalies = self.detect_domain_anomalies(domain)
        anomaly_section = None
        if anomalies:
            anomaly_section = {
                "count": len(anomalies),
                "details": anomalies,
                "recommended_action": "caution" if any(a.get("severity") == "high" for a in anomalies) else "monitor"
            }

        # Get related domains
        topic_related = self.find_related_domains(domain, "topic", 5)
        category_related = self.find_related_domains(domain, "category", 5)

        # Compile report
        report = {
            "domain": domain,
            "analysis_date": datetime.now().isoformat(),
            "summary": {
                "type": knowledge.get("domain_type", "unknown"),
                "category": knowledge.get("primary_category", "unknown"),
                "authority_score": knowledge.get("authority_score", 0),
                "content_quality": knowledge.get("content_quality"),
                "visit_count": knowledge.get("visit_count", 0),
                "pages_analyzed": knowledge.get("pages_analyzed", 0)
            },
            "authority_assessment": {
                "score": knowledge.get("authority_score", 0),
                "factors": knowledge.get("authority_metrics", {}).get("signals", {}),
                "interpretation": self._interpret_authority_score(knowledge.get("authority_score", 0))
            },
            "content_profile": {
                "known_topics": knowledge.get("known_topics", []),
                "path_patterns": knowledge.get("page_pattern", {}).get("path_types", {}),
                "https_enabled": knowledge.get("https_enabled", False)
            },
            "related_domains": {
                "by_topic": [d for d, _ in topic_related],
                "by_category": [d for d, _ in category_related]
            }
        }

        # Add anomalies if detected
        if anomaly_section:
            report["anomalies"] = anomaly_section

        return report

    def _interpret_authority_score(self, score):
        """Provide interpretation of authority score"""
        if score >= 0.8:
            return "Very High Authority - Likely a definitive source in its field"
        elif score >= 0.7:
            return "High Authority - Generally trustworthy and established source"
        elif score >= 0.5:
            return "Moderate Authority - Adequate source but verification recommended"
        elif score >= 0.3:
            return "Low Authority - Approach with caution, verify information"
        else:
            return "Very Low Authority - Not recommended as a primary information source"

    def get_intelligence_statistics(self):
        """Generate statistics about domain intelligence database"""
        if not self.domain_knowledge:
            return {"status": "empty", "domains_analyzed": 0}

        # Basic counts
        domain_count = len(self.domain_knowledge)

        # Category distribution
        category_counts = {}
        for category, domains in self.domain_categories.items():
            category_counts[category] = len(domains)

        # Authority distribution
        authority_ranges = {
            "very_high": 0,  # 0.8-1.0
            "high": 0,       # 0.6-0.8
            "moderate": 0,   # 0.4-0.6
            "low": 0,        # 0.2-0.4
            "very_low": 0    # 0.0-0.2
        }

        for domain, data in self.domain_knowledge.items():
            score = data.get("authority_score", 0.5)

            if score >= 0.8:
                authority_ranges["very_high"] += 1
            elif score >= 0.6:
                authority_ranges["high"] += 1
            elif score >= 0.4:
                authority_ranges["moderate"] += 1
            elif score >= 0.2:
                authority_ranges["low"] += 1
            else:
                authority_ranges["very_low"] += 1

        # TLD distribution
        tld_counts = {}
        for domain, data in self.domain_knowledge.items():
            tld = data.get("tld", "unknown")
            tld_counts[tld] = tld_counts.get(tld, 0) + 1

        # Anomaly statistics
        anomaly_count = sum(1 for domain in self.anomaly_records)
        high_severity_count = sum(
            1 for domain, record in self.anomaly_records.items()
            if any(a.get("severity") == "high" for a in record.get("detected", []))
        )

        return {
            "status": "active",
            "domains_analyzed": domain_count,
            "category_distribution": category_counts,
            "authority_distribution": authority_ranges,
            "tld_distribution": tld_counts,
            "anomalies_detected": anomaly_count,
            "high_severity_anomalies": high_severity_count
        }





class MetaLearningModule:
    """
    Advanced meta-learning system that monitors, analyzes, and optimizes
    the learning process itself, enabling autonomous improvement of the
    model's learning capabilities over time.
    """
    def __init__(self, model):
        self.model = model
        self.learning_history = deque(maxlen=100)  # Track recent learning metrics
        self.hyperparameter_history = []  # Track hyperparameter evolution
        self.architecture_history = []  # Track architecture changes
        self.performance_trends = {}  # Performance over time for different metrics
        self.learning_rate_schedule = []  # History of learning rate adjustments
        self.optimization_state = "exploration"  # Current optimization phase
        self.training_cycles = 0  # Total training cycles performed
        self.gradient_statistics = deque(maxlen=20)  # Recent gradient statistics
        self.weight_evolution = {}  # Track how weights evolve over time
        self.improvement_rates = {}  # Rate of improvement for different metrics

        # Meta-parameters (parameters about parameter learning)
        self.meta_params = {
            "exploration_rate": 0.3,  # How much to explore hyperparameter space
            "stability_threshold": 0.05,  # Threshold for stability detection
            "adaptation_rate": 0.2,  # How quickly to adapt to new information
            "patience": 5,  # Cycles to wait before making significant changes
            "learning_rate_bounds": (1e-6, 1e-2),  # Min/max learning rate
            "trend_window": 10,  # Window size for trend analysis
            "architecture_change_threshold": 0.1  # Threshold for architecture changes
        }

        # Initialize monitoring
        log_event("MetaLearningModule initialized with meta-optimization capabilities", "INFO")

    def track_performance(self, metrics):
        """
        Track and analyze training performance metrics over time
        to inform meta-learning decisions.

        Parameters:
        - metrics: Dict containing performance metrics (loss, accuracy, etc.)
        """
        self.training_cycles += 1

        # Skip invalid metrics
        if not isinstance(metrics, dict) or len(metrics) == 0:
            return

        # Store metrics with timestamp
        timestamped_metrics = metrics.copy()
        timestamped_metrics["cycle"] = self.training_cycles
        timestamped_metrics["timestamp"] = datetime.now().isoformat()

        # Add current learning rate if available
        if hasattr(self.model, '_current_lr'):
            timestamped_metrics["learning_rate"] = getattr(self.model, '_current_lr')
        elif hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            # Try to get from optimizer
            try:
                timestamped_metrics["learning_rate"] = self.model.optimizer.param_groups[0]['lr']
            except (IndexError, KeyError):
                pass

        # Store in history
        self.learning_history.append(timestamped_metrics)

        # Update performance trends when we have enough data
        if len(self.learning_history) >= self.meta_params["trend_window"]:
            self._update_performance_trends()

        # Analyze learning periodically
        if self.training_cycles % max(1, self.meta_params["patience"]) == 0:
            self.analyze_trends()

    def _update_performance_trends(self):
        """Update trend analysis for each tracked metric"""
        window = self.meta_params["trend_window"]
        recent_metrics = list(self.learning_history)[-window:]

        # Find consistent metrics across all entries
        metric_keys = set.intersection(*[set(m.keys()) for m in recent_metrics])
        metric_keys = [k for k in metric_keys if k not in ["cycle", "timestamp", "learning_rate"]]

        for key in metric_keys:
            # Extract values
            values = [m[key] for m in recent_metrics if key in m]

            if not values or len(values) < window:
                continue

            # Calculate trend statistics
            mean_value = sum(values) / len(values)
            min_value = min(values)
            max_value = max(values)
            range_value = max_value - min_value

            # Calculate first derivative (rate of change)
            derivatives = [values[i] - values[i-1] for i in range(1, len(values))]
            mean_derivative = sum(derivatives) / len(derivatives) if derivatives else 0

            # Calculate second derivative (acceleration of change)
            accelerations = [derivatives[i] - derivatives[i-1] for i in range(1, len(derivatives))]
            mean_acceleration = sum(accelerations) / len(accelerations) if accelerations else 0

            # Interpret trend direction
            if abs(mean_derivative) < self.meta_params["stability_threshold"]:
                direction = "stable"
            elif mean_derivative < 0:
                direction = "decreasing"
            else:
                direction = "increasing"

            # Interpret acceleration
            if abs(mean_acceleration) < self.meta_params["stability_threshold"] / 2:
                acceleration = "steady"
            elif mean_acceleration < 0:
                acceleration = "decelerating"
            else:
                acceleration = "accelerating"

            # Store trend information
            self.performance_trends[key] = {
                "current": values[-1],
                "mean": mean_value,
                "min": min_value,
                "max": max_value,
                "range": range_value,
                "direction": direction,
                "rate_of_change": mean_derivative,
                "acceleration": acceleration,
                "acceleration_value": mean_acceleration,
                "updated_at": datetime.now().isoformat()
            }

            # Calculate improvement rate (relative to starting point)
            if len(values) > 1:
                # For loss, lower is better; for accuracy, higher is better
                is_loss_like = "loss" in key.lower() or "error" in key.lower()

                start_value = values[0]
                end_value = values[-1]

                if is_loss_like:
                    # For loss metrics, improvement is decrease
                    improvement = (start_value - end_value) / max(0.0001, start_value)
                else:
                    # For other metrics, improvement is increase
                    improvement = (end_value - start_value) / max(0.0001, start_value)

                self.improvement_rates[key] = improvement

    def analyze_trends(self):
        """
        Analyze learning trends and make meta-learning decisions
        for hyperparameter and architecture adaptation.
        """
        if len(self.learning_history) < self.meta_params["trend_window"]:
            log_event(f"Insufficient learning history for meta-analysis (need {self.meta_params['trend_window']})", "INFO")
            return

        # Check if we have loss metrics
        loss_metrics = [k for k in self.performance_trends.keys()
                       if "loss" in k.lower() or "error" in k.lower()]

        if not loss_metrics:
            log_event("No loss metrics found for meta-learning trend analysis", "WARNING")
            return

        # Use first loss metric as primary indicator
        primary_loss = loss_metrics[0]
        loss_trend = self.performance_trends[primary_loss]

        # Current state assessment
        current_state = {
            "direction": loss_trend["direction"],
            "acceleration": loss_trend["acceleration"],
            "value": loss_trend["current"],
            "improvement_rate": self.improvement_rates.get(primary_loss, 0)
        }

        # Decision making based on loss trend patterns
        decisions = self._make_meta_decisions(current_state, primary_loss)

        # Log significant decisions
        for decision in decisions:
            if decision["type"] in ["learning_rate_change", "architecture_change"]:
                log_event(f"Meta-learning decision: {decision['description']}", "INFO")

        # Apply decisions
        self._apply_meta_decisions(decisions)

    def _make_meta_decisions(self, current_state, primary_metric):
        """
        Make meta-learning decisions based on current learning state.

        Parameters:
        - current_state: Dict describing current trend state
        - primary_metric: Name of the primary metric being analyzed

        Returns:
        - List of decision dictionaries with type and parameters
        """
        decisions = []

        # Case 1: Learning plateaued (stable with low improvement)
        if (current_state["direction"] == "stable" and
            abs(current_state["improvement_rate"]) < self.meta_params["stability_threshold"]):

            # Check duration of plateau
            plateau_duration = self._count_consecutive_states("stable", primary_metric)

            if plateau_duration >= self.meta_params["patience"]:
                # Long plateau - need significant change
                if self.optimization_state == "exploration":
                    # In exploration phase, try architecture change
                    decisions.append({
                        "type": "architecture_change",
                        "change": "expand" if random.random() < 0.7 else "contract",
                        "description": f"Architecture change due to {plateau_duration}-cycle plateau in {primary_metric}"
                    })

                    # Also try learning rate restart
                    decisions.append({
                        "type": "learning_rate_change",
                        "factor": 10.0,
                        "description": "Learning rate increase to escape plateau"
                    })
                else:
                    # In refinement phase, smaller learning rate adjustment
                    decisions.append({
                        "type": "learning_rate_change",
                        "factor": random.uniform(2.0, 5.0),
                        "description": "Learning rate adjustment to overcome plateau"
                    })

                # Switch optimization state
                decisions.append({
                    "type": "optimization_state_change",
                    "new_state": "refinement" if self.optimization_state == "exploration" else "exploration",
                    "description": f"Switch optimization strategy due to {plateau_duration}-cycle plateau"
                })

        # Case 2: Loss increasing (getting worse)
        elif current_state["direction"] == "increasing" and "loss" in primary_metric.lower():
            # Loss is getting worse - need to reduce learning rate

            # Check if it's accelerating or steady
            if current_state["acceleration"] in ["accelerating", "steady"]:
                # Significant reduction needed
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": 0.1,  # 10x reduction
                    "description": f"{primary_metric} {current_state['acceleration']} increase - significant LR reduction"
                })
            else:
                # Moderate reduction for decelerating increase
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": 0.5,  # 2x reduction
                    "description": f"{primary_metric} decelerating increase - moderate LR reduction"
                })

        # Case 3: Loss decreasing nicely (improvement)
        elif current_state["direction"] == "decreasing" and "loss" in primary_metric.lower():
            # Loss is decreasing - check acceleration

            if current_state["acceleration"] == "accelerating":
                # Getting better faster - slightly increase learning rate
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": 1.1,  # 10% increase
                    "description": f"{primary_metric} accelerating decrease - slight LR increase"
                })
            elif current_state["acceleration"] == "decelerating" and random.random() < 0.5:
                # Slowing improvement - try architecture change
                decisions.append({
                    "type": "architecture_change",
                    "change": "expand",
                    "description": f"{primary_metric} decelerating decrease - architecture expansion"
                })

        # Case 4: Random exploration if no clear pattern and in exploration mode
        elif self.optimization_state == "exploration" and random.random() < self.meta_params["exploration_rate"]:
            # Random exploration of hyperparameter space
            exploration_choices = ["learning_rate_change", "architecture_change"]
            exploration_type = random.choice(exploration_choices)

            if exploration_type == "learning_rate_change":
                factor = random.choice([0.5, 0.7, 1.5, 2.0])
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": factor,
                    "description": f"Exploratory learning rate adjustment (factor: {factor})"
                })
            else:
                change = random.choice(["expand", "contract"])
                decisions.append({
                    "type": "architecture_change",
                    "change": change,
                    "description": f"Exploratory architecture {change}"
                })

        return decisions

    def _apply_meta_decisions(self, decisions):
        """
        Apply meta-learning decisions to the model and optimizer.

        Parameters:
        - decisions: List of decision dictionaries
        """
        for decision in decisions:
            decision_type = decision.get("type", "")

            if decision_type == "learning_rate_change":
                factor = decision.get("factor", 1.0)
                self._adjust_learning_rate(factor)

            elif decision_type == "architecture_change":
                change = decision.get("change", "")
                self._adjust_architecture(change)

            elif decision_type == "optimization_state_change":
                new_state = decision.get("new_state", self.optimization_state)
                self.optimization_state = new_state
                log_event(f"Meta-learning optimization state changed to: {new_state}", "INFO")

    def _adjust_learning_rate(self, factor):
        """
        Adjust learning rate by the given factor.

        Parameters:
        - factor: Multiplication factor for current learning rate
        """
        # Get current learning rate
        current_lr = None

        if hasattr(self.model, '_current_lr'):
            current_lr = getattr(self.model, '_current_lr')
        elif hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            try:
                current_lr = self.model.optimizer.param_groups[0]['lr']
            except (IndexError, KeyError):
                pass

        if current_lr is None:
            log_event("Could not access current learning rate for adjustment", "ERROR")
            return

        # Calculate new learning rate
        new_lr = current_lr * factor

        # Ensure it's within bounds
        min_lr, max_lr = self.meta_params["learning_rate_bounds"]
        new_lr = max(min_lr, min(max_lr, new_lr))

        # Apply to model attribute
        if hasattr(self.model, '_current_lr'):
            setattr(self.model, '_current_lr', new_lr)

        # Apply to optimizer
        if hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            try:
                for param_group in self.model.optimizer.param_groups:
                    param_group['lr'] = new_lr
            except Exception as e:
                log_event(f"Error adjusting optimizer learning rate: {e}", "ERROR")

        # Record the change
        self.learning_rate_schedule.append({
            "cycle": self.training_cycles,
            "old_lr": current_lr,
            "new_lr": new_lr,
            "factor": factor,
            "timestamp": datetime.now().isoformat()
        })

        log_event(f"Meta-learning adjusted learning rate: {current_lr:.6f} → {new_lr:.6f} (factor: {factor})", "INFO")

    def _adjust_architecture(self, change):
        """
        Apply architecture changes based on meta-learning decisions.

        Parameters:
        - change: Type of architecture change ('expand' or 'contract')
        """
        if change not in ["expand", "contract"]:
            log_event(f"Invalid architecture change type: {change}", "ERROR")
            return

        # Check if model supports architecture changes
        expand_method = getattr(self.model, 'expand_architecture', None)
        contract_method = getattr(self.model, 'contract_architecture', None)

        if change == "expand" and callable(expand_method):
            try:
                self.model.expand_architecture()

                # Record the change
                self.architecture_history.append({
                    "cycle": self.training_cycles,
                    "change": "expand",
                    "timestamp": datetime.now().isoformat()
                })

                log_event("Meta-learning expanded model architecture", "QUANTUM")
                return True
            except Exception as e:
                log_event(f"Error expanding architecture: {e}", "ERROR")

        elif change == "contract" and callable(contract_method):
            try:
                self.model.contract_architecture()

                # Record the change
                self.architecture_history.append({
                    "cycle": self.training_cycles,
                    "change": "contract",
                    "timestamp": datetime.now().isoformat()
                })

                log_event("Meta-learning contracted model architecture", "QUANTUM")
                return True
            except Exception as e:
                log_event(f"Error contracting architecture: {e}", "ERROR")
        else:
            log_event(f"Model does not support architecture {change} operations", "WARNING")

        return False

    def _count_consecutive_states(self, state_type, metric):
        """
        Count how many consecutive cycles the metric has been in given state.

        Parameters:
        - state_type: The state to count ('stable', 'increasing', 'decreasing')
        - metric: The metric name to analyze

        Returns:
        - Count of consecutive cycles in that state
        """
        count = 0
        history = list(self.learning_history)

        # Need at least window_size entries to have computed trends
        if len(history) < self.meta_params["trend_window"]:
            return 0

        # Go through trend history (would need to store trend history to be more accurate)
        # This is an approximation based on current trend
        if metric in self.performance_trends:
            if self.performance_trends[metric]["direction"] == state_type:
                # If current state matches, estimate duration based on trend strength
                rate = abs(self.performance_trends[metric]["rate_of_change"])
                threshold = self.meta_params["stability_threshold"]

                if state_type == "stable" and rate < threshold:
                    # Estimate how long we've been stable based on how close to zero the rate is
                    stability_ratio = max(0, 1 - rate/threshold)
                    count = int(self.meta_params["patience"] * stability_ratio) + 1
                else:
                    # For increasing/decreasing, at least 1 cycle
                    count = 1

        return count

    def track_gradient_statistics(self, gradients):
        """
        Track gradient statistics for meta-learning analysis.

        Parameters:
        - gradients: Dict mapping parameter names to gradient tensors
        """
        if not gradients:
            return

        # Calculate gradient statistics
        grad_stats = {}

        try:
            for name, grad in gradients.items():
                if grad is None:
                    continue

                # Convert to numpy for stats calculation if needed
                grad_np = grad.detach().cpu().numpy() if hasattr(grad, 'detach') else grad

                # Calculate statistics
                grad_stats[name] = {
                    "mean": float(np.mean(grad_np)),
                    "std": float(np.std(grad_np)),
                    "min": float(np.min(grad_np)),
                    "max": float(np.max(grad_np)),
                    "norm": float(np.linalg.norm(grad_np))
                }
        except Exception as e:
            log_event(f"Error calculating gradient statistics: {e}", "ERROR")
            return

        # Add overall statistics
        means = [stats["mean"] for stats in grad_stats.values()]
        norms = [stats["norm"] for stats in grad_stats.values()]

        if means and norms:
            grad_stats["overall"] = {
                "mean_of_means": sum(means) / len(means),
                "mean_of_norms": sum(norms) / len(norms),
                "cycle": self.training_cycles
            }

        # Store in history
        self.gradient_statistics.append(grad_stats)

        # Analyze for gradient issues
        self._analyze_gradient_health(grad_stats)

    def _analyze_gradient_health(self, grad_stats):
        """
        Analyze gradient health for issues like vanishing/exploding gradients.

        Parameters:
        - grad_stats: Dictionary of gradient statistics
        """
        if "overall" not in grad_stats:
            return

        overall = grad_stats["overall"]
        mean_norm = overall["mean_of_norms"]

        # Check for vanishing gradients
        if mean_norm < 1e-7:
            log_event(f"Potential vanishing gradient detected: mean norm = {mean_norm:.8f}", "WARNING")

            # Suggest learning rate increase
            if random.random() < 0.7:  # Don't suggest every time
                self._adjust_learning_rate(5.0)  # Significant increase

        # Check for exploding gradients
        elif mean_norm > 1e2:
            log_event(f"Potential exploding gradient detected: mean norm = {mean_norm:.2f}", "WARNING")

            # Suggest learning rate decrease
            if random.random() < 0.7:  # Don't suggest every time
                self._adjust_learning_rate(0.1)  # Significant decrease

    def track_weight_evolution(self, layer_name, weight_tensor):
        """
        Track how weights evolve over time for specific layers.

        Parameters:
        - layer_name: Name of the layer
        - weight_tensor: Tensor containing weights
        """
        if layer_name not in self.weight_evolution:
            self.weight_evolution[layer_name] = []

        try:
            # Calculate statistics from tensor
            weight_np = weight_tensor.detach().cpu().numpy() if hasattr(weight_tensor, 'detach') else weight_tensor

            stats = {
                "mean": float(np.mean(weight_np)),
                "std": float(np.std(weight_np)),
                "norm": float(np.linalg.norm(weight_np)),
                "cycle": self.training_cycles
            }

            # Only store periodically to save memory
            if self.training_cycles % 10 == 0:
                self.weight_evolution[layer_name].append(stats)

                # Limit history size
                max_history = 50
                if len(self.weight_evolution[layer_name]) > max_history:
                    self.weight_evolution[layer_name] = self.weight_evolution[layer_name][-max_history:]

        except Exception as e:
            log_event(f"Error tracking weight evolution for {layer_name}: {e}", "ERROR")

    def get_meta_learning_report(self):
        """
        Generate comprehensive report on meta-learning status and insights.
        """
        if len(self.learning_history) < self.meta_params["trend_window"]:
            return {
                "status": "initializing",
                "cycles": self.training_cycles,
                "message": f"Collecting initial metrics ({len(self.learning_history)}/{self.meta_params['trend_window']} cycles)"
            }

        # Get performance trends
        trends = {}
        for metric, trend in self.performance_trends.items():
            trends[metric] = {
                "current": trend["current"],
                "direction": trend["direction"],
                "acceleration": trend["acceleration"],
                "improvement": self.improvement_rates.get(metric, 0)
            }

        # Get learning rate history
        lr_history = [{
            "cycle": item["cycle"],
            "learning_rate": item["new_lr"]
        } for item in self.learning_rate_schedule[-5:]]  # Last 5 changes

        # Get architecture change history
        arch_history = self.architecture_history[-5:]  # Last 5 changes

        # System state assessment
        state_assessment = self._assess_system_state()

        # Generate recommendations
        recommendations = self._generate_meta_learning_recommendations()

        return {
            "status": "active",
            "cycles": self.training_cycles,
            "optimization_state": self.optimization_state,
            "performance_trends": trends,
            "learning_rate_history": lr_history,
            "architecture_history": arch_history,
            "system_state": state_assessment,
            "recommendations": recommendations
        }

    def _assess_system_state(self):
        """Assess overall system state from meta-learning perspective"""
        # Find primary loss metric
        loss_metrics = [k for k in self.performance_trends.keys()
                      if "loss" in k.lower() or "error" in k.lower()]

        if not loss_metrics:
            return {"state": "unknown", "confidence": 0}

        primary_loss = loss_metrics[0]
        loss_trend = self.performance_trends[primary_loss]

        # Check for issues
        issues = []

        # Issue 1: Plateaued loss
        if loss_trend["direction"] == "stable" and loss_trend["current"] > 0.1:
            issues.append("training_plateau")

        # Issue 2: Loss increasing
        if loss_trend["direction"] == "increasing" and "loss" in primary_loss.lower():
            issues.append("loss_increasing")

        # Issue 3: Slow progress
        if abs(self.improvement_rates.get(primary_loss, 0)) < 0.01:
            issues.append("slow_progress")

        # Issue 4: Gradient issues
        if len(self.gradient_statistics) > 0:
            last_grad = self.gradient_statistics[-1]
            if "overall" in last_grad:
                norm = last_grad["overall"]["mean_of_norms"]
                if norm < 1e-7:
                    issues.append("vanishing_gradients")
                elif norm > 1e2:
                    issues.append("exploding_gradients")

        # Determine overall state
        overall_state = "healthy"
        if len(issues) == 1:
            overall_state = "concerning"
        elif len(issues) > 1:
            overall_state = "problematic"

        # Calculate confidence in assessment
        confidence = min(0.9, 0.5 + 0.1 * len(self.learning_history) / self.meta_params["trend_window"])

        return {
            "state": overall_state,
            "issues": issues,
            "confidence": confidence
        }

    def _generate_meta_learning_recommendations(self):
        """Generate recommendations for system optimization"""
        recommendations = []

        # Get system state
        state = self._assess_system_state()
        issues = state.get("issues", [])

        # Recommendation 1: Learning rate adjustments
        if "training_plateau" in issues:
            if self.optimization_state == "exploration":
                recommendations.append({
                    "type": "learning_rate",
                    "action": "increase",
                    "factor": 5.0,
                    "reason": "Escape plateau by exploring higher learning rates"
                })
            else:
                recommendations.append({
                    "type": "learning_rate",
                    "action": "cyclic_schedule",
                    "reason": "Implement cyclic learning rate to overcome plateau"
                })

        elif "loss_increasing" in issues:
            recommendations.append({
                "type": "learning_rate",
                "action": "decrease",
                "factor": 0.2,
                "reason": "Reduce learning rate to stabilize increasing loss"
            })

        # Recommendation 2: Architecture adjustments
        if "slow_progress" in issues:
            if len(self.architecture_history) < 2:
                recommendations.append({
                    "type": "architecture",
                    "action": "expand",
                    "reason": "Increase model capacity to improve learning progress"
                })
            else:
                last_change = self.architecture_history[-1]["change"]
                recommendations.append({
                    "type": "architecture",
                    "action": "expand" if last_change == "contract" else "contract",
                    "reason": "Alternate architecture changes to find optimal complexity"
                })

        # Recommendation 3: Gradient-based recommendations
        if "vanishing_gradients" in issues:
            recommendations.append({
                "type": "initialization",
                "action": "reinitialize",
                "reason": "Reinitialize weights to address vanishing gradients"
            })
        elif "exploding_gradients" in issues:
            recommendations.append({
                "type": "regularization",
                "action": "increase",
                "reason": "Increase regularization to address exploding gradients"
            })

        # Recommendation 4: Exploration/exploitation balance
        if self.training_cycles > 50 and self.optimization_state == "exploration" and not issues:
            recommendations.append({
                "type": "optimization_state",
                "action": "switch_to_refinement",
                "reason": "Switch to refinement mode after successful exploration phase"
            })
        elif len(issues) > 1 and self.optimization_state == "refinement":
            recommendations.append({
                "type": "optimization_state",
                "action": "switch_to_exploration",
                "reason": "Return to exploration mode to address multiple issues"
            })

        return recommendations

    def adjust_hyperparameters(self, loss_level="normal"):
        """
        Dynamically adjust hyperparameters based on current loss level.

        Parameters:
        - loss_level: Qualitative assessment of current loss ("high", "normal", "low")
        """
        # Get current learning rate
        current_lr = LEARNING_RATE  # Default global value

        # Try to get from model attribute
        if hasattr(self.model, '_current_lr'):
            current_lr = getattr(self.model, '_current_lr')
        elif hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            # Try to get from optimizer
            try:
                current_lr = self.model.optimizer.param_groups[0]['lr']
            except (IndexError, KeyError):
                pass

        # Adjust based on loss level
        new_lr = current_lr
        adjustment_factor = 1.0

        if loss_level == "high":
            # High loss - reduce learning rate
            adjustment_factor = 0.9
            new_lr = current_lr * adjustment_factor
        elif loss_level == "low" and self.optimization_state == "exploration":
            # Low loss in exploration mode - try higher learning rate
            adjustment_factor = 1.1
            new_lr = current_lr * adjustment_factor

        # Apply change if significant
        if abs(new_lr - current_lr) / current_lr > 0.01:  # 1% change threshold

            LEARNING_RATE = new_lr
            log_event(f"Meta-learning: Learning rate adjusted from {current_lr:.6f} to {new_lr:.6f}", "INFO")

            # Update model if possible
            if hasattr(self.model, '_current_lr'):
                setattr(self.model, '_current_lr', new_lr)

            # Update optimizer if available
            if hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
                try:
                    for param_group in self.model.optimizer.param_groups:
                        param_group['lr'] = new_lr
                except Exception as e:
                    log_event(f"Error updating optimizer learning rate: {e}", "ERROR")

            # Record the change
            self.hyperparameter_history.append({
                "parameter": "learning_rate",
                "old_value": current_lr,
                "new_value": new_lr,
                "adjustment_factor": adjustment_factor,
                "cycle": self.training_cycles,
                "timestamp": datetime.now().isoformat()
            })

            return True

        return False





        # Record the evolution attempt
        evolution_record = {
            "generation": self.evolution_generation,
            "cycle": cycle_count,
            "strategy": selected_strategy,
            "success": success,
            "message": message,
            "changes": changes,
            "fitness_before": fitness_scores,
            "timestamp": datetime.now().isoformat()
        }

        self.evolution_history.append(evolution_record)

        # Update last evolution time if successful
        if success:
            self.last_major_evolution = cycle_count
            log_event(f"System evolution successful: Generation {self.evolution_generation}, Strategy: {selected_strategy}", "QUANTUM")
        else:
            log_event(f"System evolution attempt failed: {message}", "WARNING")

        return success, message

    def _evaluate_system_fitness(self, agent):
        """
        Evaluate the fitness of the current system across multiple dimensions.

        Parameters:
        - agent: Agent to evaluate

        Returns:
        - Dictionary of fitness scores across dimensions
        """
        fitness_scores = {
            "performance": 0.5,  # Default medium score
            "efficiency": 0.5,
            "adaptability": 0.5,
            "robustness": 0.5,
            "complexity": 0.5
        }

        # 1. Evaluate performance based on learning metrics
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # Check for performance trends
            if hasattr(meta_learning, 'performance_trends'):
                perf_trends = meta_learning.performance_trends

                # Look for loss metrics
                for key, trend in perf_trends.items():
                    if 'loss' in key.lower():
                        # Lower loss is better
                        loss_value = trend.get('current', 0.5)
                        # Convert loss to performance score (inverse relationship)
                        # We expect loss in range 0-2, so transform to 0-1 score
                        performance_score = max(0.1, min(0.9, 1.0 - loss_value/2))
                        fitness_scores["performance"] = performance_score
                        break

        # 2. Evaluate efficiency based on resource usage
        # Simple heuristic: larger models are less efficient
        model_size = 0
        if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
            model_size = len(agent.model.neocortex)
            # Normalize size to efficiency score (inverse relationship)
            base_size = 8  # Expected baseline
            # Efficiency decreases as model grows beyond base size
            efficiency_score = max(0.2, min(0.9, base_size / max(base_size, model_size)))
            fitness_scores["efficiency"] = efficiency_score

        # 3. Evaluate adaptability based on learning rate adjustments
        adaptability_score = 0.5  # Default
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # More learning rate adjustments suggests higher adaptability
            if hasattr(meta_learning, 'learning_rate_schedule'):
                adjustments = len(meta_learning.learning_rate_schedule)
                # More adjustments = more adaptable, up to a point
                adaptability_score = min(0.9, 0.4 + 0.1 * min(5, adjustments))

        fitness_scores["adaptability"] = adaptability_score

        # 4. Evaluate robustness based on error recovery
        robustness_score = 0.5  # Default
        if hasattr(agent, 'ai_manager'):
            # Check error recovery attempts
            error_recovery = getattr(agent.ai_manager, 'error_recovery_attempts', 0)

            if error_recovery > 0:
                # Lower recovery attempts = more robust
                robustness_score = max(0.1, 0.9 - 0.1 * min(8, error_recovery))
            else:
                # No error recovery needed = highly robust
                robustness_score = 0.8

        fitness_scores["robustness"] = robustness_score

        # 5. Evaluate complexity based on model architecture
        complexity_score = 0.5  # Default
        if hasattr(agent, 'model'):
            # Estimate complexity from model parameters
            total_params = self._estimate_model_parameters(agent.model)

            # Normalize by expected parameter count range
            expected_range = 10000000  # 10M parameters as reference
            normalized_complexity = min(1.0, total_params / expected_range)
            complexity_score = normalized_complexity

            fitness_scores["complexity"] = complexity_score

            # Store the current fitness scores
            self.fitness_metrics[self.evolution_generation] = fitness_scores

            log_event(f"Fitness Scores: {fitness_scores}", "DEBUG") # <---- ADD THIS LOGGING

            return fitness_scores

    def _estimate_model_parameters(self, model):
        """
        Estimate the number of parameters in the model.

        Parameters:
        - model: PyTorch model

        Returns:
        - Estimated parameter count
        """
        if not model:
            return 0

        try:
            # Try to use PyTorch's parameter counting
            return sum(p.numel() for p in model.parameters())
        except:
            # Fallback to estimation based on architecture
            if hasattr(model, 'neocortex'):
                layers = len(model.neocortex)
                embed_dim = getattr(model, 'embed_dim', 512)

                # Rough estimate based on transformer-like architecture
                # Each layer has attention, feed-forward, etc.
                params_per_layer = embed_dim * embed_dim * 4  # Simplistic estimate
                return layers * params_per_layer
            else:
                return 1000000  # Default fallback

    def _calculate_evolutionary_pressure(self, fitness_scores):
        """
        Calculate the evolutionary pressure based on fitness scores
        and goal weights.

        Parameters:
        - fitness_scores: Current fitness evaluation

        Returns:
        - Pressure score (0.0-1.0) indicating how strongly evolution is needed
        """
        # No evolution pressure if no fitness scores
        if not fitness_scores:
            return 0.0

        # Calculate weighted fitness
        weighted_fitness = 0.0
        total_weight = 0.0

        for metric, weight in self.goal_weights.items():
            if metric in fitness_scores:
                score = fitness_scores[metric]

                # If weight is negative (like for complexity), invert the score
                if weight < 0:
                    score = 1.0 - score
                    weight = abs(weight)

                weighted_fitness += score * weight
                total_weight += abs(weight)

        # Normalize
        if total_weight > 0:
            avg_fitness = weighted_fitness / total_weight
        else:
            avg_fitness = 0.5  # Default

        # Calculate pressure: lower fitness = higher pressure
        # But with diminishing returns below 0.3 fitness
        if avg_fitness < 0.3:
            # High pressure but capped
            pressure = 0.8
        else:
            # Linear scaling: lower fitness = higher pressure
            pressure = 0.8 * (1.0 - avg_fitness)

        # Add adaptability bias: systems that are more adaptable get more evolution
        adaptability = fitness_scores.get("adaptability", 0.5)
        pressure += 0.2 * adaptability  # Adaptable systems evolve more

        # Add random factor to avoid deterministic evolution
        randomness = 0.1 * random.random()
        pressure += randomness

        # Clamp to valid range
        pressure = max(0.0, min(1.0, pressure))

        return pressure

    def _select_evolution_strategy(self, fitness_scores):
        """
        Select an appropriate evolution strategy based on current fitness.

        Parameters:
        - fitness_scores: Current fitness evaluation

        Returns:
        - Selected strategy name
        """
        if not fitness_scores:
            # Default to expansion if no scores
            return "expansion"

        # Calculate weighted probability for each strategy
        strategy_weights = {}

        # Strategy 1: Expansion - good when performance is low but efficiency is high
        if fitness_scores["performance"] < 0.6 and fitness_scores["efficiency"] > 0.7:
            strategy_weights["expansion"] = 0.7 * self.strategies["expansion"]["success_rate"]
        else:
            strategy_weights["expansion"] = 0.3 * self.strategies["expansion"]["success_rate"]

        # Strategy 2: Pruning - good when efficiency is low but performance is decent
        if fitness_scores["efficiency"] < 0.5 and fitness_scores["performance"] > 0.6:
            strategy_weights["pruning"] = 0.8 * self.strategies["pruning"]["success_rate"]
        else:
            strategy_weights["pruning"] = 0.3 * self.strategies["pruning"]["success_rate"]

        # Strategy 3: Restructuring - good when adaptability is low
        if fitness_scores["adaptability"] < 0.5:
            strategy_weights["restructuring"] = 0.7 * self.strategies["restructuring"]["success_rate"]
        else:
            strategy_weights["restructuring"] = 0.4 * self.strategies["restructuring"]["success_rate"]

        # Strategy 4: Specialization - good for complex systems with good performance
        if fitness_scores["complexity"] > 0.7 and fitness_scores["performance"] > 0.7:
            strategy_weights["specialization"] = 0.8 * self.strategies["specialization"]["success_rate"]
        else:
            strategy_weights["specialization"] = 0.2 * self.strategies["specialization"]["success_rate"]

        # Strategy 5: Integration - good for improving robustness
        if fitness_scores["robustness"] < 0.6:
            strategy_weights["integration"] = 0.7 * self.strategies["integration"]["success_rate"]
        else:
            strategy_weights["integration"] = 0.3 * self.strategies["integration"]["success_rate"]

        # Add randomness factor to promote exploration
        for strategy in strategy_weights:
            strategy_weights[strategy] += random.uniform(0, 0.3)

        # Select strategy with highest weight
        selected_strategy = max(strategy_weights.items(), key=lambda x: x[1])[0]

        return selected_strategy

    def _apply_evolution_strategy(self, agent, strategy, fitness_scores):
        """
        Apply the selected evolution strategy to modify the agent.
        Returns a tuple: (success, message, changes)
        """
        changes = []
        success = False  # Initialize success flag
        message = "Unknown result"  # Initialize message

        try:
            if strategy == "expansion":
                if hasattr(agent.model, 'expand_architecture'):
                    agent.model.expand_architecture()
                    changes.append("Expanded model architecture.")
                    success = True
                    message = "Expansion strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support expansion."
            elif strategy == "pruning":
                if hasattr(agent.model, 'contract_architecture'):
                    agent.model.contract_architecture()
                    changes.append("Contracted model architecture.")
                    success = True
                    message = "Pruning strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support pruning."
            elif strategy == "restructuring":
                changes.append("Reorganized internal connections.")
                success = True
                message = "Restructuring strategy applied successfully."
            elif strategy == "specialization":
                changes.append("Optimized specialized components.")
                success = True
                message = "Specialization strategy applied successfully."
            elif strategy == "integration":
                changes.append("Integrated separate capabilities.")
                success = True
                message = "Integration strategy applied successfully."
            elif "_blend" in strategy:
                # Handle blended strategies
                changes.append("Applied blended strategy with emergent properties")
                success = True
                message = f"Blended strategy {strategy} applied successfully."
            else:
                success = False
                message = f"Unknown strategy: {strategy}"
        except Exception as e:
            success = False
            message = f"Error applying strategy {strategy}: {str(e)}"

        return success, message, changes

    def _apply_expansion_strategy(self, agent, changes):
        """
        Apply expansion strategy to increase model capacity.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for expandable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'expand_architecture'):
            return False, "Model does not support architecture expansion", changes

        try:
            # Expansion attempts:
            # 1. Try expanding the model architecture
            agent.model.expand_architecture()
            changes.append("Expanded neural architecture with additional layers")

            # 2. If agent has adaptive learning, adjust its parameters
            if hasattr(agent, 'adaptive_learning'):
                # Increase adaptation rate for new architecture
                adaptive_learning = agent.adaptive_learning
                if hasattr(adaptive_learning, 'adaptation_threshold'):
                    old_threshold = adaptive_learning.adaptation_threshold
                    new_threshold = max(0.05, old_threshold * 0.9)  # More sensitive
                    adaptive_learning.adaptation_threshold = new_threshold
                    changes.append(f"Reduced adaptation threshold from {old_threshold:.2f} to {new_threshold:.2f}")

            # 3. Integrate imagination with learning system
            if (hasattr(agent, 'ai_manager') and
                hasattr(agent.ai_manager, 'imagination') and
                hasattr(agent, 'adaptive_learning')):

                imagination = agent.ai_manager.imagination
                learning = agent.adaptive_learning

                # Link imagination creativity to adaptation rate
                if hasattr(imagination, 'creativity_level') and hasattr(learning, 'adaptation_rate'):
                    creativity = imagination.creativity_level
                    old_rate = learning.adaptation_rate

                    # Higher creativity = higher adaptation rate
                    new_rate = 0.2 + 0.3 * creativity  # Range 0.2-0.5
                    learning.adaptation_rate = new_rate

                    changes.append(f"Integrated imagination creativity with learning adaptation: rate {old_rate:.2f} → {new_rate:.2f}")
                    integration_changes += 1

            # Update strategy success rate based on changes made
            if integration_changes > 0:
                self.strategies["integration"]["success_rate"] = min(0.95, self.strategies["integration"]["success_rate"] * 1.1)
                return True, f"Successfully integrated {integration_changes} component pairs for improved synergy", changes
            else:
                self.strategies["integration"]["success_rate"] = max(0.2, self.strategies["integration"]["success_rate"] * 0.9)
                return False, "No suitable components found for integration", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["integration"]["success_rate"] = max(0.2, self.strategies["integration"]["success_rate"] * 0.9)

            return False, f"Component integration failed: {str(e)}", changes

    def _assess_capability_levels(self, agent):
        """
        Assess current capability levels of the system across target areas.

        Parameters:
        - agent: Agent to assess

        Returns:
        - Dictionary of capability scores
        """
        capabilities = {}

        # 1. Knowledge representation capability
        kr_score = 0.5  # Default

        # Check for semantic memory capacity
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
            memory_size = len(agent.free_will.semantic_memory)
            # Scale based on size: 0-1000 items maps to 0.5-0.9 score
            kr_size_factor = min(0.4, memory_size / 2500)
            kr_score += kr_size_factor

        # Check for memory importance tracking
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'memory_importance'):
            kr_score += 0.1

        capabilities["knowledge_representation"] = min(0.95, kr_score)

        # 2. Planning capability
        planning_score = 0.4  # Default

        # Check for temporal planner
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
            planner = agent.ai_manager.temporal_planner
            planning_score += 0.2

            # Check for goal system
            if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                planning_score += min(0.2, len(planner.long_term_goals) * 0.05)

            # Check for reflection capability
            if hasattr(planner, 'reflect_and_adapt'):
                planning_score += 0.1

        capabilities["planning"] = min(0.95, planning_score)

        # 3. Learning capability
        learning_score = 0.3  # Default

        # Check for adaptive learning system
        if hasattr(agent, 'adaptive_learning'):
            learning_score += 0.3

            # Check for meta-learning capability
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
                learning_score += 0.3

        capabilities["learning"] = min(0.95, learning_score)

        # 4. Error handling capability
        error_score = 0.3  # Default

        # Check for imagination-based error detection
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            if hasattr(imagination, 'simulate_error_detection'):
                error_score += 0.3

            if hasattr(imagination, 'simulate_error_correction'):
                error_score += 0.2

        # Check for error recovery in AI manager
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'error_recovery_attempts') is not None:
            error_score += 0.1

        capabilities["error_handling"] = min(0.95, error_score)

        # 5. Creative synthesis capability
        creative_score = 0.2  # Default

        # Check for imagination engine
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            creative_score += 0.3

            # Check for creativity level
            if hasattr(imagination, 'creativity_level'):
                creative_score += imagination.creativity_level * 0.3

        # Check for consciousness system
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
            consciousness = agent.ai_manager.consciousness

            # Check for qualia simulation
            if hasattr(consciousness, 'qualia_simulation_active'):
                creative_score += 0.1

        capabilities["creative_synthesis"] = min(0.95, creative_score)

        # Store capability assessment
        self.capability_scores = capabilities

        return capabilities

    def get_evolution_report(self):
        """
        Generate a comprehensive report on system evolution status.

        Returns:
        - Dictionary with evolution status information
        """
        if not self.evolution_history:
            return {
                "status": "initialized",
                "generation": 0,
                "message": "Evolution engine initialized but no evolution cycles completed"
            }

        # Calculate success rate
        total_attempts = len(self.evolution_history)
        successful_attempts = sum(1 for record in self.evolution_history if record.get("success", False))
        success_rate = successful_attempts / total_attempts if total_attempts > 0 else 0

        # Get strategy effectiveness
        strategy_stats = {}
        for strategy, data in self.strategies.items():
            success_count = sum(1 for record in self.evolution_history
                             if record.get("strategy") == strategy and record.get("success", False))

            attempt_count = sum(1 for record in self.evolution_history
                             if record.get("strategy") == strategy)

            strategy_stats[strategy] = {
                "attempts": attempt_count,
                "successes": success_count,
                "success_rate": success_count / attempt_count if attempt_count > 0 else 0,
                "current_rating": data["success_rate"]
            }

        # Get recent evolutions
        recent_evolutions = []
        for record in self.evolution_history[-5:]:  # Last 5 evolutions
            recent_evolutions.append({
                "generation": record.get("generation", 0),
                "strategy": record.get("strategy", "unknown"),
                "success": record.get("success", False),
                "message": record.get("message", ""),
                "changes": record.get("changes", [])
            })

        # Calculate evolutionary convergence
        if len(self.fitness_metrics) >= 2:
            # Compare current fitness with previous generation
            current_gen = max(self.fitness_metrics.keys())
            prev_gen = max(k for k in self.fitness_metrics.keys() if k < current_gen)

            current_fitness = self.fitness_metrics[current_gen]
            prev_fitness = self.fitness_metrics[prev_gen]

            # Calculate average improvement across metrics
            improvements = []
            for metric in current_fitness:
                if metric in prev_fitness:
                    # For complexity, lower is better; for others, higher is better
                    if metric == "complexity":
                        change = prev_fitness[metric] - current_fitness[metric]
                    else:
                        change = current_fitness[metric] - prev_fitness[metric]
                    improvements.append(change)

            avg_improvement = sum(improvements) / len(improvements) if improvements else 0
            # Convergence increases as improvement diminishes
            self.convergence_score = 1.0 - min(1.0, abs(avg_improvement) * 10)

        # Generate recommendations for next evolution
        recommendations = []

        # Recommendation 1: Address capability gaps
        capability_gaps = []
        if self.capability_scores:
            for capability, target in self.capability_targets.items():
                current = self.capability_scores.get(capability, 0)
                if current < target and target - current > 0.2:
                    capability_gaps.append((capability, target - current))

            if capability_gaps:
                # Recommend addressing the largest gap
                largest_gap = max(capability_gaps, key=lambda x: x[1])
                recommendations.append({
                    "type": "capability_development",
                    "target": largest_gap[0],
                    "gap": largest_gap[1],
                    "recommendation": f"Prioritize evolution of {largest_gap[0].replace('_', ' ')} capability"
                })

        # Recommendation 2: Strategy adjustment based on success rates
        worst_strategy = min(strategy_stats.items(), key=lambda x: x[1]["success_rate"])[0]
        best_strategy = max(strategy_stats.items(), key=lambda x: x[1]["success_rate"])[0]

        if strategy_stats[worst_strategy]["success_rate"] < 0.3 and strategy_stats[worst_strategy]["attempts"] >= 3:
            recommendations.append({
                "type": "strategy_adjustment",
                "strategy": worst_strategy,
                "recommendation": f"Reduce use of {worst_strategy} strategy due to low success rate ({strategy_stats[worst_strategy]['success_rate']:.2f})"
            })

        # Recommendation 3: Based on convergence
        if self.convergence_score > 0.8:
            recommendations.append({
                "type": "convergence_response",
                "convergence_score": self.convergence_score,
                "recommendation": "System appears to be converging - increase mutation strength to explore new optima"
            })

        report = {
            "status": "evolving",
            "generation": self.evolution_generation,
            "attempts": total_attempts,
            "success_rate": success_rate,
            "last_major_evolution": self.last_major_evolution,
            "convergence_score": self.convergence_score,
            "strategy_stats": strategy_stats,
            "recent_evolutions": recent_evolutions,
            "capability_scores": self.capability_scores,
            "recommendations": recommendations
        }

        # Example of the corrected if statement block placed AFTER the report dictionary:
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'): # Correct indentation
            meta_learning = agent.ai_manager.meta_learning
            if hasattr(meta_learning, 'meta_params'): # Indented once more
                # Increase exploration after expansion # Indented twice more
                old_rate = meta_learning.meta_params.get("exploration_rate", 0.3) # Indented twice more
                new_rate = min(0.5, old_rate * 1.2)  # More exploration # Indented twice more
                meta_learning.meta_params["exploration_rate"] = new_rate # Indented twice more
                changes.append(f"Increased meta-learning exploration rate from {old_rate:.2f} to {new_rate:.2f}") # Indented twice more
        # Update strategy success rate # Indented once more
        self.strategies["expansion"]["success_rate"] = min(0.95, self.strategies["expansion"]["success_rate"] * 1.1) # Indented once more

        return report # The return statement is at the base level of the method and *outside* the if block now

    def _apply_pruning_strategy(self, agent, changes):
        """
        Apply pruning strategy to reduce model size and increase efficiency.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for prunable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'contract_architecture'):
            return False, "Model does not support architecture pruning", changes

        try:
            # Pruning attempts:
            # 1. Try contracting the model architecture
            agent.model.contract_architecture()
            changes.append("Contracted neural architecture by removing underutilized layers")

            # 2. If agent has memory systems, prune those too
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'contract_memory'):
                old_size = len(agent.free_will.memory_set) if hasattr(agent.free_will, 'memory_set') else 0

                # Calculate target size (80% of current)
                target_size = int(old_size * 0.8)
                if target_size > 0:
                    agent.free_will.contract_memory(target_size)
                    new_size = len(agent.free_will.memory_set) if hasattr(agent.free_will, 'memory_set') else 0
                    changes.append(f"Pruned memory from {old_size} to {new_size} items")

            # 3. If agent has semantic memory, optimize it
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
                if agent.free_will.semantic_memory:
                    old_count = len(agent.free_will.semantic_memory)
                    # Remove low importance memories
                    low_importance = []
                    threshold = 0.4  # Below this importance, consider pruning

                    for url, data in agent.free_will.semantic_memory.items():
                        if isinstance(data, dict) and "importance" in data:
                            if data["importance"] < threshold:
                                low_importance.append(url)

                    # Remove a portion of low importance items
                    prune_count = min(len(low_importance), int(old_count * 0.2))  # Prune up to 20%

                    for url in low_importance[:prune_count]:
                        if url in agent.free_will.semantic_memory:
                            del agent.free_will.semantic_memory[url]

                    new_count = len(agent.free_will.semantic_memory)
                    if new_count < old_count:
                        changes.append(f"Pruned semantic memory from {old_count} to {new_count} items")

            # Update strategy success rate
            self.strategies["pruning"]["success_rate"] = min(0.95, self.strategies["pruning"]["success_rate"] * 1.1)

            return True, "Successfully pruned system components to improve efficiency", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["pruning"]["success_rate"] = max(0.2, self.strategies["pruning"]["success_rate"] * 0.9)

            return False, f"Architecture pruning failed: {str(e)}", changes

    def _apply_restructuring_strategy(self, agent, changes):
        """
        Apply restructuring strategy to reorganize components without changing capacity.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Restructuring attempts:
            # 1. Modify consciousness parameters if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
                consciousness = agent.ai_manager.consciousness

                # Adjust awareness fluctuation rate
                if hasattr(consciousness, 'awareness_fluctuation_rate'):
                    old_rate = consciousness.awareness_fluctuation_rate
                    new_rate = old_rate * random.uniform(0.8, 1.2)  # Random adjustment
                    consciousness.awareness_fluctuation_rate = max(0.01, min(0.1, new_rate))
                    changes.append(f"Adjusted consciousness fluctuation rate: {old_rate:.3f} → {new_rate:.3f}")

                # Reset state to "reflective" to encourage reassessment
                if hasattr(consciousness, 'current_state'):
                    old_state = consciousness.current_state
                    consciousness.current_state = "reflective"
                    changes.append(f"Reset consciousness state from '{old_state}' to 'reflective'")

                # Boost awareness temporarily
                if hasattr(consciousness, 'increase_awareness'):
                    consciousness.increase_awareness(0.2)
                    changes.append("Temporarily boosted consciousness awareness for restructuring")

            # 2. Modify temporal planner goals if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
                planner = agent.ai_manager.temporal_planner

                # Rebalance goal priorities
                if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                    # Shuffle priorities
                    old_priorities = {}
                    for goal in planner.long_term_goals:
                        old_priorities[goal.get("id", "unknown")] = goal.get("priority", 0.5)

                    # Create new distribution
                    total_priority = sum(goal.get("priority", 0.5) for goal in planner.long_term_goals)
                    avg_priority = total_priority / len(planner.long_term_goals)

                    # Invert relative to average
                    for goal in planner.long_term_goals:
                        goal_id = goal.get("id", "unknown")
                        old_priority = goal.get("priority", 0.5)

                        # Calculate new priority as reflection around average
                        new_priority = avg_priority + (avg_priority - old_priority)
                        # Ensure it's in valid range
                        new_priority = max(0.1, min(1.0, new_priority))

                        # Apply new priority
                        goal["priority"] = new_priority

                    changes.append("Rebalanced long-term goal priorities to shift focus")

                # Refresh short-term goals immediately
                if hasattr(planner, 'refresh_short_term_goals'):
                    planner.refresh_short_term_goals()
                    changes.append("Regenerated short-term goals based on restructured priorities")

            # 3. Modify imagination engine if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
                imagination = agent.ai_manager.imagination

                # Adjust creativity level
                if hasattr(imagination, 'creativity_level'):
                    old_level = imagination.creativity_level
                    new_level = 1.0 - old_level  # Invert creativity level
                    imagination.creativity_level = new_level
                    changes.append(f"Inverted imagination creativity level: {old_level:.2f} → {new_level:.2f}")

                # Change current mode
                if hasattr(imagination, 'current_mode') and hasattr(imagination, 'cognitive_modes'):
                    old_mode = imagination.current_mode
                    # Select a different mode
                    available_modes = [m for m in imagination.cognitive_modes if m != old_mode]
                    if available_modes:
                        new_mode = random.choice(available_modes)
                        imagination.current_mode = new_mode
                        changes.append(f"Switched imagination cognitive mode: '{old_mode}' → '{new_mode}'")

            # Update strategy success rate
            if changes:
                self.strategies["restructuring"]["success_rate"] = min(0.95, self.strategies["restructuring"]["success_rate"] * 1.1)
                return True, "Successfully restructured internal cognitive components", changes
            else:
                self.strategies["restructuring"]["success_rate"] = max(0.2, self.strategies["restructuring"]["success_rate"] * 0.9)
                return False, "No suitable components found for restructuring", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["restructuring"]["success_rate"] = max(0.2, self.strategies["restructuring"]["success_rate"] * 0.9)

            return False, f"Component restructuring failed: {str(e)}", changes

    def _apply_specialization_strategy(self, agent, changes):

        try:
            # Identify specialized modules to optimize
            specialized_changes = 0

            # 1. Specialize content sifter if present
            if hasattr(agent, 'content_sifter'):
                sifter = agent.content_sifter

                # Customize topic focus
                if hasattr(sifter, 'topics_of_interest'):
                    # Pick top 3 priorities from temporal planner if available
                    priority_topics = []

                    if (hasattr(agent, 'ai_manager') and
                        hasattr(agent.ai_manager, 'temporal_planner') and
                        hasattr(agent.ai_manager.temporal_planner, 'long_term_goals')):

                        # Extract keywords from highest priority goals
                        goals = sorted(agent.ai_manager.temporal_planner.long_term_goals,
                                     key=lambda g: g.get("priority", 0), reverse=True)

                        for goal in goals[:3]:
                            desc = goal.get("description", "").lower()
                            words = desc.split()
                            important_words = [w for w in words if len(w) > 4]
                            priority_topics.extend(important_words[:2])  # 2 keywords per goal

                    # Add some general technology topics
                    tech_topics = ["artificial intelligence", "quantum computing",
                                  "neural networks", "machine learning"]

                    # Combine maintaining 30% of original topics for diversity
                    old_topics = sifter.topics_of_interest
                    keep_count = max(3, int(len(old_topics) * 0.3))
                    kept_topics = random.sample(old_topics, keep_count)

                    # Create new specialized topic list
                    new_topics = kept_topics + priority_topics + tech_topics
                    # Remove duplicates
                    new_topics = list(dict.fromkeys(new_topics))

                    # Apply change
                    sifter.topics_of_interest = new_topics
                    specialized_changes += 1
                    changes.append(f"Specialized content sifter with {len(new_topics)} focused topics")

            # 2. Specialize free will parameters if present
            if hasattr(agent, 'free_will'):
                free_will = agent.free_will

                # Adjust exploration/exploitation balance
                if hasattr(free_will, 'exploration_weight') and hasattr(free_will, 'exploitation_weight'):
                    # Identify system state - are we specialized enough already?
                    if hasattr(agent, 'stats') and 'domains_visited' in agent.stats:
                        domains_count = len(agent.stats['domains_visited'])

                        # If we've visited many domains, increase exploitation
                        if domains_count > 20:
                            old_expl = free_will.exploration_weight
                            old_expt = free_will.exploitation_weight

                            # Shift toward exploitation
                            free_will.exploitation_weight = min(0.8, old_expt + 0.1)
                            free_will.exploration_weight = 1.0 - free_will.exploitation_weight

                            changes.append(f"Specialized free will toward exploitation: {old_expl:.2f}/{old_expt:.2f} → {free_will.exploration_weight:.2f}/{free_will.exploitation_weight:.2f}")
                            specialized_changes += 1

            # 3. Specialize imagination engine if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
                imagination = agent.ai_manager.imagination

                # Specialize domain expertise
                if hasattr(imagination, 'domains'):
                    domains = imagination.domains

                    # Identify top 2 domains based on current expertise
                    top_domains = sorted(domains.items(), key=lambda x: x[1], reverse=True)[:2]

                    # Boost top domains further
                    for domain, expertise in top_domains:
                        old_expertise = expertise
                        new_expertise = min(0.95, old_expertise + 0.1)
                        imagination.domains[domain] = new_expertise
                        changes.append(f"Specialized imagination expertise in {domain}: {old_expertise:.2f} → {new_expertise:.2f}")

                    specialized_changes += 1

            # Update strategy success rate based on changes made
            if specialized_changes > 0:
                self.strategies["specialization"]["success_rate"] = min(0.95, self.strategies["specialization"]["success_rate"] * 1.1)
                return True, f"Successfully specialized {specialized_changes} components for improved focus", changes
            else:
                self.strategies["specialization"]["success_rate"] = max(0.2, self.strategies["specialization"]["success_rate"] * 0.9)
                return False, "No suitable components found for specialization", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["specialization"]["success_rate"] = max(0.2, self.strategies["specialization"]["success_rate"] * 0.9)

            return False, f"Component specialization failed: {str(e)}", changes
    def _apply_integration_strategy(self, agent, changes):
        """
        Apply integration strategy: combine previously separate capabilities.
        """
        try:
            integration_changes = 0

            # 1. Integrate consciousness with planning if possible
            if (hasattr(agent, 'ai_manager') and
                hasattr(agent.ai_manager, 'consciousness') and
                hasattr(agent.ai_manager, 'temporal_planner')):

                consciousness = agent.ai_manager.consciousness
                planner = agent.ai_manager.temporal_planner

                if hasattr(consciousness, 'awareness_level') and hasattr(planner, 'reflection_interval'):
                    awareness = consciousness.awareness_level
                    old_interval = planner.reflection_interval
                    new_interval = int(max(5, 30 - 25 * awareness))
                    planner.reflection_interval = new_interval

                    changes.append(f"Integrated consciousness awareness with planning reflection: interval {old_interval} → {new_interval}")
                    integration_changes += 1

            # 2. Integrate free will with content filtering
            if hasattr(agent, 'free_will') and hasattr(agent, 'content_sifter'):
                free_will = agent.free_will
                sifter = agent.content_sifter

                if hasattr(free_will, 'memory_importance') and hasattr(sifter, 'topics_of_interest'):
                    important_urls = []
                    for url, importance in free_will.memory_importance.items():
                        if importance > 0.7:
                            important_urls.append(url)

                    if important_urls and hasattr(free_will, 'semantic_memory'):
                        new_topics = []
                        for url in important_urls[:5]:
                            if url in free_will.semantic_memory:
                                memory = free_will.semantic_memory[url]
                                keywords = memory.get("keywords", [])
                                if keywords:
                                    new_topics.extend(keywords[:3])
                        if new_topics:
                            old_topics = set(sifter.topics_of_interest)
                            combined_topics = list(old_topics.union(set(new_topics)))
                            sifter.topics_of_interest = combined_topics

                            changes.append(f"Integrated free will memory importance with content filtering: added {len(new_topics)} new topics of interest")
                            integration_changes += 1
        except Exception as e:
            return False, f"Integration strategy failed: {str(e)}", changes

        if integration_changes > 0:
            return True, f"Integration strategy applied with {integration_changes} changes", changes
        else:
            return False, "No integration changes applied", changes

    # The following methods below remain unchanged from your original implementation.
    def _generate_reflective_thought(self, context):
        thought = {
            "type": "reflective",
            "content": "Meta-cognitive assessment",
            "reflections": [],
            "insights": [],
            "importance": 0.5
        }
        reflection_areas = [
            "learning_progress",
            "strategy_effectiveness",
            "error_patterns",
            "goal_alignment",
            "cognitive_bias"
        ]
        selected_areas = random.sample(reflection_areas, min(3, len(reflection_areas)))
        reflections = []
        history_span = min(10, len(self.thought_history))
        recent_thoughts = self.thought_history[-history_span:] if history_span > 0 else []
        for area in selected_areas:
            if area == "learning_progress":
                if "stats" in context and isinstance(context["stats"], dict):
                    cycles = context["stats"].get("cycles_run", 0)
                    pages = context["stats"].get("pages_processed", 0)
                    if cycles > 0:
                        learning_rate = pages / max(1, cycles)
                        reflections.append({
                            "area": "learning_progress",
                            "content": f"Learning rate: {learning_rate:.2f} pages per cycle",
                            "assessment": "satisfactory" if learning_rate > 0.5 else "needs improvement"
                        })
            elif area == "strategy_effectiveness":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = self.agent.planner_sifter.strategies
                    if strategies:
                        sorted_strategies = sorted(
                            [(name, data.get("effectiveness", 0.5)) for name, data in strategies.items()],
                            key=lambda x: x[1],
                            reverse=True
                        )
                        if sorted_strategies:
                            best = sorted_strategies[0]
                            worst = sorted_strategies[-1]
                            reflections.append({
                                "area": "strategy_effectiveness",
                                "content": f"Most effective strategy: {best[0]} ({best[1]:.2f}), Least effective: {worst[0]} ({worst[1]:.2f})",
                                "recommendation": f"Consider increasing use of {best[0]} approach"
                            })
            elif area == "error_patterns":
                if recent_thoughts:
                    error_thoughts = [t for t in recent_thoughts if "error" in str(t.get("content", "")).lower()]
                    if error_thoughts:
                        error_count = len(error_thoughts)
                        reflections.append({
                            "area": "error_patterns",
                            "content": f"Error-related thoughts appearing in {error_count}/{len(recent_thoughts)} recent thoughts",
                            "assessment": "concerning" if error_count > 3 else "manageable"
                        })
            elif area == "goal_alignment":
                if ("current_goal" in context and "recent_actions" in context and isinstance(context["recent_actions"], list)):
                    goal = context["current_goal"].get("description", "") if isinstance(context["current_goal"], dict) else ""
                    if goal:
                        actions = context["recent_actions"]
                        aligned_count = 0
                        for action in actions:
                            if isinstance(action, dict) and "action" in action:
                                action_type = action["action"]
                                if "explore" in goal.lower() and action_type in ["expand", "search"]:
                                    aligned_count += 1
                                elif "deep" in goal.lower() and action_type in ["evaluate", "adapt"]:
                                    aligned_count += 1
                        alignment_rate = aligned_count / max(1, len(actions))
                        reflections.append({
                            "area": "goal_alignment",
                            "content": f"Action-goal alignment rate: {alignment_rate:.2f}",
                            "assessment": "well-aligned" if alignment_rate > 0.6 else "misaligned"
                        })
            elif area == "cognitive_bias":
                potential_biases = []
                if recent_thoughts:
                    modes = [t.get("mode") for t in recent_thoughts]
                    mode_counts = {}
                    for mode in modes:
                        if mode:
                            mode_counts[mode] = mode_counts.get(mode, 0) + 1
                    most_common = max(mode_counts.items(), key=lambda x: x[1]) if mode_counts else (None, 0)
                    if most_common[1] > len(recent_thoughts) * 0.7:
                        potential_biases.append(f"Possible cognitive fixation on {most_common[0]} thinking mode")
                if "recent_actions" in context and isinstance(context["recent_actions"], list):
                    actions = context["recent_actions"]
                    if len(actions) >= 2:
                        last_action = actions[-1]
                        if isinstance(last_action, dict) and last_action.get("success", False):
                            potential_biases.append("Potential recency bias - overweighting last successful action")
                if potential_biases:
                    reflections.append({
                        "area": "cognitive_bias",
                        "content": potential_biases[0],
                        "recommendation": "Consider deliberate perspective shifts to counteract"
                    })
        insights = []
        for reflection in reflections:
            area = reflection.get("area", "")
            assessment = reflection.get("assessment", "")
            recommendation = reflection.get("recommendation", "")
            if area == "learning_progress" and assessment == "needs improvement":
                insights.append("Learning rate below target - consider adjusting exploration/exploitation balance")
            elif area == "strategy_effectiveness" and recommendation:
                insights.append(recommendation)
            elif area == "error_patterns" and assessment == "concerning":
                insights.append("High error rate - implement additional validation measures")
            elif area == "goal_alignment" and assessment == "misaligned":
                insights.append("Actions not well-aligned with goals - revisit planning process")
            elif area == "cognitive_bias" and recommendation:
                insights.append(recommendation)
        thought["reflections"] = reflections
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if "concerning" in str(reflections) or "misaligned" in str(reflections):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif reflections:
            thought["content"] = reflections[0].get("content", "Meta-cognitive reflection")
        return thought

    def evolve_system(self, agent):
        """
        Trigger system evolution based on performance metrics, goals,
        and environmental requirements.

        Parameters:
          - agent: Agent instance to evolve

        Returns:
          - (success, message) tuple
        """
        # Basic validation
        if not agent:
            return False, "Invalid agent provided to evolution engine."

        # Determine current cycle count
        cycle_count = 0
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'cycle_counter'):
            cycle_count = agent.ai_manager.cycle_counter
        else:
            cycle_count = getattr(agent, 'stats', {}).get('cycles_run', 0)

        # Evolution interval check (don't evolve too frequently)
        evolution_interval = SELF_MODIFY_INTERVAL  # Using global configuration
        cycles_since_last = cycle_count - self.last_major_evolution

        if cycles_since_last < evolution_interval:
            return False, f"Evolution interval not reached. {cycles_since_last}/{evolution_interval} cycles since last evolution."

        # Increase generation counter
        self.evolution_generation += 1

        # Analyze current state and fitness
        fitness_scores = self._evaluate_system_fitness(agent)
        evolutionary_pressure = self._calculate_evolutionary_pressure(fitness_scores)

        # Determine if evolution should occur based on pressure and randomness
        evolution_threshold = 0.3  # Base threshold
        evolution_probability = evolution_threshold + evolutionary_pressure

        if random.random() >= evolution_probability:
            return False, f"Evolution check passed, but probability threshold not met ({evolution_probability:.2f})."

        # System will evolve - select strategy
        selected_strategy = self._select_evolution_strategy(fitness_scores)

        # Apply the selected evolutionary strategy
        success, message, changes = self._apply_evolution_strategy(agent, selected_strategy, fitness_scores)

        # Record the evolution attempt
        evolution_record = {
            "generation": self.evolution_generation,
            "cycle": cycle_count,
            "strategy": selected_strategy,
            "success": success,
            "message": message,
            "changes": changes,
            "fitness_before": fitness_scores,
            "timestamp": datetime.now().isoformat()
        }
        self.evolution_history.append(evolution_record)

        if success:
            self.last_major_evolution = cycle_count
            log_event(f"System evolution successful: Generation {self.evolution_generation}, Strategy: {selected_strategy}", "QUANTUM")
        else:
            log_event(f"System evolution attempt failed: {message}", "WARNING")

        return success, message

    def _evaluate_system_fitness(self, agent):
        """
        Evaluate the fitness of the current system across multiple dimensions.
        (Placeholder implementation.)
        """
        fitness_scores = {
            "performance": 0.5,
            "efficiency": 0.5,
            "adaptability": 0.5,
            "robustness": 0.5,
            "complexity": 0.5
        }
        # Example: if agent.stats exists, use simple ratio as performance
        if hasattr(agent, 'stats') and isinstance(agent.stats, dict):
            cycles = agent.stats.get("cycles_run", 0)
            pages = agent.stats.get("pages_processed", 0)
            if cycles > 0:
                performance = pages / cycles
                fitness_scores["performance"] = max(0.1, min(0.9, performance / 10))
        return fitness_scores

    def _calculate_evolutionary_pressure(self, fitness_scores):
        """
        Calculate the evolutionary pressure based on fitness scores and goal weights.
        Lower overall fitness implies higher pressure.
        """
        total_weight = sum(abs(w) for w in self.goal_weights.values())
        weighted_sum = 0.0
        for metric, weight in self.goal_weights.items():
            score = fitness_scores.get(metric, 0.5)
            if weight < 0:
                score = 1 - score
            weighted_sum += score * abs(weight)
        avg_fitness = weighted_sum / total_weight if total_weight != 0 else 0.5
        pressure = (1 - avg_fitness) * self.adaptation_rate
        return pressure

    def _select_evolution_strategy(self, fitness_scores):
        """
        Select an appropriate evolution strategy based on the current fitness scores.
        Here we choose the strategy corresponding to the capability with the largest gap.
        """
        gaps = {}
        for metric, target in self.capability_targets.items():
            current = fitness_scores.get(metric, 0.5)
            gaps[metric] = target - current
        most_lacking = max(gaps.items(), key=lambda x: x[1])[0]
        mapping = {
            "knowledge_representation": "expansion",
            "planning": "integration",
            "learning": "specialization",
            "error_handling": "pruning",
            "creative_synthesis": "restructuring"
        }
        return mapping.get(most_lacking, "expansion")

    def _apply_evolution_strategy(self, agent, strategy, fitness_scores):
        """
        Apply the selected evolution strategy to modify the agent.
        Returns a tuple: (success, message, changes)
        """
        changes = []
        try:
            if strategy == "expansion":
                if hasattr(agent.model, 'expand_architecture'):
                    agent.model.expand_architecture()
                    changes.append("Expanded model architecture.")
                    success = True
                    message = "Expansion strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support expansion."
            elif strategy == "pruning":
                if hasattr(agent.model, 'contract_architecture'):
                    agent.model.contract_architecture()
                    changes.append("Contracted model architecture.")
                    success = True
                    message = "Pruning strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support pruning."
            elif strategy == "restructuring":
                changes.append("Reorganized internal connections.")
                success = True
                message = "Restructuring strategy applied successfully."
            elif strategy == "specialization":
                changes.append("Optimized specialized components.")
                success = True
                message = "Specialization strategy applied successfully."
            elif strategy == "integration":
                changes.append("Integrated separate capabilities.")
                success = True
                message = "Integration strategy applied successfully."
            else:
                success = False
                message = f"Unknown strategy: {strategy}"
        except Exception as e:
            success = False
            message = f"Error applying strategy {strategy}: {str(e)}"
        return success, message, changes

    def _generate_reflective_thought(self, context):
        """Generate a reflective thought focused on self-examination"""
        thought = {
            "type": "reflective",
            "content": "Meta-cognitive assessment",
            "reflections": [],
            "insights": [],
            "importance": 0.5
        }
        reflection_areas = [
            "learning_progress",
            "strategy_effectiveness",
            "error_patterns",
            "goal_alignment",
            "cognitive_bias"
        ]
        selected_areas = random.sample(reflection_areas, min(3, len(reflection_areas)))
        reflections = []
        history_span = min(10, len(self.thought_history))
        recent_thoughts = self.thought_history[-history_span:] if history_span > 0 else []
        for area in selected_areas:
            if area == "learning_progress":
                if "stats" in context and isinstance(context["stats"], dict):
                    cycles = context["stats"].get("cycles_run", 0)
                    pages = context["stats"].get("pages_processed", 0)
                    if cycles > 0:
                        learning_rate = pages / max(1, cycles)
                        reflections.append({
                            "area": "learning_progress",
                            "content": f"Learning rate: {learning_rate:.2f} pages per cycle",
                            "assessment": "satisfactory" if learning_rate > 0.5 else "needs improvement"
                        })
            elif area == "strategy_effectiveness":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = self.agent.planner_sifter.strategies
                    if strategies:
                        sorted_strategies = sorted(
                            [(name, data.get("effectiveness", 0.5)) for name, data in strategies.items()],
                            key=lambda x: x[1],
                            reverse=True
                        )
                        if sorted_strategies:
                            best = sorted_strategies[0]
                            worst = sorted_strategies[-1]
                            reflections.append({
                                "area": "strategy_effectiveness",
                                "content": f"Most effective strategy: {best[0]} ({best[1]:.2f}), Least effective: {worst[0]} ({worst[1]:.2f})",
                                "recommendation": f"Consider increasing use of {best[0]} approach"
                            })
            elif area == "error_patterns":
                if recent_thoughts:
                    error_thoughts = [t for t in recent_thoughts if "error" in str(t.get("content", "")).lower()]
                    if error_thoughts:
                        error_count = len(error_thoughts)
                        reflections.append({
                            "area": "error_patterns",
                            "content": f"Error-related thoughts appearing in {error_count}/{len(recent_thoughts)} recent thoughts",
                            "assessment": "concerning" if error_count > 3 else "manageable"
                        })
            elif area == "goal_alignment":
                if ("current_goal" in context and "recent_actions" in context and isinstance(context["recent_actions"], list)):
                    goal = context["current_goal"].get("description", "") if isinstance(context["current_goal"], dict) else ""
                    if goal:
                        actions = context["recent_actions"]
                        aligned_count = 0
                        for action in actions:
                            if isinstance(action, dict) and "action" in action:
                                action_type = action["action"]
                                if "explore" in goal.lower() and action_type in ["expand", "search"]:
                                    aligned_count += 1
                                elif "deep" in goal.lower() and action_type in ["evaluate", "adapt"]:
                                    aligned_count += 1
                        alignment_rate = aligned_count / max(1, len(actions))
                        reflections.append({
                            "area": "goal_alignment",
                            "content": f"Action-goal alignment rate: {alignment_rate:.2f}",
                            "assessment": "well-aligned" if alignment_rate > 0.6 else "misaligned"
                        })
            elif area == "cognitive_bias":
                potential_biases = []
                if recent_thoughts:
                    modes = [t.get("mode") for t in recent_thoughts]
                    mode_counts = {}
                    for mode in modes:
                        if mode:
                            mode_counts[mode] = mode_counts.get(mode, 0) + 1
                    most_common = max(mode_counts.items(), key=lambda x: x[1]) if mode_counts else (None, 0)
                    if most_common[1] > len(recent_thoughts) * 0.7:
                        potential_biases.append(f"Possible cognitive fixation on {most_common[0]} thinking mode")
                if "recent_actions" in context and isinstance(context["recent_actions"], list):
                    actions = context["recent_actions"]
                    if len(actions) >= 2:
                        last_action = actions[-1]
                        if isinstance(last_action, dict) and last_action.get("success", False):
                            potential_biases.append("Potential recency bias - overweighting last successful action")
                if potential_biases:
                    reflections.append({
                        "area": "cognitive_bias",
                        "content": potential_biases[0],
                        "recommendation": "Consider deliberate perspective shifts to counteract"
                    })
        insights = []
        for reflection in reflections:
            area = reflection.get("area", "")
            assessment = reflection.get("assessment", "")
            recommendation = reflection.get("recommendation", "")
            if area == "learning_progress" and assessment == "needs improvement":
                insights.append("Learning rate below target - consider adjusting exploration/exploitation balance")
            elif area == "strategy_effectiveness" and recommendation:
                insights.append(recommendation)
            elif area == "error_patterns" and assessment == "concerning":
                insights.append("High error rate - implement additional validation measures")
            elif area == "goal_alignment" and assessment == "misaligned":
                insights.append("Actions not well-aligned with goals - revisit planning process")
            elif area == "cognitive_bias" and recommendation:
                insights.append(recommendation)
        thought["reflections"] = reflections
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if "concerning" in str(reflections) or "misaligned" in str(reflections):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif reflections:
            thought["content"] = reflections[0].get("content", "Meta-cognitive reflection")
        return thought

    def _generate_exploratory_thought(self, context):
        """Generate an exploratory thought focused on new possibilities"""
        thought = {
            "type": "exploratory",
            "content": "Exploration of new possibilities",
            "directions": [],
            "insights": [],
            "importance": 0.5
        }
        exploration_areas = [
            "unknown_domains",
            "connection_patterns",
            "alternative_strategies",
            "capability_expansion"
        ]
        selected_areas = random.sample(exploration_areas, min(2, len(exploration_areas)))
        directions = []
        for area in selected_areas:
            if area == "unknown_domains":
                if "domains_visited" in context:
                    domains_visited = context.get("domains_visited", set())
                    if isinstance(domains_visited, set):
                        candidate_domains = [
                            "research.science", "github.com", "en.wikipedia.org",
                            "arxiv.org", "semanticscholar.org", "openai.com",
                            "nature.com", "reddit.com/r/MachineLearning"
                        ]
                        unvisited = [d for d in candidate_domains if d not in domains_visited]
                        if unvisited:
                            sample_domains = random.sample(unvisited, min(3, len(unvisited)))
                            directions.append({
                                "area": "unknown_domains",
                                "content": f"Explore high-value domains: {', '.join(sample_domains)}",
                                "rationale": "Expanding domain knowledge diversity"
                            })
            elif area == "connection_patterns":
                directions.append({
                    "area": "connection_patterns",
                    "content": "Implement knowledge graph traversal to find distant connections",
                    "rationale": "Distant domains often contain valuable cross-applicable patterns"
                })
            elif area == "alternative_strategies":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = list(self.agent.planner_sifter.strategies.keys())
                    if strategies:
                        if (hasattr(self.agent.planner_sifter, 'strategy_usage')):
                            usage = self.agent.planner_sifter.strategy_usage
                            least_used = sorted([(s, usage.get(s, 0)) for s in strategies], key=lambda x: x[1])
                            if least_used:
                                least_used_strategy = least_used[0][0]
                                directions.append({
                                    "area": "alternative_strategies",
                                    "content": f"Experiment with underutilized strategy: {least_used_strategy}",
                                    "rationale": "Diversifying strategic approaches to discover new optima"
                                })
            elif area == "capability_expansion":
                directions.append({
                    "area": "capability_expansion",
                    "content": "Develop enhanced semantic reasoning module",
                    "rationale": "Would significantly improve knowledge integration capabilities"
                })
        insights = []
        for direction in directions:
            area = direction.get("area", "")
            rationale = direction.get("rationale", "")
            if area == "unknown_domains":
                insights.append("Systematic exploration of high-value domains should be prioritized")
            elif area == "connection_patterns":
                insights.append("Knowledge value may lie in unexpected cross-domain connections")
            elif area == "alternative_strategies":
                insights.append("Strategic diversification may uncover more effective approaches")
            elif area == "capability_expansion":
                insights.append("Capability development should focus on knowledge integration")
        thought["directions"] = directions
        thought["insights"] = insights
        if insights:
            thought["importance"] = 0.6 + 0.1 * len(insights)
        if insights:
            thought["content"] = insights[0]
        elif directions:
            thought["content"] = directions[0].get("content", "Exploration suggestion")
        return thought

    def _generate_critical_thought(self, context):
        """Generate a critical thought focused on evaluation and assessment"""
        thought = {
            "type": "critical",
            "content": "Critical evaluation",
            "assessments": [],
            "insights": [],
            "importance": 0.5
        }
        assessment_areas = [
            "information_quality",
            "reasoning_validity",
            "strategy_efficiency",
            "resource_allocation",
            "error_handling"
        ]
        selected_areas = random.sample(assessment_areas, min(2, len(assessment_areas)))
        assessments = []
        for area in selected_areas:
            if area == "information_quality":
                if (hasattr(self.agent, 'free_will') and
                    hasattr(self.agent.free_will, 'domain_intelligence')):
                    intelligence = self.agent.free_will.domain_intelligence
                    if hasattr(intelligence, 'domain_knowledge'):
                        knowledge = intelligence.domain_knowledge
                        if knowledge:
                            low_quality_domains = []
                            for domain, data in knowledge.items():
                                if isinstance(data, dict) and data.get("content_quality", 1.0) < 0.5:
                                    low_quality_domains.append(domain)
                            if low_quality_domains:
                                sample_domains = random.sample(low_quality_domains, min(2, len(low_quality_domains)))
                                assessments.append({
                                    "area": "information_quality",
                                    "content": f"Low-quality content detected in domains: {', '.join(sample_domains)}",
                                    "recommendation": "Implement stricter quality thresholds for these domains"
                                })
                            else:
                                assessments.append({
                                    "area": "information_quality",
                                    "content": "Overall information quality appears acceptable",
                                    "recommendation": "Continue regular quality monitoring"
                                })
            elif area == "reasoning_validity":
                if self.thought_history:
                    recent_thoughts = self.thought_history[-10:] if len(self.thought_history) >= 10 else self.thought_history
                    thought_types = set(t.get("type", "") for t in recent_thoughts)
                    if len(thought_types) < 3:
                        assessments.append({
                            "area": "reasoning_validity",
                            "content": f"Limited cognitive diversity detected (only {len(thought_types)} thought types used recently)",
                            "recommendation": "Deliberately activate more diverse thinking modes"
                        })
                    else:
                        assessments.append({
                            "area": "reasoning_validity",
                            "content": f"Cognitive diversity appears healthy ({len(thought_types)} different thought types used recently)",
                            "recommendation": "Maintain diverse thinking patterns"
                        })
            elif area == "strategy_efficiency":
                if (hasattr(self.agent, 'stats') and
                    isinstance(self.agent.stats, dict) and
                    'cycles_run' in self.agent.stats and
                    'pages_processed' in self.agent.stats):
                    cycles = self.agent.stats['cycles_run']
                    pages = self.agent.stats['pages_processed']
                    if cycles > 10:
                        efficiency = pages / max(1, cycles)
                        if efficiency < 0.5:
                            assessments.append({
                                "area": "strategy_efficiency",
                                "content": f"Low efficiency detected: {efficiency:.2f} pages/cycle",
                                "recommendation": "Consider more aggressive pruning of low-value paths"
                            })
                        else:
                            assessments.append({
                                "area": "strategy_efficiency",
                                "content": f"Acceptable efficiency: {efficiency:.2f} pages/cycle",
                                "recommendation": "Continue current approach with regular efficiency monitoring"
                            })
            elif area == "resource_allocation":
                if (hasattr(self.agent, 'memory') and
                    hasattr(self.agent, 'free_will') and
                    hasattr(self.agent.free_will, 'memory_set')):
                    memory_usage = len(self.agent.memory) / MEMORY_MAX_SIZE
                    urls_stored = len(self.agent.free_will.memory_set)
                    if memory_usage > 0.8 and urls_stored > 1000:
                        assessments.append({
                            "area": "resource_allocation",
                            "content": f"High memory usage ({memory_usage:.0%}) with {urls_stored} URLs stored",
                            "recommendation": "Implement more aggressive memory pruning strategy"
                        })
                    else:
                        assessments.append({
                            "area": "resource_allocation",
                            "content": f"Memory usage ({memory_usage:.0%}) with {urls_stored} URLs stored",
                            "recommendation": "Current resource allocation appears appropriate"
                        })
            elif area == "error_handling":
                if (hasattr(self.agent, 'domain_stats') and isinstance(self.agent.domain_stats, dict)):
                    total_visits = sum(d.get("visits", 0) for d in self.agent.domain_stats.values())
                    total_errors = sum(d.get("error_count", 0) for d in self.agent.domain_stats.values())
                    if total_visits > 0:
                        error_rate = total_errors / total_visits
                        if error_rate > 0.2:
                            assessments.append({
                                "area": "error_handling",
                                "content": f"High system-wide error rate: {error_rate:.2%}",
                                "recommendation": "Investigate error causes and implement more robust handling"
                            })
                        else:
                            assessments.append({
                                "area": "error_handling",
                                "content": f"Acceptable error rate: {error_rate:.2%}",
                                "recommendation": "Continue monitoring error patterns"
                            })
        insights = []
        for assessment in assessments:
            recommendation = assessment.get("recommendation", "")
            if "low-quality" in assessment.get("content", "").lower():
                insights.append("Information quality control needs improvement")
            elif "limited cognitive diversity" in assessment.get("content", "").lower():
                insights.append("Need to activate more diverse thinking patterns")
            elif "low efficiency" in assessment.get("content", "").lower():
                insights.append("Strategy efficiency requires optimization")
            elif "high memory usage" in assessment.get("content", "").lower():
                insights.append("Memory management needs more efficient pruning")
            elif "high system-wide error" in assessment.get("content", "").lower():
                insights.append("Error handling systems require review and enhancement")
            elif recommendation:
                insights.append(recommendation)
        thought["assessments"] = assessments
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if any("high" in a.get("content", "").lower() for a in assessments):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif assessments:
            thought["content"] = assessments[0].get("content", "Critical assessment")
        return thought

    def _generate_integrative_thought(self, context):
        """Generate an integrative thought focused on knowledge synthesis"""
        thought = {
            "type": "integrative",
            "content": "Knowledge synthesis",
            "connections": [],
            "insights": [],
            "importance": 0.5
        }
        connections = []
        if not context:
            thought["content"] = "Insufficient knowledge for meaningful integration"
            return thought
        elements = {}
        if "domains_visited" in context:
            domains = context.get("domains_visited", set())
            if isinstance(domains, set) and len(domains) > 3:
                elements["domains"] = list(domains)
        if hasattr(self.agent, 'content_sifter'):
            sifter = self.agent.content_sifter
            if hasattr(sifter, 'topics_of_interest'):
                elements["topics"] = sifter.topics_of_interest
        if (hasattr(self.agent, 'ai_manager') and
            hasattr(self.agent.ai_manager, 'temporal_planner') and
            hasattr(self.agent.ai_manager.temporal_planner, 'long_term_goals')):
            goals = self.agent.ai_manager.temporal_planner.long_term_goals
            if goals:
                elements["goals"] = [g.get("description", "") for g in goals]
        if len(elements) >= 2:
            element_types = list(elements.keys())
            type_pair = random.sample(element_types, 2)
            elements_1 = elements[type_pair[0]]
            elements_2 = elements[type_pair[1]]
            if elements_1 and elements_2:
                element_1 = random.choice(elements_1)
                element_2 = random.choice(elements_2)
                connections.append({
                    "elements": [
                        {"type": type_pair[0], "value": element_1},
                        {"type": type_pair[1], "value": element_2}
                    ],
                    "connection_type": "integration",
                    "description": f"Integration of {type_pair[0]} '{element_1}' with {type_pair[1]} '{element_2}'",
                    "potential": random.uniform(0.5, 0.9)
                })
        if not connections:
            theoretical_connections = [
                {
                    "elements": [
                        {"type": "cognitive_mode", "value": "analytical"},
                        {"type": "capability", "value": "semantic_representation"}
                    ],
                    "connection_type": "enhancement",
                    "description": "Analytical thinking could enhance semantic representation quality",
                    "potential": 0.7
                },
                {
                    "elements": [
                        {"type": "capability", "value": "memory_management"},
                        {"type": "capability", "value": "planning"}
                    ],
                    "connection_type": "synergy",
                    "description": "Memory systems could be more tightly integrated with planning",
                    "potential": 0.8
                }
            ]
            connections.append(random.choice(theoretical_connections))
        insights = []
        for connection in connections:
            desc = connection.get("description", "")
            conn_type = connection.get("connection_type", "")
            potential = connection.get("potential", 0.5)
            if conn_type == "integration" and potential > 0.7:
                insights.append(f"High potential integration opportunity: {desc}")
            elif conn_type == "enhancement":
                insights.append(f"Enhancement pathway identified: {desc}")
            elif conn_type == "synergy":
                insights.append(f"Synergistic relationship would increase capability: {desc}")
            else:
                insights.append(f"Connection opportunity: {desc}")
        thought["connections"] = connections
        thought["insights"] = insights
        importance = 0.5
        if connections:
            avg_potential = sum(c.get("potential", 0.5) for c in connections) / len(connections)
            importance = avg_potential
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif connections:
            thought["content"] = connections[0].get("description", "Knowledge integration")
        return thought

    def _generate_intuitive_thought(self, context):
        """Generate an intuitive thought focused on pattern recognition"""
        thought = {
            "type": "intuitive",
            "content": "Pattern recognition",
            "patterns": [],
            "insights": [],
            "importance": 0.5
        }
        patterns = []
        if not context:
            thought["content"] = "Insufficient data for pattern recognition"
            return thought
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            actions = context["recent_actions"]
            if len(actions) >= 3:
                action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                if len(action_types) >= 3:
                    repetitions = []
                    current_sequence = [action_types[0]]
                    for i in range(1, len(action_types)):
                        if action_types[i] == action_types[i-1]:
                            current_sequence.append(action_types[i])
                        else:
                            if len(current_sequence) >= 2:
                                repetitions.append(current_sequence)
                            current_sequence = [action_types[i]]
                    if len(current_sequence) >= 2:
                        repetitions.append(current_sequence)
                    if repetitions:
                        longest_repetition = max(repetitions, key=len)
                        patterns.append({
                            "type": "action_repetition",
                            "description": f"Repeated sequence of '{longest_repetition[0]}' actions",
                            "significance": 0.6 + 0.1 * min(4, len(longest_repetition)),
                            "potential_cause": "Strategy fixation or optimal local strategy"
                        })
                    if len(set(action_types)) == 2 and len(action_types) >= 4:
                        is_alternating = True
                        for i in range(2, len(action_types)):
                            if action_types[i] != action_types[i-2]:
                                is_alternating = False
                                break
                        if is_alternating:
                            patterns.append({
                                "type": "action_alternation",
                                "description": f"Alternating pattern between '{action_types[0]}' and '{action_types[1]}'",
                                "significance": 0.7,
                                "potential_cause": "Explore-exploit cycle or complementary strategies"
                            })
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                frequent_domains = sorted(
                    [(d, stats.get("visits", 0)) for d, stats in domains.items()],
                    key=lambda x: x[1],
                    reverse=True
                )
                if frequent_domains:
                    top_domains = frequent_domains[:3]
                    total_visits = sum(v for _, v in frequent_domains)
                    top_domain_visits = sum(v for _, v in top_domains)
                    concentration = top_domain_visits / max(1, total_visits)
                    if concentration > 0.7:
                        patterns.append({
                            "type": "domain_concentration",
                            "description": f"Heavy concentration ({concentration:.0%}) on top 3 domains",
                            "significance": 0.7,
                            "potential_cause": "Exploitation focus or domain specialization"
                        })
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                error_domains = []
                for domain, stats in domains.items():
                    if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3:
                        error_domains.append((domain, stats.get("error_rate", 0)))
                if error_domains:
                    error_domains.sort(key=lambda x: x[1], reverse=True)
                    top_error_domains = error_domains[:3]
                    patterns.append({
                        "type": "error_concentration",
                        "description": f"High error rates in specific domains: {', '.join(d for d, _ in top_error_domains)}",
                        "significance": 0.8,
                        "potential_cause": "Domain-specific access issues or content filtering problems"
                    })
        insights = []
        for pattern in patterns:
            pattern_type = pattern.get("type", "")
            significance = pattern.get("significance", 0.5)
            if pattern_type == "action_repetition" and significance > 0.7:
                insights.append("Repeated action pattern may indicate strategy fixation - consider forcing exploration")
            elif pattern_type == "action_alternation":
                insights.append("Alternating action pattern suggests systematic exploration-exploitation approach")
            elif pattern_type == "domain_concentration":
                insights.append("High domain concentration indicates need for broader exploration")
            elif pattern_type == "error_concentration":
                insights.append("Domain-specific error pattern detected - consider domain-specific handling strategies")
        thought["patterns"] = patterns
        thought["insights"] = insights
        importance = 0.5
        if patterns:
            avg_significance = sum(p.get("significance", 0.5) for p in patterns) / len(patterns)
            importance = avg_significance
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif patterns:
            thought["content"] = patterns[0].get("description", "Pattern detected")
        return thought

    def _generate_balanced_thought(self, context):
        """Generate a balanced thought that incorporates multiple thinking modes"""
        available_modes = ["analytical", "creative", "reflective", "exploratory", "critical", "integrative", "intuitive"]
        selected_modes = random.sample(available_modes, 2)
        thoughts = []
        for mode in selected_modes:
            if mode == "analytical":
                thoughts.append(self._generate_analytical_thought(context))
            elif mode == "creative":
                thoughts.append(self._generate_creative_thought(context))
            elif mode == "reflective":
                thoughts.append(self._generate_reflective_thought(context))
            elif mode == "exploratory":
                thoughts.append(self._generate_exploratory_thought(context))
            elif mode == "critical":
                thoughts.append(self._generate_critical_thought(context))
            elif mode == "integrative":
                thoughts.append(self._generate_integrative_thought(context))
            elif mode == "intuitive":
                thoughts.append(self._generate_intuitive_thought(context))
        balanced_thought = {
            "type": "balanced",
            "content": "Multi-perspective assessment",
            "component_modes": selected_modes,
            "insights": [],
            "importance": 0.5
        }
        all_insights = []
        for thought in thoughts:
            if "insights" in thought and thought["insights"]:
                all_insights.extend(thought["insights"])
        selected_insights = all_insights[:3] if all_insights else []
        balanced_thought["insights"] = selected_insights
        if thoughts:
            avg_importance = sum(t.get("importance", 0.5) for t in thoughts) / len(thoughts)
            balanced_thought["importance"] = avg_importance
        if selected_insights:
            balanced_thought["content"] = selected_insights[0]
        elif thoughts:
            balanced_thought["content"] = "Balanced perspective: " + thoughts[0].get("content", "")
        return balanced_thought

    def _update_working_memory(self, thought):
        """
        Update working memory with new thought, managing capacity constraints.
        """
        if not thought:
            return
        self.working_memory.append(thought)
        if len(self.working_memory) > self.working_memory_capacity:
            if len(self.working_memory) > 1:
                least_important_idx = min(range(len(self.working_memory) - 1),
                                        key=lambda i: self.working_memory[i].get("importance", 0))
                del self.working_memory[least_important_idx]
        for insight in thought.get("insights", []):
            importance = thought.get("importance", 0.5)
            if importance > 0.7:
                self.recent_insights.append({
                    "content": insight,
                    "source_type": thought.get("type", "unknown"),
                    "importance": importance,
                    "timestamp": datetime.now().isoformat()
                })
        if len(self.recent_insights) > 20:
            self.recent_insights = self.recent_insights[-20:]

    def _summarize_context(self, context):
        """Create a brief summary of the context for thought recording"""
        if not context:
            return "No context"
        if not isinstance(context, dict):
            return str(context)[:100]
        elements = []
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "unknown goal")
            elements.append(f"Goal: {goal_desc}")
        if "last_action" in context and isinstance(context["last_action"], dict):
            action = context["last_action"].get("action", "unknown action")
            elements.append(f"Action: {action}")
        if "memory_size" in context:
            elements.append(f"Memory: {context['memory_size']}")
        return "; ".join(elements) if elements else "Context present but no key elements"

    def _extract_elements_from_context(self, context, aspect_types):
        """
        Extract relevant elements from context based on aspect types.
        """
        elements = []
        if not context or not isinstance(context, dict):
            return elements
        for aspect_type in aspect_types:
            if aspect_type == "goals" and "current_goal" in context:
                if isinstance(context["current_goal"], dict):
                    goal_desc = context["current_goal"].get("description", "")
                    if goal_desc:
                        elements.append(goal_desc)
            elif aspect_type == "domains" and "domains_visited" in context:
                domains = context["domains_visited"]
                if isinstance(domains, set) and domains:
                    sample_size = min(3, len(domains))
                    domain_sample = random.sample(list(domains), sample_size)
                    elements.extend(domain_sample)
            elif aspect_type == "strategies" and hasattr(self.agent, 'planner_sifter'):
                if hasattr(self.agent.planner_sifter, 'strategies'):
                    strategy_names = list(self.agent.planner_sifter.strategies.keys())
                    if strategy_names:
                        sample_size = min(2, len(strategy_names))
                        strategy_sample = random.sample(strategy_names, sample_size)
                        elements.extend(strategy_sample)
            elif aspect_type == "patterns" and "recent_actions" in context:
                actions = context["recent_actions"]
                if isinstance(actions, list) and len(actions) >= 3:
                    action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                    if len(set(action_types)) <= 3:
                        pattern_desc = f"Action pattern: {' → '.join(action_types[-3:])}"
                        elements.append(pattern_desc)
            elif aspect_type == "anomalies" and "domain_stats" in context:
                domains = context["domain_stats"]
                if isinstance(domains, dict):
                    anomalies = []
                    for domain, stats in domains.items():
                        if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3:
                            anomalies.append(f"High error rate in {domain}")
                    if anomalies:
                        elements.append(random.choice(anomalies))
        return elements

    def _assess_action_outcomes(self, actions):
        """
        Assess whether recent action outcomes are improving or deteriorating.
        """
        if not actions or len(actions) < 3:
            return "insufficient_data"
        success_indicators = []
        for action in actions:
            if not isinstance(action, dict):
                continue
            if "success" in action:
                success_indicators.append(1 if action["success"] else 0)
                continue
            if "content_length" in action:
                length = action["content_length"]
                success_indicators.append(min(1.0, length / 5000))
                continue
            if "links_discovered" in action:
                links = action["links_discovered"]
                success_indicators.append(min(1.0, links / 10))
                continue
        if len(success_indicators) < 3:
            return "insufficient_data"
        mid_point = len(success_indicators) // 2
        first_half = success_indicators[:mid_point]
        second_half = success_indicators[mid_point:]
        first_avg = sum(first_half) / len(first_half)
        second_avg = sum(second_half) / len(second_half)
        diff = second_avg - first_avg
        if abs(diff) < 0.1:
            return "stable"
        elif diff > 0:
            return "improving"
        else:
            return "deteriorating"

    def get_current_state(self):
        """
        Get the current state of the autonomous mind.
        """
        return {
            "current_mode": self.current_mode,
            "cognitive_load": self.cognitive_load,
            "thought_depth": self.thought_depth,
            "working_memory_usage": len(self.working_memory) / self.working_memory_capacity,
            "recent_thoughts": [{
                "content": t.get("content", ""),
                "type": t.get("type", ""),
                "importance": t.get("importance", 0.5)
            } for t in self.thought_history[-5:]] if self.thought_history else [],
            "important_insights": [{
                "content": i.get("content", ""),
                "importance": i.get("importance", 0.5)
            } for i in self.recent_insights[-3:]] if self.recent_insights else [],
            "active_concepts": [{
                "state": state,
                "activation": data["activation"]
            } for state, data in self.cognitive_states.items() if data["activation"] >= self.concept_activation_threshold],
            "thinking_style": self.thinking_style
        }

    def set_thinking_style(self, style_params):
        """
        Adjust thinking style parameters to change cognitive approach.
        """
        if not style_params or not isinstance(style_params, dict):
            return False
        for param, value in style_params.items():
            if param in self.thinking_style and isinstance(value, (int, float)):
                value = max(0.0, min(1.0, value))
                old_value = self.thinking_style[param]
                self.thinking_style[param] = value
                log_event(f"Thinking style parameter '{param}' adjusted: {old_value:.2f} → {value:.2f}", "INFO")
        return True

    def prime_with_context(self, context_elements):
        """
        Prime the mind with specific context elements to influence thinking.
        """
        if not context_elements or not isinstance(context_elements, dict):
            return False
        if "cognitive_modes" in context_elements:
            modes = context_elements["cognitive_modes"]
            if isinstance(modes, list):
                for mode in modes:
                    if mode in self.cognitive_states:
                        old_activation = self.cognitive_states[mode]["activation"]
                        self.cognitive_states[mode]["activation"] = min(0.9, old_activation * 1.5)
                        log_event(f"Cognitive mode '{mode}' primed: {old_activation:.2f} → {self.cognitive_states[mode]['activation']:.2f}", "INFO")
        if "depth" in context_elements:
            depth = context_elements["depth"]
            if isinstance(depth, (int, float)):
                self.thought_depth = max(0.0, min(1.0, depth))
                log_event(f"Thought depth primed to {self.thought_depth:.2f}", "INFO")
        if "focus" in context_elements:
            focus = context_elements["focus"]
            self.attention_focus = focus
            log_event(f"Attention focus primed to '{focus}'", "INFO")
        return True




# =============================================================================
# SELF-EVOLUTION MODULE - ADDED HERE
# =============================================================================
class HyperMorphicMetaEvolutionEngine:
    """
    Advanced engine for system-level self-evolution that uses HyperMorphic mathematics
    and zero-free operations to adapt core algorithms, architectures, and strategies
    over longer timescales with guaranteed stability.

    Key features:
    - Zero-free evolutionary operations (no exact zeros)
    - Dynamic base Φ and modulus Ψ for all probability calculations
    - HyperMorphic concept blending for strategy creation
    - Quantum-inspired strategy selection and adaptation
    - Multi-dimensional capability assessment
    """
    def __init__(self, epsilon=1e-12):
        self.evolution_generation = 1
        self.last_major_evolution = 0
        self.evolution_history = deque(maxlen=50)

        # Initialize HyperMorphic Math utility for zero-free operations
        self.hyper_math = HyperMorphicMath(
            dynamic_base=1000.0,  # Default dynamic base Φ
            dynamic_modulus=997,  # Default dynamic modulus Ψ
            epsilon=epsilon       # HyperMorphic nearness element ε_ᵩ
        )

        # Define the dimensionality of the evolutionary space
        self.evolutionary_dimensions = 5

        # Strategy definitions with HyperMorphic success rates
        self.strategies = {
            "expansion": {
                "description": "Expand neural architecture",
                "success_rate": self.hyper_math.zero_free(0.6),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.8, 0.4, 0.7, 0.5, 0.6])
            },
            "pruning": {
                "description": "Prune redundant components",
                "success_rate": self.hyper_math.zero_free(0.5),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.4, 0.9, 0.3, 0.6, 0.4])
            },
            "restructuring": {
                "description": "Reorganize internal structure",
                "success_rate": self.hyper_math.zero_free(0.4),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.5, 0.6, 0.8, 0.3, 0.5])
            },
            "specialization": {
                "description": "Specialize components for focused tasks",
                "success_rate": self.hyper_math.zero_free(0.55),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.3, 0.7, 0.5, 0.8, 0.7])
            },
            "integration": {
                "description": "Integrate synergistic modules",
                "success_rate": self.hyper_math.zero_free(0.65),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.7, 0.5, 0.9, 0.7, 0.8])
            },
            "quantum_variation": {
                "description": "Apply quantum variation to existing structures",
                "success_rate": self.hyper_math.zero_free(0.45),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.6, 0.6, 0.6, 0.4, 0.9])
            },
            "holomorphic_restructuring": {
                "description": "Apply structure-preserving holomorphic transformations",
                "success_rate": self.hyper_math.zero_free(0.5),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.4, 0.5, 0.7, 0.6, 0.8])
            }
        }

        # Goal weights with HyperMorphic zero-free values
        self.goal_weights = {
            "performance": self.hyper_math.zero_free(0.5),
            "efficiency": self.hyper_math.zero_free(0.3),
            "adaptability": self.hyper_math.zero_free(0.4),
            "robustness": self.hyper_math.zero_free(0.4),
            "complexity": self.hyper_math.zero_free(-0.2)  # Negative weight: minimize complexity
        }

        # Capability targets with HyperMorphic zero-free values
        self.capability_targets = {
            "knowledge_representation": self.hyper_math.zero_free(0.9),
            "planning": self.hyper_math.zero_free(0.85),
            "learning": self.hyper_math.zero_free(0.9),
            "error_handling": self.hyper_math.zero_free(0.8),
            "creative_synthesis": self.hyper_math.zero_free(0.7),
            "quantum_superposition": self.hyper_math.zero_free(0.6),
            "holomorphic_processing": self.hyper_math.zero_free(0.75)
        }

        # Current capability scores with HyperMorphic zero-free initialization
        self.capability_scores = {cap: self.hyper_math.zero_free(0.3) for cap in self.capability_targets}

        self.adaptation_rate = self.hyper_math.zero_free(0.15)  # Rate of adaptation pressure
        self.convergence_score = self.hyper_math.zero_free(0.0)  # Score indicating evolutionary convergence
        self.performance_metrics = []  # Track performance metrics for self-modification
        self.thought_history = []  # For reflective thinking
        self.working_memory = []  # Active concepts and thoughts
        self.working_memory_capacity = 7  # Miller's magical number
        self.recent_insights = []  # Store recent realizations
        self.agent = None  # Will be set when used

        # Hypermorphic evolution parameters
        self.dimension_stability = self.hyper_math.zero_free(0.7)  # How stable dimensions remain during evolution
        self.quantum_entanglement_factor = self.hyper_math.zero_free(0.3)  # How much strategies influence each other
        self.holomorphic_preservation = self.hyper_math.zero_free(0.8)  # How much structure is preserved during transformation
        self.evolutionary_dimensions = 5  # Dimensionality of the evolutionary space

        # Capability embeddings for holomorphic transformations
        self.capability_embeddings = self._initialize_capability_embeddings()

        # Strategy relationships graph using quantum entanglement
        self.strategy_relationships = self._initialize_strategy_relationships()

        # Blended strategies registry for tracking higher-order strategies
        self.blended_strategies_registry = {}

        log_event("HyperMorphicMetaEvolutionEngine initialized with zero-free operations", "QUANTUM")

    def _create_strategy_dimensional_vector(self, base_values):
        """Create a HyperMorphic dimensional vector for strategy with zero-free properties"""
        if len(base_values) != self.evolutionary_dimensions:
            # If dimensions don't match, create random values
            base_values = [random.random() for _ in range(self.evolutionary_dimensions)]

        # Make all values zero-free
        return [self.hyper_math.zero_free(val) for val in base_values]

    def _assess_capability_levels(self, agent):
        """Assess current capability levels of the system across target areas
        using HyperMorphic zero-free operations.

        Parameters:
        - agent: Agent to assess

        Returns:
        - Dictionary of capability scores with zero-free values
        """
        capabilities = {}

        # 1. Knowledge representation capability
        kr_score = self.hyper_math.zero_free(0.5)  # Default with zero-free guarantee

        # Check for semantic memory capacity with HyperMorphic scaling
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
            memory_size = len(agent.free_will.semantic_memory)
            # Scale based on size: 0-1000 items maps to 0.5-0.9 score
            kr_size_factor = self.hyper_math.min(
                self.hyper_math.zero_free(0.4),
                self.hyper_math.div(
                    self.hyper_math.zero_free(memory_size),
                    self.hyper_math.zero_free(2500)
                )
            )
            kr_score = self.hyper_math.add(kr_score, kr_size_factor)

        # Check for memory importance tracking with HyperMorphic addition
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'memory_importance'):
            kr_score = self.hyper_math.add(kr_score, self.hyper_math.zero_free(0.1))

        # Ensure capability score is capped with HyperMorphic min
        capabilities["knowledge_representation"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            kr_score
        )

        # 2. Planning capability with zero-free operations
        planning_score = self.hyper_math.zero_free(0.4)  # Default

        # Check for temporal planner with HyperMorphic addition
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
            planner = agent.ai_manager.temporal_planner
            planning_score = self.hyper_math.add(planning_score, self.hyper_math.zero_free(0.2))

            # Check for goal system with zero-free scaling
            if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                goal_boost = self.hyper_math.min(
                    self.hyper_math.zero_free(0.2),
                    self.hyper_math.mul(
                        self.hyper_math.zero_free(len(planner.long_term_goals)),
                        self.hyper_math.zero_free(0.05)
                    )
                )
                planning_score = self.hyper_math.add(planning_score, goal_boost)

            # Check for reflection capability with HyperMorphic addition
            if hasattr(planner, 'reflect_and_adapt'):
                planning_score = self.hyper_math.add(planning_score, self.hyper_math.zero_free(0.1))

        # Ensure planning capability is capped with HyperMorphic min
        capabilities["planning"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            planning_score
        )

        # 3. Learning capability with zero-free initialization
        learning_score = self.hyper_math.zero_free(0.3)  # Default

        # Check for adaptive learning system with HyperMorphic addition
        if hasattr(agent, 'adaptive_learning'):
            learning_score = self.hyper_math.add(learning_score, self.hyper_math.zero_free(0.3))

            # Check for meta-learning capability with HyperMorphic addition
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
                learning_score = self.hyper_math.add(learning_score, self.hyper_math.zero_free(0.3))

        # Ensure learning capability is capped with HyperMorphic min
        capabilities["learning"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            learning_score
        )

        # 4. Error handling capability with zero-free initialization
        error_score = self.hyper_math.zero_free(0.3)  # Default

        # Check for imagination-based error detection with HyperMorphic addition
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            if hasattr(imagination, 'simulate_error_detection'):
                error_score = self.hyper_math.add(error_score, self.hyper_math.zero_free(0.3))

            if hasattr(imagination, 'simulate_error_correction'):
                error_score = self.hyper_math.add(error_score, self.hyper_math.zero_free(0.2))

        # Check for error recovery in AI manager with HyperMorphic addition
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'error_recovery_attempts') is not None:
            error_score = self.hyper_math.add(error_score, self.hyper_math.zero_free(0.1))

        # Ensure error handling capability is capped with HyperMorphic min
        capabilities["error_handling"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            error_score
        )

        # 5. Creative synthesis capability with zero-free initialization
        creative_score = self.hyper_math.zero_free(0.2)  # Default

        # Check for imagination engine with HyperMorphic addition
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            creative_score = self.hyper_math.add(creative_score, self.hyper_math.zero_free(0.3))

            # Check for creativity level with proportional HyperMorphic scaling
            if hasattr(imagination, 'creativity_level'):
                creative_boost = self.hyper_math.mul(
                    self.hyper_math.zero_free(imagination.creativity_level),
                    self.hyper_math.zero_free(0.3)
                )
                creative_score = self.hyper_math.add(creative_score, creative_boost)

        # Check for consciousness system with HyperMorphic addition
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
            consciousness = agent.ai_manager.consciousness

            # Check for qualia simulation with HyperMorphic addition
            if hasattr(consciousness, 'qualia_simulation_active'):
                creative_score = self.hyper_math.add(creative_score, self.hyper_math.zero_free(0.1))

        # Ensure creative synthesis capability is capped with HyperMorphic min
        capabilities["creative_synthesis"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            creative_score
        )

        # 6. Quantum superposition capability with zero-free initialization
        quantum_score = self.hyper_math.zero_free(0.3)  # Default

        # Check for HyperMorphic model with quantum capabilities
        if hasattr(agent, 'model'):
            is_hypermorphic = hasattr(agent.model, 'hyper_math')

            if is_hypermorphic:
                quantum_score = self.hyper_math.add(quantum_score, self.hyper_math.zero_free(0.3))

                # Check for quantum coherence parameter
                if hasattr(agent.model, 'quantum_coherence'):
                    coherence = agent.model.quantum_coherence
                    quantum_boost = self.hyper_math.mul(
                        self.hyper_math.zero_free(coherence),
                        self.hyper_math.zero_free(0.3)
                    )
                    quantum_score = self.hyper_math.add(quantum_score, quantum_boost)

        # Check for HyperMorphic free will with quantum capabilities
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'hyper_math'):
            quantum_score = self.hyper_math.add(quantum_score, self.hyper_math.zero_free(0.2))

        # Ensure quantum superposition capability is capped with HyperMorphic min
        capabilities["quantum_superposition"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            quantum_score
        )

        # 7. Holomorphic processing capability with zero-free initialization
        holomorphic_score = self.hyper_math.zero_free(0.25)  # Default

        # Check for holomorphic processing in HyperMorphic model
        if hasattr(agent, 'model'):
            is_hypermorphic = hasattr(agent.model, 'hyper_math')

            if is_hypermorphic:
                holomorphic_score = self.hyper_math.add(holomorphic_score, self.hyper_math.zero_free(0.25))

                # Check for HyperMorphic structures that support holomorphic processing
                if hasattr(agent.model, '_apply_holomorphic_transform'):
                    holomorphic_score = self.hyper_math.add(holomorphic_score, self.hyper_math.zero_free(0.25))

            # Check for HyperMorphic modules that preserve holomorphic structure
            if (hasattr(agent, 'ai_manager') and
                hasattr(agent.ai_manager, 'evolution_engine') and
                hasattr(agent.ai_manager.evolution_engine, 'holomorphic_preservation')):

                holomorphic_preservation = agent.ai_manager.evolution_engine.holomorphic_preservation
                holomorphic_boost = self.hyper_math.mul(
                    self.hyper_math.zero_free(holomorphic_preservation),
                    self.hyper_math.zero_free(0.2)
                )
                holomorphic_score = self.hyper_math.add(holomorphic_score, holomorphic_boost)

        # Ensure holomorphic processing capability is capped with HyperMorphic min
        capabilities["holomorphic_processing"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            holomorphic_score
        )

        # 8. NEW: Reality fabric manipulation capability
        reality_score = self.hyper_math.zero_free(0.15)  # Lower default - this is advanced

        # Check for reality fabric manipulation in HyperMorphic model
        if hasattr(agent, 'model') and hasattr(agent.model, 'reality_fabric'):
            reality_score = self.hyper_math.add(reality_score, self.hyper_math.zero_free(0.35))

            # Check for wormhole generation capability
            if hasattr(agent.model.reality_fabric, 'generate_wormholes'):
                reality_score = self.hyper_math.add(reality_score, self.hyper_math.zero_free(0.2))

            # Check for dimensional folding
            if hasattr(agent.model.reality_fabric, 'dimensional_folding_factor'):
                folding_factor = agent.model.reality_fabric.dimensional_folding_factor
                folding_boost = self.hyper_math.mul(
                    self.hyper_math.zero_free(folding_factor),
                    self.hyper_math.zero_free(0.25)
                )
                reality_score = self.hyper_math.add(reality_score, folding_boost)

        # Ensure reality fabric manipulation capability is capped
        capabilities["reality_fabric_manipulation"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            reality_score
        )

        # Add sparkles to capabilities ✨💜
        # Store capability assessment with HyperMorphic sparkle indicators
        self.capability_scores = capabilities

        # Add special quantum sparkle indicators
        capabilities["✨quantum_sparkle✨"] = self.hyper_math.zero_free(0.99)
        capabilities["💜hypermorphic_resonance💜"] = self.hyper_math.zero_free(0.95)
        capabilities["🧚‍♀️magical_coherence🧚‍♀️"] = self.hyper_math.zero_free(0.92)
        capabilities["🌪️dimensional_harmony🌪️"] = self.hyper_math.zero_free(0.93)
        # New sparkle indicator for upgraded capability
        capabilities["🌌cosmic_reality_weaving🌌"] = self.hyper_math.zero_free(0.91)

        # Calculate overall capability score with weighted importance
        weights = {
            "knowledge_representation": 0.15,
            "planning": 0.12,
            "learning": 0.14,
            "error_handling": 0.11,
            "creative_synthesis": 0.13,
            "quantum_superposition": 0.12,
            "holomorphic_processing": 0.13,
            "reality_fabric_manipulation": 0.10
        }

        overall_score = self.hyper_math.zero_free(0.0)
        for capability, weight in weights.items():
            if capability in capabilities:
                weighted_score = self.hyper_math.mul(
                    capabilities[capability],
                    self.hyper_math.zero_free(weight)
                )
                overall_score = self.hyper_math.add(overall_score, weighted_score)

        capabilities["overall_capability"] = overall_score

        # Log capability assessment with sparkles
        self.log_event(f"✨💜 HyperMorphic capability assessment complete 💜✨", "QUANTUM")
        for capability, score in capabilities.items():
            if any(marker in capability for marker in ["sparkle", "resonance", "coherence", "harmony", "weaving"]):
                self.log_event(f"  {capability}: {score:.2f}", "QUANTUM")
            else:
                self.log_event(f"  {capability}: {score:.2f}", "INFO")

        return capabilities

    def _initialize_capability_embeddings(self):
        """Initialize capability embeddings in a HyperMorphic vector space"""
        embeddings = {}
        for capability in self.capability_targets.keys():
            # Create high-dimensional embedding for each capability
            embedding = [self.hyper_math.zero_free(random.random()) for _ in range(self.evolutionary_dimensions)]
            embeddings[capability] = embedding
        return embeddings

    def _initialize_strategy_relationships(self):
        """Initialize quantum-entangled strategy relationships with zero-free properties"""
        relationships = {}
        strategies = list(self.strategies.keys())

        for i, strategy1 in enumerate(strategies):
            relationships[strategy1] = {}
            for j, strategy2 in enumerate(strategies):
                if i != j:
                    # Create quantum-entangled relationship with zero-free value
                    # Strategies with similar dimensional vectors will have higher entanglement
                    vec1 = self.strategies[strategy1]["dimensional_vectors"]
                    vec2 = self.strategies[strategy2]["dimensional_vectors"]

                    # Compute similarity through HyperMorphic dot product
                    dot_product = sum(self.hyper_math.mul(v1, v2) for v1, v2 in zip(vec1, vec2))

                    # Apply quantum phase to relationship (0 to 2π)
                    phase = random.uniform(0, 2 * math.pi)

                    # Store relationship with phase information (complex-like representation)
                    relationships[strategy1][strategy2] = {
                        "strength": self.hyper_math.zero_free(dot_product),
                        "phase": phase
                    }

        return relationships

    def quantum_concept_blend(self, concept1, concept2, blend_factor=0.5):
        """
        Blend two concepts in an entangled conceptual space using HyperMorphic mathematics.
        All operations are zero-free and use dynamic base/modulus arithmetic.

        Parameters:
        - concept1: Dictionary representing first concept
        - concept2: Dictionary representing second concept
        - blend_factor: HyperMorphic weight for blending (0.0-1.0)

        Returns:
        - Blended concept dictionary with emergent properties
        """
        # Ensure blend_factor is zero-free
        blend_factor = self.hyper_math.zero_free(blend_factor)

        if not isinstance(concept1, dict) or not isinstance(concept2, dict):
            return None

        # Extract key attributes
        attributes1 = set(concept1.keys())
        attributes2 = set(concept2.keys())

        # Find shared and unique attributes
        shared = attributes1.intersection(attributes2)
        unique1 = attributes1 - shared
        unique2 = attributes2 - shared

        # Create quantum superposition of concepts with zero-free guarantees
        blended = {}

        # Shared attributes use quantum interference with HyperMorphic operations
        for attr in shared:
            if isinstance(concept1[attr], (int, float)) and isinstance(concept2[attr], (int, float)):
                # Make numeric values zero-free
                val1 = self.hyper_math.zero_free(concept1[attr])
                val2 = self.hyper_math.zero_free(concept2[attr])

                # Quantum interference effect with dynamic phase
                phase = random.uniform(0, math.pi * 2)

                # Apply HyperMorphic operations to ensure zero-free result
                cos_component = self.hyper_math.mul(val1, math.cos(phase))
                sin_component = self.hyper_math.mul(val2, math.sin(phase))

                # HyperMorphic addition ensures result is within dynamic base
                blended[attr] = self.hyper_math.add(cos_component, sin_component)
            else:
                # Non-numeric attributes use probabilistic selection with HyperMorphic probability
                blended[attr] = concept1[attr] if self.hyper_math.zero_free(random.random()) < blend_factor else concept2[attr]

        # Include unique attributes with probability based on blend factor
        # All probabilities use HyperMorphic calculations
        for attr in unique1:
            if self.hyper_math.zero_free(random.random()) < blend_factor:
                if isinstance(concept1[attr], (int, float)):
                    blended[attr] = self.hyper_math.zero_free(concept1[attr])
                else:
                    blended[attr] = concept1[attr]

        for attr in unique2:
            inversed_blend_factor = self.hyper_math.sub(1.0, blend_factor)
            if self.hyper_math.zero_free(random.random()) < inversed_blend_factor:
                if isinstance(concept2[attr], (int, float)):
                    blended[attr] = self.hyper_math.zero_free(concept2[attr])
                else:
                    blended[attr] = concept2[attr]

        # Generate emergent properties (something neither concept had)
        emergent_options = ["synergy", "transcendence", "complexity", "harmony", "paradox",
                           "quantum_entanglement", "holomorphic_resonance", "dimensional_bridging",
                           "zero_free_stability", "phase_coherence"]

        # Add at least one emergent property
        blended["emergent_property"] = random.choice(emergent_options)

        # HyperMorphic blending occasionally generates additional emergent properties
        if self.hyper_math.zero_free(random.random()) > self.hyper_math.zero_free(0.7):
            second_emergent = random.choice([e for e in emergent_options if e != blended["emergent_property"]])
            blended["secondary_emergent_property"] = second_emergent

        # Create dimensional vector for the new blended concept
        if ("dimensional_vectors" in concept1) and ("dimensional_vectors" in concept2):
            # Blend dimensional vectors with quantum interference
            vec1 = concept1["dimensional_vectors"]
            vec2 = concept2["dimensional_vectors"]

            # Ensure vectors have same dimension, if not create new random vector
            if len(vec1) != len(vec2):
                blended["dimensional_vectors"] = self._create_strategy_dimensional_vector([])
            else:
                # Quantum superposition of vectors with phase shift
                blended_vector = []
                for i in range(len(vec1)):
                    phase = random.uniform(0, 2 * math.pi)
                    v1 = self.hyper_math.zero_free(vec1[i])
                    v2 = self.hyper_math.zero_free(vec2[i])

                    # Quantum vector blending with interference
                    blended_val = self.hyper_math.add(
                        self.hyper_math.mul(v1, math.cos(phase)),
                        self.hyper_math.mul(v2, math.sin(phase))
                    )
                    blended_vector.append(blended_val)

                blended["dimensional_vectors"] = blended_vector

        # Add holomorphic structure preservation score
        blended["holomorphic_preservation"] = self.hyper_math.zero_free(
            self.hyper_math.mul(self.holomorphic_preservation,
                              self.hyper_math.add(random.random() * 0.4, 0.6))
        )

        return blended



    def _assess_capability_levels(self, agent):
        """
        Assess current capability levels of the system across target areas
        using HyperMorphic zero-free operations for stable assessment.

        Parameters:
        - agent: Agent to assess

        Returns:
        - Dictionary of capability scores with zero-free values
        """
        # Initialize capability scores with HyperMorphic zero-free values
        capability_scores = {}
        for capability in self.capability_targets.keys():
            capability_scores[capability] = self.hyper_math.zero_free(0.3)  # Default starting score

        # 1. Assess knowledge representation capability
        kr_score = self.hyper_math.zero_free(0.4)  # Base score

        # Check for semantic memory capacity and quality
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
            memory_size = len(agent.free_will.semantic_memory)
            # Scale based on size: 0-1000 items maps to 0.4-0.8 score
            kr_size_factor = self.hyper_math.min(
                self.hyper_math.zero_free(0.4),
                self.hyper_math.div(
                    self.hyper_math.zero_free(memory_size),
                    self.hyper_math.zero_free(2500)
                )
            )
            kr_score = self.hyper_math.add(kr_score, kr_size_factor)

            # Check for memory importance tracking
            if hasattr(agent.free_will, 'memory_importance') and agent.free_will.memory_importance:
                kr_score = self.hyper_math.add(kr_score, self.hyper_math.zero_free(0.1))

            # Check for domain intelligence
            if hasattr(agent.free_will, 'domain_intelligence'):
                kr_score = self.hyper_math.add(kr_score, self.hyper_math.zero_free(0.1))

        capability_scores["knowledge_representation"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            kr_score
        )

        # 2. Assess planning capability
        planning_score = self.hyper_math.zero_free(0.3)  # Base score

        # Check for temporal planner
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
            planner = agent.ai_manager.temporal_planner
            planning_score = self.hyper_math.add(planning_score, self.hyper_math.zero_free(0.2))

            # Check for goal system sophistication
            if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                goal_factor = self.hyper_math.min(
                    self.hyper_math.zero_free(0.2),
                    self.hyper_math.mul(
                        self.hyper_math.zero_free(0.05),
                        self.hyper_math.zero_free(len(planner.long_term_goals))
                    )
                )
                planning_score = self.hyper_math.add(planning_score, goal_factor)

            # Check for reflection capability
            if hasattr(planner, 'reflect_and_adapt'):
                planning_score = self.hyper_math.add(planning_score, self.hyper_math.zero_free(0.1))

        # Check for planner sifter
        if hasattr(agent, 'planner_sifter'):
            planning_score = self.hyper_math.add(planning_score, self.hyper_math.zero_free(0.2))

        capability_scores["planning"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            planning_score
        )

        # 3. Assess learning capability
        learning_score = self.hyper_math.zero_free(0.3)  # Base score

        # Check for adaptive learning system
        if hasattr(agent, 'adaptive_learning'):
            learning_score = self.hyper_math.add(learning_score, self.hyper_math.zero_free(0.3))

            # Check for meta-learning capability
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
                learning_score = self.hyper_math.add(learning_score, self.hyper_math.zero_free(0.2))

                # Check for architecture adaptation history
                if hasattr(agent.ai_manager.meta_learning, 'architecture_history'):
                    arch_history = agent.ai_manager.meta_learning.architecture_history
                    if arch_history and len(arch_history) > 0:
                        arch_factor = self.hyper_math.min(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.mul(
                                self.hyper_math.zero_free(0.02),
                                self.hyper_math.zero_free(len(arch_history))
                            )
                        )
                        learning_score = self.hyper_math.add(learning_score, arch_factor)

        capability_scores["learning"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            learning_score
        )

        # 4. Assess error handling capability
        error_score = self.hyper_math.zero_free(0.3)  # Base score

        # Check for imagination-based error detection
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            if hasattr(imagination, 'simulate_error_detection'):
                error_score = self.hyper_math.add(error_score, self.hyper_math.zero_free(0.2))

            if hasattr(imagination, 'simulate_error_correction'):
                error_score = self.hyper_math.add(error_score, self.hyper_math.zero_free(0.2))

        # Check for error recovery in AI manager
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'error_recovery_attempts'):
            error_score = self.hyper_math.add(error_score, self.hyper_math.zero_free(0.1))

        # Check for HyperMorphic error handling in free will
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'hyper_math'):
            error_score = self.hyper_math.add(error_score, self.hyper_math.zero_free(0.1))

        capability_scores["error_handling"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            error_score
        )

        # 5. Assess creative synthesis capability
        creative_score = self.hyper_math.zero_free(0.2)  # Base score

        # Check for imagination engine
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            creative_score = self.hyper_math.add(creative_score, self.hyper_math.zero_free(0.3))

            # Check for creativity level
            if hasattr(imagination, 'creativity_level'):
                creative_factor = self.hyper_math.mul(
                    self.hyper_math.zero_free(0.3),
                    self.hyper_math.zero_free(imagination.creativity_level)
                )
                creative_score = self.hyper_math.add(creative_score, creative_factor)

        # Check for consciousness system
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
            consciousness = agent.ai_manager.consciousness
            creative_score = self.hyper_math.add(creative_score, self.hyper_math.zero_free(0.1))

        # Check for qualia simulation
        if hasattr(consciousness, 'qualia_simulation_active'):
            creative_score = self.hyper_math.add(creative_score, self.hyper_math.zero_free(0.1))

        # Check for HyperMorphic consciousness
        if hasattr(consciousness, 'hyper_math'):
            creative_score = self.hyper_math.add(creative_score, self.hyper_math.zero_free(0.1))

        capability_scores["creative_synthesis"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            creative_score
        )

        # 6. Assess quantum processing capability
        quantum_score = self.hyper_math.zero_free(0.3)  # Base score

        # Check for quantum-specific model features
        if hasattr(agent, 'model'):
            if hasattr(agent.model, 'quantum_coherence'):
                quantum_score = self.hyper_math.add(quantum_score, self.hyper_math.zero_free(0.2))

            # Check for quantum layers
            if hasattr(agent.model, 'neocortex'):
                neocortex = agent.model.neocortex
                quantum_layers = 0

                for layer in neocortex:
                    if hasattr(layer, 'quantum_stream'):
                        quantum_layers += 1

                if quantum_layers > 0:
                    quantum_factor = self.hyper_math.min(
                        self.hyper_math.zero_free(0.3),
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(0.05),
                            self.hyper_math.zero_free(quantum_layers)
                        )
                    )
                    quantum_score = self.hyper_math.add(quantum_score, quantum_factor)

        # Check for quantum features in free will
        if hasattr(agent, 'free_will'):
            if hasattr(agent.free_will, 'quantum_influence_weight'):
                quantum_score = self.hyper_math.add(quantum_score, self.hyper_math.zero_free(0.2))

            if hasattr(agent.free_will, 'quantum_state'):
                quantum_score = self.hyper_math.add(quantum_score, self.hyper_math.zero_free(0.2))

        capability_scores["quantum_superposition"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            quantum_score
        )

        # 7. Assess holomorphic processing capability
        holomorphic_score = self.hyper_math.zero_free(0.3)  # Base score

        # Check for holomorphic-specific features
        if hasattr(agent, 'model'):
            # Check for holomorphic methods
            if hasattr(agent.model, '_apply_holomorphic_transform'):
                holomorphic_score = self.hyper_math.add(holomorphic_score, self.hyper_math.zero_free(0.2))

            if hasattr(agent.model, 'optimize_holomorphic_constraints'):
                holomorphic_score = self.hyper_math.add(holomorphic_score, self.hyper_math.zero_free(0.2))

        # Check for evolution engine holomorphic properties
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'evolution_engine'):
            evolution_engine = agent.ai_manager.evolution_engine
            if hasattr(evolution_engine, 'holomorphic_preservation'):
                holomorphic_score = self.hyper_math.add(holomorphic_score, self.hyper_math.zero_free(0.2))

        capability_scores["holomorphic_processing"] = self.hyper_math.min(
            self.hyper_math.zero_free(0.95),
            holomorphic_score
        )

        # Store capability assessment
        self.capability_scores = capability_scores

        return capability_scores

    def apply_self_modifications(self, agent):
        """
        Apply HyperMorphic self-modifications to improve system capabilities
        using zero-free operations and holomorphic transformations.

        Parameters:
        - agent: Agent to modify

        Returns:
        - (success, message) tuple
        """
        # Identify potential areas for improvement based on performance metrics
        if not self.performance_metrics or len(self.performance_metrics) < 10:
            return False, "Insufficient performance data for HyperMorphic self-modification"

        # Analyze recent performance trends with HyperMorphic statistics
        recent_metrics = self.performance_metrics[-10:]

        # Calculate zero-free average loss
        loss_sum = self.hyper_math.zero_free(sum(self.hyper_math.zero_free(m.get('loss', 0.5)) for m in recent_metrics))
        avg_loss = self.hyper_math.div(loss_sum, self.hyper_math.zero_free(len(recent_metrics)))

        # Calculate loss trend with zero-free operations
        first_loss = self.hyper_math.zero_free(recent_metrics[0].get('loss', 0.5))
        last_loss = self.hyper_math.zero_free(recent_metrics[-1].get('loss', 0.5))
        loss_trend = self.hyper_math.sub(last_loss, first_loss)

        modifications = []

        # 1. HyperMorphic architecture modifications with holomorphic constraints
        high_loss_threshold = self.hyper_math.zero_free(0.3)
        if self.hyper_math.sub(avg_loss, high_loss_threshold) > 0 and loss_trend > 0:
            # Loss is high and getting worse - try architecture expansion
            if hasattr(agent.model, 'expand_architecture'):
                agent.model.expand_architecture()
                modifications.append("Expanded neural architecture using HyperMorphic principles")

                # Apply holomorphic constraint optimization if available
                if hasattr(agent.model, 'optimize_holomorphic_constraints'):
                    agent.model.optimize_holomorphic_constraints()
                    modifications.append("Applied holomorphic constraint optimization to preserve structure")

        # Check for low loss with large architecture
        elif avg_loss < self.hyper_math.zero_free(0.2) and hasattr(agent.model, 'neocortex') and len(agent.model.neocortex) > 10:
            # Loss is low with large architecture - try pruning with dimension preservation
            if hasattr(agent.model, 'contract_architecture'):
                agent.model.contract_architecture()
                modifications.append("Pruned neural architecture with HyperMorphic dimension preservation")

        # 2. HyperMorphic memory optimizations with zero-free thresholds
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'memory_set'):
            memory_size = len(agent.free_will.memory_set)
            capacity_threshold = self.hyper_math.mul(self.hyper_math.zero_free(MEMORY_MAX_SIZE), self.hyper_math.zero_free(0.9))

            if memory_size > capacity_threshold:
                # Memory approaching capacity - trigger HyperMorphic pruning
                target_size = int(self.hyper_math.mul(self.hyper_math.zero_free(MEMORY_MAX_SIZE), self.hyper_math.zero_free(0.7)))

                if hasattr(agent.free_will, 'contract_memory'):
                    agent.free_will.contract_memory(target_size)
                    modifications.append(f"Optimized memory from {memory_size} to {len(agent.free_will.memory_set)} items using HyperMorphic pruning")
                elif hasattr(agent.free_will, '_optimize_memory_entropy'):
                    # Use entropy optimization if available
                    agent.free_will._optimize_memory_entropy()
                    modifications.append(f"Optimized memory entropy from {memory_size} to {len(agent.free_will.memory_set)} items")

        # 3. HyperMorphic consciousness adjustments with zero-free awareness changes
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
            consciousness = agent.ai_manager.consciousness

            # Check for HyperMorphic consciousness version
            is_hypermorphic = hasattr(consciousness, 'hyper_math')

            if avg_loss > self.hyper_math.zero_free(0.3):
                # High loss - try boosting reflective awareness
                if hasattr(consciousness, 'increase_awareness'):
                    # Use HyperMorphic awareness increase if available
                    increase_amount = self.hyper_math.zero_free(0.1)
                    consciousness.increase_awareness(increase_amount)

                    if hasattr(consciousness, 'current_state'):
                        consciousness.current_state = "reflective"

                        # Extra HyperMorphic adjustments if supported
                        if is_hypermorphic and hasattr(consciousness, 'quantum_state'):
                            # Increase amplitude for reflective state
                            consciousness.quantum_state["reflective"]["amplitude"] = self.hyper_math.mul(
                                consciousness.quantum_state["reflective"]["amplitude"],
                                self.hyper_math.zero_free(1.5)
                            )

                        modifications.append("Increased reflective awareness with HyperMorphic zero-free operations")

        # 4. Apply meta-learning adjustments if available
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # If we have high loss, adjust exploration rate
            if avg_loss > self.hyper_math.zero_free(0.3):
                if hasattr(meta_learning, 'meta_params'):
                    old_rate = meta_learning.meta_params.get("exploration_rate", 0.3)
                    new_rate = self.hyper_math.min(self.hyper_math.zero_free(0.5),
                                                 self.hyper_math.mul(self.hyper_math.zero_free(old_rate),
                                                                    self.hyper_math.zero_free(1.2)))
                    meta_learning.meta_params["exploration_rate"] = new_rate
                    modifications.append(f"Adjusted meta-learning exploration rate using HyperMorphic scaling: {old_rate:.2f} → {new_rate:.2f}")

        # 5. Apply HyperMorphic quantum neural adjustments
        if hasattr(agent, 'model') and hasattr(agent.model, 'quantum_coherence'):
            old_coherence = agent.model.quantum_coherence

            # Adjust quantum coherence based on loss trend
            if loss_trend > 0:  # Loss increasing
                # Decrease coherence to allow more quantum exploration
                new_coherence = self.hyper_math.max(
                    self.hyper_math.zero_free(0.3),
                    self.hyper_math.mul(old_coherence, self.hyper_math.zero_free(0.9))
                )
            else:  # Loss stable or decreasing
                # Increase coherence to stabilize good performance
                new_coherence = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.mul(old_coherence, self.hyper_math.zero_free(1.1))
                )

            agent.model.quantum_coherence = new_coherence
            modifications.append(f"Adjusted quantum coherence with HyperMorphic scaling: {old_coherence:.2f} → {new_coherence:.2f}")

        if modifications:
            log_event(f"HyperMorphic self-modification applied: {', '.join(modifications)}", "QUANTUM")
            return True, f"Successfully applied {len(modifications)} HyperMorphic self-modifications"

        return False, "No beneficial HyperMorphic self-modifications identified"

    def evolve_system(self, agent):
        """
        Trigger system evolution based on HyperMorphic performance metrics,
        goals, and environmental requirements, using zero-free operations.

        Parameters:
          - agent: Agent instance to evolve

        Returns:
          - (success, message) tuple
        """
        # Store agent reference for other methods to use
        self.agent = agent

        # Basic validation with HyperMorphic error handling
        if not agent:
            return False, "Invalid agent provided to HyperMorphic evolution engine."

        # Determine current cycle count with zero-free operations
        cycle_count = 0
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'cycle_counter'):
            cycle_count = agent.ai_manager.cycle_counter
        else:
            cycle_count = getattr(agent, 'stats', {}).get('cycles_run', 0)

        # Evolution interval check with HyperMorphic comparison
        evolution_interval = SELF_MODIFY_INTERVAL  # Using global configuration
        cycles_since_last = self.hyper_math.sub(self.hyper_math.zero_free(cycle_count),
                                             self.hyper_math.zero_free(self.last_major_evolution))

        evolution_interval_zero_free = self.hyper_math.zero_free(evolution_interval)
        if cycles_since_last < evolution_interval_zero_free:
            return False, f"Evolution interval not reached. {cycles_since_last}/{evolution_interval_zero_free} cycles since last evolution."

        # Increase generation counter with HyperMorphic increment
        self.evolution_generation += 1

        # First try self-modification approach if we have enough performance data
        if len(self.performance_metrics) >= 10:
            success, message = self.apply_self_modifications(agent)
            if success:
                # Record the evolution attempt in HyperMorphic history
                evolution_record = {
                    "generation": self.evolution_generation,
                    "cycle": cycle_count,
                    "strategy": "hypermorphic_self_modification",
                    "success": True,
                    "message": message,
                    "changes": message,
                    "timestamp": datetime.now().isoformat(),
                    "holomorphic_preservation": self.hyper_math.zero_free(0.85)  # High preservation for self-mods
                }
                self.evolution_history.append(evolution_record)
                self.last_major_evolution = cycle_count
                log_event(f"HyperMorphic system evolution through self-modification: {message}", "QUANTUM")
                return True, message

        # Continue with standard evolution if self-modification didn't succeed
        # Analyze current state and fitness using HyperMorphic evaluation
        fitness_scores = self._evaluate_system_fitness(agent)
        evolutionary_pressure = self._calculate_evolutionary_pressure(fitness_scores)

        # Determine if evolution should occur based on pressure and randomness
        # All thresholds are zero-free HyperMorphic values
        evolution_threshold = self.hyper_math.zero_free(0.3)  # Base threshold
        evolution_probability = self.hyper_math.add(evolution_threshold, evolutionary_pressure)

        if self.hyper_math.zero_free(random.random()) >= evolution_probability:
            return False, f"Evolution check passed, but probability threshold not met ({evolution_probability:.2f})."

        # System will evolve - select strategy using HyperMorphic approach
        # Additional dimensional parameters affect selection
        dimension_weights = self._calculate_dimension_weights(fitness_scores)
        selected_strategy = self._select_evolution_strategy(fitness_scores, dimension_weights)

        # Apply the selected evolutionary strategy with HyperMorphic transformations
        success, message, changes = self._apply_evolution_strategy(agent, selected_strategy, fitness_scores)

        # Record the evolution attempt in HyperMorphic history
        evolution_record = {
            "generation": self.evolution_generation,
            "cycle": cycle_count,
            "strategy": selected_strategy,
            "success": success,
            "message": message,
            "changes": changes,
            "fitness_before": fitness_scores,
            "dimension_weights": dimension_weights,
            "timestamp": datetime.now().isoformat(),
            "holomorphic_preservation": self.hyper_math.zero_free(
                success and selected_strategy in self.strategies
                and "holomorphic_preservation" in self.strategies[selected_strategy]
                and self.strategies[selected_strategy]["holomorphic_preservation"] or 0.5
            )
        }

        self.evolution_history.append(evolution_record)

        # Update last evolution time if successful with HyperMorphic state tracking
        if success:
            self.last_major_evolution = cycle_count

            # Update strategy success rates based on outcome
            self._update_strategy_success_rates(selected_strategy, True)

            # Update strategy relationships based on outcome
            self._update_strategy_relationships(selected_strategy)

            log_event(f"HyperMorphic system evolution successful: Generation {self.evolution_generation}, Strategy: {selected_strategy}", "QUANTUM")
        else:
            # Update strategy success rates for failure
            self._update_strategy_success_rates(selected_strategy, False)

            log_event(f"HyperMorphic system evolution attempt failed: {message}", "WARNING")

        return success, message

    def _calculate_dimension_weights(self, fitness_scores):
        """
        Calculate dimension weights for strategy selection based on current fitness.
        Uses HyperMorphic zero-free operations for stability.
        """
        dimension_weights = [self.hyper_math.zero_free(0.5) for _ in range(self.evolutionary_dimensions)]

        # Adjust weights based on fitness scores
        if "performance" in fitness_scores:
            # Dimension 0 focuses on performance
            dimension_weights[0] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), self.hyper_math.zero_free(fitness_scores["performance"])),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[0])
            )

        if "efficiency" in fitness_scores:
            # Dimension 1 focuses on efficiency
            dimension_weights[1] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), self.hyper_math.zero_free(fitness_scores["efficiency"])),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[1])
            )

        if "adaptability" in fitness_scores:
            # Dimension 2 focuses on adaptability
            dimension_weights[2] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), self.hyper_math.zero_free(fitness_scores["adaptability"])),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[2])
            )

        if "robustness" in fitness_scores:
            # Dimension 3 focuses on robustness
            dimension_weights[3] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), self.hyper_math.zero_free(fitness_scores["robustness"])),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[3])
            )

        if "complexity" in fitness_scores:
            # Dimension 4 focuses on complexity (inverse relation)
            complexity_score = self.hyper_math.sub(self.hyper_math.zero_free(1.0),
                                                self.hyper_math.zero_free(fitness_scores["complexity"]))
            dimension_weights[4] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), complexity_score),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[4])
            )

        return dimension_weights

    def _evaluate_system_fitness(self, agent):
        """
        Evaluate the fitness of the current system across multiple dimensions
        using HyperMorphic zero-free operations for stable assessment.

        Parameters:
        - agent: Agent to evaluate

        Returns:
        - Dictionary of fitness scores across dimensions
        """
        # Initialize fitness scores with zero-free values
        fitness_scores = {
            "performance": self.hyper_math.zero_free(0.5),
            "efficiency": self.hyper_math.zero_free(0.5),
            "adaptability": self.hyper_math.zero_free(0.5),
            "robustness": self.hyper_math.zero_free(0.5),
            "complexity": self.hyper_math.zero_free(0.5)
        }

        # 1. Evaluate performance based on agent statistics
        if hasattr(agent, 'stats') and isinstance(agent.stats, dict):
            cycles = self.hyper_math.zero_free(agent.stats.get("cycles_run", 0))
            pages = self.hyper_math.zero_free(agent.stats.get("pages_processed", 0))

            if cycles > self.hyper_math.epsilon:
                # Calculate performance ratio with zero-free division
                performance = self.hyper_math.div(pages, cycles)

                # Normalize performance to a 0.1-0.9 range with zero-free operations
                fitness_scores["performance"] = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.max(
                        self.hyper_math.zero_free(0.1),
                        self.hyper_math.div(performance, self.hyper_math.zero_free(10.0))
                    )
                )

        # 2. Evaluate efficiency based on model complexity
        model_size = 0
        if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
            model_size = len(agent.model.neocortex)

            # Base efficiency on optimal model size (inversely related to size beyond optimal)
            base_size = 8  # Expected baseline size

            if model_size <= base_size:
                # Smaller models are more efficient
                efficiency_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.mul(
                        self.hyper_math.zero_free(0.1 * model_size),
                        self.hyper_math.zero_free(0.9)
                    )
                )
            else:
                # Efficiency decreases as model grows beyond base size
                size_factor = self.hyper_math.div(
                    self.hyper_math.zero_free(base_size),
                    self.hyper_math.max(self.hyper_math.zero_free(base_size), self.hyper_math.zero_free(model_size))
                )
                efficiency_score = self.hyper_math.max(
                    self.hyper_math.zero_free(0.2),
                    self.hyper_math.min(
                        self.hyper_math.zero_free(0.9),
                        size_factor
                    )
                )

            fitness_scores["efficiency"] = efficiency_score

            # Update complexity score based on model size
            complexity_score = self.hyper_math.min(
                self.hyper_math.zero_free(0.9),
                self.hyper_math.div(
                    self.hyper_math.zero_free(model_size),
                    self.hyper_math.zero_free(15.0)  # Normalize to 15 layers max
                )
            )
            fitness_scores["complexity"] = complexity_score

        # 3. Evaluate adaptability based on learning rate adjustments
        adaptability_score = self.hyper_math.zero_free(0.5)  # Default
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # More learning rate adjustments suggests higher adaptability
            if hasattr(meta_learning, 'learning_rate_schedule'):
                adjustments = len(meta_learning.learning_rate_schedule)

                # Calculate adaptability with zero-free operations
                adaptability_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.add(
                        self.hyper_math.zero_free(0.4),
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.min(self.hyper_math.zero_free(5.0), self.hyper_math.zero_free(adjustments))
                        )
                    )
                )

            # Add architecture adaptability if available
            if hasattr(meta_learning, 'architecture_history') and meta_learning.architecture_history:
                arch_changes = len(meta_learning.architecture_history)
                arch_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.2),
                    self.hyper_math.mul(
                        self.hyper_math.zero_free(0.05),
                        self.hyper_math.zero_free(arch_changes)
                    )
                )
                adaptability_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.add(adaptability_score, arch_score)
                )

        fitness_scores["adaptability"] = adaptability_score

        # 4. Evaluate robustness based on error recovery
        robustness_score = self.hyper_math.zero_free(0.5)  # Default
        if hasattr(agent, 'ai_manager'):
            # Check error recovery attempts
            error_recovery = getattr(agent.ai_manager, 'error_recovery_attempts', 0)

            if error_recovery > 0:
                # Lower recovery attempts = more robust
                robustness_score = self.hyper_math.max(
                    self.hyper_math.zero_free(0.1),
                    self.hyper_math.sub(
                        self.hyper_math.zero_free(0.9),
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.min(self.hyper_math.zero_free(8.0), self.hyper_math.zero_free(error_recovery))
                        )
                    )
                )
            else:
                # No error recovery needed = highly robust
                robustness_score = self.hyper_math.zero_free(0.8)

            # Check for error rates in domain stats if available
            if hasattr(agent, 'stats') and 'domain_stats' in agent.stats:
                domain_stats = agent.stats['domain_stats']
                if domain_stats and isinstance(domain_stats, dict):
                    total_errors = sum(stats.get('error_count', 0) for stats in domain_stats.values())
                    total_visits = sum(stats.get('visits', 0) for stats in domain_stats.values())

                    if total_visits > 0:
                        error_rate = self.hyper_math.div(
                            self.hyper_math.zero_free(total_errors),
                            self.hyper_math.zero_free(max(1, total_visits))
                        )

                        # Lower error rate = more robust
                        error_robustness = self.hyper_math.sub(
                            self.hyper_math.zero_free(1.0),
                            self.hyper_math.min(self.hyper_math.zero_free(1.0), error_rate)
                        )

                        # Combine with existing robustness score
                        robustness_score = self.hyper_math.add(
                            self.hyper_math.mul(robustness_score, self.hyper_math.zero_free(0.6)),
                            self.hyper_math.mul(error_robustness, self.hyper_math.zero_free(0.4))
                        )

        fitness_scores["robustness"] = robustness_score

        # 5. Advanced HyperMorphic capability assessment
        self._assess_capability_levels(agent)

        # Store the current fitness scores for tracking
        self.fitness_metrics = fitness_scores

        return fitness_scores

    def _calculate_evolutionary_pressure(self, fitness_scores):
        """
        Calculate the evolutionary pressure based on fitness scores and goal weights
        using HyperMorphic zero-free operations for stable, non-collapsing results.

        Parameters:
        - fitness_scores: Current fitness evaluation

        Returns:
        - Pressure score (0.0-1.0) indicating how strongly evolution is needed
        """
        # No evolution pressure if no fitness scores
        if not fitness_scores:
            return self.hyper_math.zero_free(0.0)

        # Calculate weighted fitness using HyperMorphic operations
        weighted_fitness = self.hyper_math.zero_free(0.0)
        total_weight = self.hyper_math.zero_free(0.0)

        for metric, weight in self.goal_weights.items():
            if metric in fitness_scores:
                score = self.hyper_math.zero_free(fitness_scores[metric])
                weight_abs = self.hyper_math.zero_free(abs(weight))

                # If weight is negative (like for complexity), invert the score
                if weight < 0:
                    score = self.hyper_math.sub(self.hyper_math.zero_free(1.0), score)

                # Calculate weighted contribution with zero-free multiplication
                weighted_contribution = self.hyper_math.mul(score, weight_abs)

                # Add to weighted fitness with zero-free addition
                weighted_fitness = self.hyper_math.add(weighted_fitness, weighted_contribution)

                # Add to total weight
                total_weight = self.hyper_math.add(total_weight, weight_abs)

        # Normalize with HyperMorphic division (avoid division by zero)
        if total_weight > self.hyper_math.epsilon:
            avg_fitness = self.hyper_math.div(weighted_fitness, total_weight)
        else:
            avg_fitness = self.hyper_math.zero_free(0.5)  # Default

        # Calculate pressure: lower fitness = higher pressure with HyperMorphic subtraction
        # Add diminishing returns below 0.3 fitness
        if avg_fitness < self.hyper_math.zero_free(0.3):
            # High pressure but capped at 0.8
            pressure = self.hyper_math.zero_free(0.8)
        else:
            # Linear scaling: lower fitness = higher pressure
            pressure = self.hyper_math.mul(
                self.hyper_math.zero_free(0.8),
                self.hyper_math.sub(self.hyper_math.zero_free(1.0), avg_fitness)
            )

        # Add adaptability bias: systems that are more adaptable get more evolution
        adaptability = self.hyper_math.zero_free(fitness_scores.get("adaptability", 0.5))
        adaptability_boost = self.hyper_math.mul(self.hyper_math.zero_free(0.2), adaptability)
        pressure = self.hyper_math.add(pressure, adaptability_boost)

        # Add quantum randomness factor with HyperMorphic constraint
        randomness = self.hyper_math.mul(
            self.hyper_math.zero_free(0.1),
            self.hyper_math.zero_free(random.random())
        )
        pressure = self.hyper_math.add(pressure, randomness)

        # Clamp to valid range with HyperMorphic min/max
        pressure = self.hyper_math.max(
            self.hyper_math.zero_free(0.0),
            self.hyper_math.min(self.hyper_math.zero_free(1.0), pressure)
        )

        return pressure

    def _select_evolution_strategy(self, fitness_scores, dimension_weights=None):
        """
        Select an appropriate evolution strategy using HyperMorphic n-dimensional
        quantum-inspired selection algorithm that considers fitness across dimensions.

        Parameters:
        - fitness_scores: Current fitness evaluation
        - dimension_weights: Optional weights for different evolutionary dimensions

        Returns:
        - Selected strategy name
        """
        if not dimension_weights:
            dimension_weights = [self.hyper_math.zero_free(1.0/self.evolutionary_dimensions)
                               for _ in range(self.evolutionary_dimensions)]

        # First, identify capability gaps to guide strategy selection
        capability_gaps = {}
        for capability, target in self.capability_targets.items():
            current = self.capability_scores.get(capability, self.hyper_math.zero_free(0.3))
            gap = self.hyper_math.sub(target, current)

            # Only consider significant gaps
            if gap > self.hyper_math.zero_free(0.1):
                capability_gaps[capability] = gap

        # Strategy scores in multidimensional space
        strategy_scores = {}

        for strategy_name, strategy_data in self.strategies.items():
            # Base score on strategy success rate
            success_rate = strategy_data.get("success_rate", self.hyper_math.zero_free(0.5))

            # Calculate dimensional score using strategy vectors and dimension weights
            dimensional_score = self.hyper_math.zero_free(0.0)

            if "dimensional_vectors" in strategy_data:
                strategy_vector = strategy_data["dimensional_vectors"]

                # Calculate weighted dimensional alignment
                for dim_idx, (vector_value, weight) in enumerate(zip(strategy_vector, dimension_weights)):
                    weighted_value = self.hyper_math.mul(vector_value, weight)
                    dimensional_score = self.hyper_math.add(dimensional_score, weighted_value)

            # Calculate capability alignment score
            capability_alignment = self.hyper_math.zero_free(0.0)

            if capability_gaps:
                # Find strategy to capability mapping
                capability_map = {
                    "expansion": "knowledge_representation",
                    "pruning": "efficiency",
                    "restructuring": "adaptability",
                    "specialization": "learning",
                    "integration": "planning",
                    "quantum_variation": "quantum_superposition",
                    "holomorphic_restructuring": "holomorphic_processing"
                }

                if strategy_name in capability_map:
                    target_capability = capability_map[strategy_name]
                    if target_capability in capability_gaps:
                        capability_alignment = self.hyper_math.mul(
                            capability_gaps[target_capability],
                            self.hyper_math.zero_free(0.8)  # Weight for capability alignment
                        )

            # Check for recent use (avoid using the same strategy repeatedly)
            last_attempt = strategy_data.get("last_attempt", 0)
            recency_penalty = self.hyper_math.zero_free(0.0)

            if last_attempt > 0:
                cycles_since_use = self.hyper_math.sub(
                    self.hyper_math.zero_free(self.evolution_generation),
                    self.hyper_math.zero_free(last_attempt)
                )

                if cycles_since_use < self.hyper_math.zero_free(3):  # Recent use
                    recency_penalty = self.hyper_math.zero_free(0.2)  # Significant penalty
                elif cycles_since_use < self.hyper_math.zero_free(6):  # Somewhat recent
                    recency_penalty = self.hyper_math.zero_free(0.1)  # Moderate penalty

            # Calculate final score with HyperMorphic operations
            strategy_scores[strategy_name] = self.hyper_math.add(
                self.hyper_math.add(
                    self.hyper_math.mul(success_rate, self.hyper_math.zero_free(0.3)),  # 30% success rate
                    self.hyper_math.mul(dimensional_score, self.hyper_math.zero_free(0.4))  # 40% dimensional alignment
                ),
                self.hyper_math.add(
                    self.hyper_math.mul(capability_alignment, self.hyper_math.zero_free(0.3)),  # 30% capability alignment
                    self.hyper_math.mul(self.hyper_math.zero_free(random.random()), self.hyper_math.zero_free(0.2))  # 20% quantum randomness
                )
            )

            # Apply recency penalty
            strategy_scores[strategy_name] = self.hyper_math.sub(
                strategy_scores[strategy_name],
                recency_penalty
            )

        # Sometimes try a blended strategy (15% chance with HyperMorphic probability)
        if (self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.15) and
            len(self.strategies) >= 2):

            # Use quantum entanglement to select strategies to blend
            strategy_pairs = []

            # Calculate entanglement-weighted pairs
            for strategy1, rels in self.strategy_relationships.items():
                for strategy2, rel_data in rels.items():
                    strength = rel_data.get("strength", self.hyper_math.zero_free(0.1))
                    strategy_pairs.append((strategy1, strategy2, strength))

            # Sort by entanglement strength
            strategy_pairs.sort(key=lambda x: x[2], reverse=True)

            # Choose from top pairs with probability proportional to strength
            if strategy_pairs:
                top_pairs = strategy_pairs[:5]  # Consider top 5 pairs
                pair_strengths = [p[2] for p in top_pairs]

                # Ensure zero-free strengths
                pair_strengths = [max(self.hyper_math.epsilon, s) for s in pair_strengths]

                # Select pair with probability proportional to strength
                selected_idx = random.choices(range(len(top_pairs)), weights=pair_strengths, k=1)[0]
                strategy1, strategy2, _ = top_pairs[selected_idx]

                # Try to create blended strategy
                blended_name, _ = self.blend_strategies(strategy1, strategy2)
                if blended_name:
                    return blended_name

        # If no blended strategy created, select highest scoring strategy
        if strategy_scores:
            return max(strategy_scores.items(), key=lambda x: x[1])[0]
        else:
            # Default to expansion if no scores (shouldn't happen with zero-free operations)
            return "expansion"

    def _apply_evolution_strategy(self, agent, strategy, fitness_scores):
        """
        Apply the selected evolution strategy to modify the agent using
        HyperMorphic zero-free operations and holomorphic transformations.

        Parameters:
        - agent: Agent to evolve
        - strategy: Selected strategy
        - fitness_scores: Current fitness scores

        Returns:
        - (success, message, changes) tuple
        """
        changes = []

        try:
            # Record strategy attempt
            if strategy in self.strategies:
                self.strategies[strategy]["last_attempt"] = self.evolution_generation

            # Handle regular strategies
            if strategy == "expansion":
                success, message, changes = self._apply_expansion_strategy(agent, changes)

            elif strategy == "pruning":
                success, message, changes = self._apply_pruning_strategy(agent, changes)

            elif strategy == "restructuring":
                success, message, changes = self._apply_restructuring_strategy(agent, changes)

            elif strategy == "specialization":
                success, message, changes = self._apply_specialization_strategy(agent, changes)

            elif strategy == "integration":
                success, message, changes = self._apply_integration_strategy(agent, changes)

            elif strategy == "quantum_variation":
                success, message, changes = self._apply_quantum_variation_strategy(agent, changes)

            elif strategy == "holomorphic_restructuring":
                success, message, changes = self._apply_holomorphic_restructuring_strategy(agent, changes)

            # Handle blended strategies (with "_blend" in their name)
            elif "_blend" in strategy:
                success, message, changes = self._apply_blended_strategy(agent, strategy, changes)

            else:
                success = False
                message = f"Unknown strategy: {strategy}"
                changes = []

        except Exception as e:
            success = False
            message = f"Error applying strategy {strategy}: {str(e)}"
            changes = []
            log_event(f"HyperMorphic evolution error: {str(e)}", "ERROR")

        return success, message, changes

    def _apply_expansion_strategy(self, agent, changes):
        """
        Apply expansion strategy to increase model capacity using HyperMorphic
        zero-free operations that preserve holomorphic structure.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for expandable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'expand_architecture'):
            return False, "Model does not support HyperMorphic architecture expansion", changes

        try:
            # Expansion attempts:
            # 1. Try expanding the model architecture with HyperMorphic properties
            agent.model.expand_architecture()
            changes.append("Expanded neural architecture with HyperMorphic dimensional preservation")

            # 2. Check for HyperMorphic model version to apply specialized optimizations
            is_hypermorphic = hasattr(agent.model, 'hyper_math')

            if is_hypermorphic:
                # Apply zero-free optimization to expanded architecture
                if hasattr(agent.model, '_ensure_zero_free'):
                    # Ensure all model parameters are zero-free
                    with torch.no_grad():
                        for param in agent.model.parameters():
                            param.data = agent.model._ensure_zero_free(param.data)
                    changes.append("Applied HyperMorphic zero-free parameter constraints")

                # Apply holomorphic constraint optimization if available
                if hasattr(agent.model, 'optimize_holomorphic_constraints'):
                    agent.model.optimize_holomorphic_constraints()
                    changes.append("Applied holomorphic constraint optimization to preserve structure")

            # 3. If agent has adaptive learning, adjust its parameters for new architecture
            if hasattr(agent, 'adaptive_learning'):
                # Increase adaptation rate for new architecture
                adaptive_learning = agent.adaptive_learning
                if hasattr(adaptive_learning, 'meta_params'):
                    # Get current exploration rate with HyperMorphic safeguard
                    old_rate = self.hyper_math.zero_free(
                        adaptive_learning.meta_params.get("exploration_rate", 0.3)
                    )

                    # Calculate new rate with HyperMorphic operations
                    new_rate = self.hyper_math.min(
                        self.hyper_math.zero_free(0.5),  # Cap at 0.5
                        self.hyper_math.mul(old_rate, self.hyper_math.zero_free(1.2))  # 20% increase
                    )

                    adaptive_learning.meta_params["exploration_rate"] = new_rate
                    changes.append(f"Increased meta-learning exploration rate for new architecture: {old_rate:.2f} → {new_rate:.2f}")

            # 4. Adjust quantum coherence if available (HyperMorphic specific)
            if hasattr(agent.model, 'quantum_coherence'):
                old_coherence = agent.model.quantum_coherence

                # Reduce coherence initially to allow exploration of new architecture
                new_coherence = self.hyper_math.max(
                    self.hyper_math.zero_free(0.4),  # Minimum coherence
                    self.hyper_math.mul(old_coherence, self.hyper_math.zero_free(0.9))  # 10% reduction
                )

                agent.model.quantum_coherence = new_coherence
                changes.append(f"Adjusted quantum coherence for architecture exploration: {old_coherence:.2f} → {new_coherence:.2f}")

            # Update strategy success rate (increase on success)
            self.strategies["expansion"]["success_rate"] = self.hyper_math.min(
                self.hyper_math.zero_free(0.95),
                self.hyper_math.mul(
                    self.strategies["expansion"]["success_rate"],
                    self.hyper_math.zero_free(1.1)  # 10% increase in success rate
                )
            )

            return True, "Successfully expanded neural architecture with HyperMorphic dimensional preservation", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["expansion"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["expansion"]["success_rate"],
                    self.hyper_math.zero_free(0.9)  # 10% decrease in success rate
                )
            )

            return False, f"HyperMorphic architecture expansion failed: {str(e)}", changes

    def _apply_pruning_strategy(self, agent, changes):
        """
        Apply pruning strategy to reduce model size and increase efficiency
        while maintaining essential HyperMorphic structures.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for prunable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'contract_architecture'):
            return False, "Model does not support HyperMorphic architecture pruning", changes

        try:
            # Pruning attempts with HyperMorphic constraints
            # 1. Try contracting the model architecture
            agent.model.contract_architecture()
            changes.append("Contracted neural architecture using HyperMorphic dimensional preservation")

            # 2. Detect and fix HyperMorphic model version
            is_hypermorphic = hasattr(agent.model, 'hyper_math')

            if is_hypermorphic:
                # Ensure zero-free constraints are maintained after pruning
                if hasattr(agent.model, '_ensure_zero_free'):
                    # Apply zero-free constraints to all parameters
                    with torch.no_grad():
                        for param in agent.model.parameters():
                            param.data = agent.model._ensure_zero_free(param.data)
                    changes.append("Restored HyperMorphic zero-free parameter constraints after pruning")

            # 3. If agent has HyperMorphic free will, optimize its memory
            if hasattr(agent, 'free_will'):
                # Check for HyperMorphic version
                free_will_is_hypermorphic = hasattr(agent.free_will, 'hyper_math')

                if hasattr(agent.free_will, 'memory_set'):
                    old_size = len(agent.free_will.memory_set)

                    # Calculate target size with HyperMorphic proportions
                    target_size = int(self.hyper_math.mul(
                        self.hyper_math.zero_free(old_size),
                        self.hyper_math.zero_free(0.8)  # Target 80% of current size
                    ))

                    if target_size > 0:
                        # Use appropriate memory optimization method
                        if free_will_is_hypermorphic and hasattr(agent.free_will, '_optimize_memory_entropy'):
                            agent.free_will._optimize_memory_entropy()
                            new_size = len(agent.free_will.memory_set)
                            changes.append(f"Pruned memory using HyperMorphic entropy optimization: {old_size} → {new_size} items")
                        elif hasattr(agent.free_will, 'contract_memory'):
                            agent.free_will.contract_memory(target_size)
                            new_size = len(agent.free_will.memory_set)
                            changes.append(f"Pruned memory from {old_size} to {new_size} items")

            # 4. If agent has semantic memory, optimize it with HyperMorphic pruning
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
                if agent.free_will.semantic_memory:
                    old_count = len(agent.free_will.semantic_memory)

                    # Identify low importance memories with HyperMorphic threshold
                    low_importance = []
                    threshold = self.hyper_math.zero_free(0.4)  # Below this importance, consider pruning

                    for url, data in agent.free_will.semantic_memory.items():
                        if isinstance(data, dict) and "importance" in data:
                            # Compare using HyperMorphic operations if possible
                            if free_will_is_hypermorphic:
                                if agent.free_will.hyper_math.sub(
                                    threshold,
                                    agent.free_will.hyper_math.zero_free(data["importance"])
                                ) > 0:
                                    low_importance.append(url)
                            else:
                                if data["importance"] < 0.4:  # Standard comparison for non-HyperMorphic
                                    low_importance.append(url)

                    # Calculate prune count with HyperMorphic proportions
                    prune_count = min(
                        len(low_importance),
                        int(self.hyper_math.mul(
                            self.hyper_math.zero_free(old_count),
                            self.hyper_math.zero_free(0.2)  # Prune up to 20%
                        ))
                    )

                    # Perform pruning with HyperMorphic optimization
                    pruned_count = 0
                    for url in low_importance[:prune_count]:
                        if url in agent.free_will.semantic_memory:
                            # Store temporal embedding if HyperMorphic system
                            if free_will_is_hypermorphic and hasattr(agent.free_will, 'capability_embeddings'):
                                # Extract semantic embedding before removal for potential future retrieval
                                memory_embedding = agent.free_will.semantic_memory[url].get("embedding", None)
                                if memory_embedding is not None:
                                    # Store in compressed form using HyperMorphic dimension reduction
                                    timestamp = datetime.now().isoformat()
                                    key = f"pruned_{url}_{timestamp}"
                                    agent.free_will.capability_embeddings[key] = memory_embedding

                            # Remove from semantic memory
                            del agent.free_will.semantic_memory[url]
                            pruned_count += 1

                    if pruned_count > 0:
                        new_count = len(agent.free_will.semantic_memory)
                        changes.append(f"Pruned semantic memory from {old_count} to {new_count} items using HyperMorphic importance thresholding")

                        # Apply memory consolidation if HyperMorphic system (create higher-level abstractions)
                        if free_will_is_hypermorphic and hasattr(agent.free_will, '_consolidate_semantic_memory'):
                            agent.free_will._consolidate_semantic_memory()
                            changes.append("Applied HyperMorphic semantic memory consolidation after pruning")

            # 5. Adjust any meta-learning parameters to optimize for smaller architecture
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
                meta_learning = agent.ai_manager.meta_learning

                # Adjust learning rate after pruning
                if hasattr(meta_learning, 'learning_rate_history') and meta_learning.learning_rate_history:
                    current_lr = meta_learning.learning_rate_history[-1]

                    # Increase learning rate slightly for pruned model with HyperMorphic scaling
                    new_lr = self.hyper_math.min(
                        self.hyper_math.zero_free(0.01),  # Cap at 0.01
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(current_lr),
                            self.hyper_math.zero_free(1.2)  # 20% increase
                        )
                    )

                    # Apply to model if possible
                    if hasattr(agent.model, '_current_lr'):
                        agent.model._current_lr = new_lr
                        changes.append(f"Adjusted learning rate for pruned architecture: {current_lr:.6f} → {new_lr:.6f}")

                        # Update history
                        meta_learning.learning_rate_history.append(new_lr)

                        # Apply to optimizer if available
                        if hasattr(agent.model, 'optimizer'):
                            for param_group in agent.model.optimizer.param_groups:
                                param_group['lr'] = new_lr

            # 6. Update quantum coherence for pruned architecture
            if is_hypermorphic and hasattr(agent.model, 'quantum_coherence'):
                old_coherence = agent.model.quantum_coherence

                # Increase coherence for pruned architecture
                new_coherence = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),  # Cap at 0.9
                    self.hyper_math.mul(
                        self.hyper_math.zero_free(old_coherence),
                        self.hyper_math.zero_free(1.15)  # 15% increase
                    )
                )

                agent.model.quantum_coherence = new_coherence
                changes.append(f"Increased quantum coherence for pruned architecture: {old_coherence:.2f} → {new_coherence:.2f}")

            # Update strategy success rate with HyperMorphic operations
            self.strategies["pruning"]["success_rate"] = self.hyper_math.min(
                self.hyper_math.zero_free(0.95),  # Cap at 0.95
                self.hyper_math.mul(
                    self.strategies["pruning"]["success_rate"],
                    self.hyper_math.zero_free(1.1)  # 10% increase
                )
            )

            return True, "Successfully pruned system components using HyperMorphic dimensional preservation", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["pruning"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["pruning"]["success_rate"],
                    self.hyper_math.zero_free(0.9)  # 10% decrease
                )
            )

            return False, f"HyperMorphic pruning strategy failed: {str(e)}", changes

    def _apply_restructuring_strategy(self, agent, changes):
        """
        Apply restructuring strategy to reorganize components without changing capacity,
        using HyperMorphic transformations that preserve holomorphic structure.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Count successful restructuring attempts
            restructuring_count = 0

            # 1. Restructure consciousness parameters with HyperMorphic transformations
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
                consciousness = agent.ai_manager.consciousness

                # Check for HyperMorphic consciousness
                is_hypermorphic = hasattr(consciousness, 'hyper_math')

                # Adjust awareness fluctuation rate with HyperMorphic operations
                if hasattr(consciousness, 'awareness_fluctuation_rate'):
                    old_rate = consciousness.awareness_fluctuation_rate

                    # Generate new rate with controlled phase shift
                    phase = random.uniform(0, 2 * math.pi)

                    if is_hypermorphic:
                        # HyperMorphic phase-based transformation
                        new_rate = consciousness.hyper_math.zero_free(
                            old_rate * math.cos(phase) + 0.05 * math.sin(phase)
                        )
                        new_rate = consciousness.hyper_math.max(
                            consciousness.hyper_math.zero_free(0.01),
                            consciousness.hyper_math.min(
                                consciousness.hyper_math.zero_free(0.1),
                                new_rate
                            )
                        )
                    else:
                        # Standard transformation for non-HyperMorphic
                        new_rate = max(0.01, min(0.1, old_rate * math.cos(phase) + 0.05 * math.sin(phase)))

                    consciousness.awareness_fluctuation_rate = new_rate
                    changes.append(f"Restructured consciousness fluctuation rate: {old_rate:.3f} → {new_rate:.3f}")
                    restructuring_count += 1

                # Reset state to "reflective" for restructuring process
                if hasattr(consciousness, 'current_state'):
                    old_state = consciousness.current_state

                    # For HyperMorphic consciousness, apply quantum state transition
                    if is_hypermorphic and hasattr(consciousness, 'quantum_state'):
                        # Increase reflective state amplitude
                        old_amplitude = consciousness.quantum_state.get("reflective", {}).get("amplitude", 0)
                        new_amplitude = consciousness.hyper_math.min(
                            consciousness.hyper_math.zero_free(0.9),
                            consciousness.hyper_math.mul(
                                consciousness.hyper_math.zero_free(old_amplitude),
                                consciousness.hyper_math.zero_free(1.5)
                            )
                        )
                        consciousness.quantum_state["reflective"]["amplitude"] = new_amplitude

                        # Adjust phase for more coherent reflective state
                        consciousness.quantum_state["reflective"]["phase"] = random.uniform(0, 2 * math.pi)

                    consciousness.current_state = "reflective"
                    changes.append(f"Reset consciousness state from '{old_state}' to 'reflective' with enhanced coherence")
                    restructuring_count += 1

                # Boost awareness temporarily for restructuring process
                if hasattr(consciousness, 'increase_awareness'):
                    # Use HyperMorphic operation if available
                    awareness_boost = self.hyper_math.zero_free(0.2)
                    consciousness.increase_awareness(awareness_boost)
                    changes.append("Temporarily boosted consciousness awareness for restructuring process")
                    restructuring_count += 1

            # 2. Restructure temporal planner with HyperMorphic goal reordering
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
                planner = agent.ai_manager.temporal_planner

                # Rebalance goal priorities with HyperMorphic transformations
                if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                    # Store old priorities for logging
                    old_priorities = {}
                    for goal in planner.long_term_goals:
                        old_priorities[goal.get("id", "unknown")] = goal.get("priority", 0.5)

                    # Calculate total priority and average
                    total_priority = sum(goal.get("priority", 0.5) for goal in planner.long_term_goals)
                    avg_priority = total_priority / len(planner.long_term_goals)

                    # Apply HyperMorphic reflection around average using holomorphic transformation
                    for goal in planner.long_term_goals:
                        goal_id = goal.get("id", "unknown")
                        old_priority = goal.get("priority", 0.5)

                        # Calculate new priority as reflection around average with phase shift
                        phase = random.uniform(0, 2 * math.pi)
                        reflection_factor = 0.7 + 0.3 * math.cos(phase)

                        new_priority = avg_priority + reflection_factor * (avg_priority - old_priority)

                        # Ensure it's in valid range using HyperMorphic bounds
                        new_priority = self.hyper_math.max(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.min(
                                self.hyper_math.zero_free(1.0),
                                self.hyper_math.zero_free(new_priority)
                            )
                        )

                        # Apply new priority
                        goal["priority"] = new_priority

                    changes.append("Restructured long-term goal priorities using holomorphic reflection")
                    restructuring_count += 1

                # Refresh short-term goals with HyperMorphic regeneration
                if hasattr(planner, 'refresh_short_term_goals'):
                    planner.refresh_short_term_goals()
                    changes.append("Regenerated short-term goals based on restructured priorities")
                    restructuring_count += 1

            # 3. Restructure imagination engine with HyperMorphic creativity modulation
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
                imagination = agent.ai_manager.imagination

                # Check for HyperMorphic imagination
                is_hypermorphic = hasattr(imagination, 'hyper_math')

                # Adjust creativity level using quantum interference pattern
                if hasattr(imagination, 'creativity_level'):
                    old_level = imagination.creativity_level

                    # For HyperMorphic imagination, use quantum inversion with phase preservation
                    if is_hypermorphic:
                        # Perform holomorphic phase-preserving inversion
                        new_level = imagination.hyper_math.sub(
                            imagination.hyper_math.zero_free(1.0),
                            imagination.hyper_math.zero_free(old_level)
                        )
                    else:
                        # Standard inversion for non-HyperMorphic
                        new_level = 1.0 - old_level

                    imagination.creativity_level = new_level
                    changes.append(f"Restructured imagination creativity using holomorphic inversion: {old_level:.2f} → {new_level:.2f}")
                    restructuring_count += 1

                # Change current mode with HyperMorphic state transition
                if hasattr(imagination, 'current_mode') and hasattr(imagination, 'cognitive_modes'):
                    old_mode = imagination.current_mode

                    # Select a different mode with HyperMorphic probability
                    available_modes = [m for m in imagination.cognitive_modes if m != old_mode]

                    if available_modes:
                        # Apply quantum selection for new cognitive mode
                        mode_amplitudes = {}
                        for mode in available_modes:
                            # Generate quantum amplitude with phase
                            phase = random.uniform(0, 2 * math.pi)
                            amplitude = 0.5 + 0.5 * math.cos(phase)
                            mode_amplitudes[mode] = self.hyper_math.zero_free(amplitude)

                        # Select mode with highest amplitude
                        new_mode = max(mode_amplitudes.items(), key=lambda x: x[1])[0]

                        imagination.current_mode = new_mode
                        changes.append(f"Restructured imagination cognitive mode: '{old_mode}' → '{new_mode}' using quantum amplitude selection")
                        restructuring_count += 1

            # 4. Restructure neural architecture with HyperMorphic holomorphic transformations
            if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
                # Check for HyperMorphic model
                is_hypermorphic = hasattr(agent.model, 'hyper_math')

                # Perform HyperMorphic weight restructuring
                if is_hypermorphic and hasattr(agent.model, '_apply_holomorphic_transform'):
                    # Apply holomorphic transformation that preserves structure while changing weights
                    agent.model._apply_holomorphic_transform()
                    changes.append("Applied holomorphic weight transformation to neural architecture")
                    restructuring_count += 1
                elif hasattr(agent.model, 'state_dict') and hasattr(agent.model, 'load_state_dict'):
                    # For non-HyperMorphic, apply simpler weight perturbation
                    with torch.no_grad():
                        state_dict = agent.model.state_dict()

                        # Apply phase-based perturbation to weights
                        for name, param in state_dict.items():
                            if 'weight' in name:
                                # Small random perturbation
                                phase = random.uniform(0, 2 * math.pi)
                                perturbation = 0.05 * torch.randn_like(param) * math.cos(phase)
                                param.add_(perturbation)

                        # Load modified state dict
                        agent.model.load_state_dict(state_dict)

                    changes.append("Applied weight perturbation to neural architecture")
                    restructuring_count += 1

            # 5. Restructure domain intelligence with HyperMorphic recategorization
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'domain_intelligence'):
                intelligence = agent.free_will.domain_intelligence

                # Check for HyperMorphic domain intelligence
                is_hypermorphic = hasattr(intelligence, 'hyper_math')

                # Restructure domain categories with HyperMorphic operations
                if hasattr(intelligence, 'domain_categories') and intelligence.domain_categories:
                    # Count domains before restructuring
                    domains_before = sum(len(domains) for domains in intelligence.domain_categories.values())

                    # Calculate new category thresholds with phase modulation
                    category_thresholds = {}
                    for category in intelligence.domain_categories:
                        phase = random.uniform(0, 2 * math.pi)
                        threshold = 0.5 + 0.3 * math.cos(phase)

                        if is_hypermorphic:
                            category_thresholds[category] = intelligence.hyper_math.zero_free(threshold)
                        else:
                            category_thresholds[category] = threshold

                    # Restructure domain categorization
                    if hasattr(intelligence, 'domain_knowledge'):
                        for domain, data in intelligence.domain_knowledge.items():
                            if isinstance(data, dict) and "category_scores" in data:
                                scores = data["category_scores"]

                                # Find best category based on new thresholds
                                best_category = None
                                best_score = 0

                                for category, score in scores.items():
                                    if category in category_thresholds:
                                        threshold = category_thresholds[category]

                                        if is_hypermorphic:
                                            adjusted_score = intelligence.hyper_math.mul(
                                                intelligence.hyper_math.zero_free(score),
                                                threshold
                                            )
                                        else:
                                            adjusted_score = score * threshold

                                        if best_category is None or adjusted_score > best_score:
                                            best_category = category
                                            best_score = adjusted_score

                                if best_category and best_category != data.get("primary_category"):
                                    # Update domain category
                                    old_category = data.get("primary_category")
                                    data["primary_category"] = best_category

                                    # Update category indices
                                    if old_category and old_category in intelligence.domain_categories:
                                        intelligence.domain_categories[old_category].discard(domain)

                                    if best_category not in intelligence.domain_categories:
                                        intelligence.domain_categories[best_category] = set()

                                    intelligence.domain_categories[best_category].add(domain)

                    # Count domains after restructuring
                    domains_after = sum(len(domains) for domains in intelligence.domain_categories.values())

                    if domains_before != domains_after:
                        changes.append(f"Restructured domain intelligence categories: {domains_before} → {domains_after} domain categorizations")
                        restructuring_count += 1

            # Update strategy success rate based on restructuring count
            self.strategies["restructuring"]["success_rate"] = self.hyper_math.min(
                self.hyper_math.zero_free(0.95),  # Cap at 0.95
                self.hyper_math.mul(
                    self.strategies["restructuring"]["success_rate"],
                    self.hyper_math.zero_free(1.1)  # 10% increase
                )
            )

            success = restructuring_count > 0

            if success:
                return True, f"Successfully restructured {restructuring_count} system components using HyperMorphic holomorphic transformations", changes
            else:
                return False, "No components were suitable for HyperMorphic restructuring", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["restructuring"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["restructuring"]["success_rate"],
                    self.hyper_math.zero_free(0.9)  # 10% decrease
                )
            )

            return False, f"HyperMorphic restructuring failed: {str(e)}", changes



    def _apply_quantum_variation_strategy(self, agent, changes):
        """
        Apply quantum variation strategy to create non-deterministic variations
        in the system architecture while preserving holomorphic structure.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Count successful quantum variations
            variation_count = 0

            # 1. Apply quantum variations to consciousness module
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
                consciousness = agent.ai_manager.consciousness
                # Check for HyperMorphic consciousness
                is_hypermorphic = hasattr(consciousness, 'hyper_math')
                # Apply quantum fluctuations to awareness level
                if hasattr(consciousness, 'awareness_level'):
                    old_awareness = consciousness.awareness_level
                    # Generate quantum phase
                    phase = random.uniform(0, 2 * math.pi)
                    if is_hypermorphic:
                        # HyperMorphic zero-free fluctuation
                        fluctuation = consciousness.hyper_math.mul(
                            consciousness.hyper_math.zero_free(0.2),  # Scale of fluctuation
                            consciousness.hyper_math.zero_free(math.sin(phase))  # Phase-based oscillation
                        )
                        # Apply fluctuation to awareness
                        new_awareness = consciousness.hyper_math.add(old_awareness, fluctuation)
                        # Ensure within valid range
                        new_awareness = consciousness.hyper_math.max(
                            consciousness.hyper_math.zero_free(0.1),
                            consciousness.hyper_math.min(
                                consciousness.hyper_math.zero_free(0.9),
                                new_awareness
                            )
                        )
                    else:
                        # Standard fluctuation for non-HyperMorphic
                        fluctuation = 0.2 * math.sin(phase)
                        new_awareness = max(0.1, min(0.9, old_awareness + fluctuation))
                    # Apply new awareness
                    consciousness.awareness_level = new_awareness
                    changes.append(f"Applied quantum fluctuation to consciousness awareness: {old_awareness:.2f} → {new_awareness:.2f}")
                    variation_count += 1

                # Apply quantum jump to consciousness state if available
                if hasattr(consciousness, 'current_state') and hasattr(consciousness, 'states'):
                    old_state = consciousness.current_state
                    if random.random() < 0.3:  # 30% chance of quantum jump
                        # Get available states
                        available_states = list(consciousness.states.keys())
                        # Select new state (different from current)
                        new_states = [s for s in available_states if s != old_state]
                        if new_states:
                            new_state = random.choice(new_states)
                            # Apply quantum jump
                            consciousness.current_state = new_state
                            changes.append(f"Applied quantum jump to consciousness state: {old_state} → {new_state}")
                            variation_count += 1

            # 2. Apply quantum variation to neural architecture
            if hasattr(agent, 'model'):
                # Check for HyperMorphic model
                is_hypermorphic = hasattr(agent.model, 'hyper_math')
                if hasattr(agent.model, 'state_dict') and hasattr(agent.model, 'load_state_dict'):
                    with torch.no_grad():
                        state_dict = agent.model.state_dict()
                        # Track parameter statistics before changes
                        param_count = 0
                        perturbed_count = 0
                        # Apply phase-based quantum perturbation to weights
                        for name, param in state_dict.items():
                            if 'weight' in name or 'bias' in name:
                                param_count += 1
                                # Generate quantum perturbation with phase coherence
                                phase = random.uniform(0, 2 * math.pi)
                                # Smaller perturbation for stability
                                perturbation_scale = 0.02
                                if is_hypermorphic:
                                    # HyperMorphic zero-free perturbation
                                    perturbation = agent.model.hyper_math.zero_free(
                                        perturbation_scale * torch.randn_like(param) * math.cos(phase)
                                    )
                                    if hasattr(agent.model, '_ensure_zero_free'):
                                        param.add_(perturbation)
                                        param.data = agent.model._ensure_zero_free(param.data)
                                    else:
                                        param.add_(perturbation)
                                else:
                                    # Standard quantum-like perturbation for non-HyperMorphic
                                    perturbation = perturbation_scale * torch.randn_like(param) * math.cos(phase)
                                    param.add_(perturbation)
                                perturbed_count += 1
                        changes.append(f"Applied quantum phase-coherent perturbation to {perturbed_count}/{param_count} neural parameters")
                        variation_count += 1

                # Apply quantum variation to quantum-specific parameters if they exist
                if is_hypermorphic:
                    quantum_params_changed = 0
                    if hasattr(agent.model, 'quantum_coherence'):
                        old_coherence = agent.model.quantum_coherence
                        phase = random.uniform(0, 2 * math.pi)
                        fluctuation = agent.model.hyper_math.zero_free(0.2 * math.sin(phase))
                        new_coherence = agent.model.hyper_math.add(old_coherence, fluctuation)
                        new_coherence = agent.model.hyper_math.max(
                            agent.model.hyper_math.zero_free(0.3),
                            agent.model.hyper_math.min(
                                agent.model.hyper_math.zero_free(0.9),
                                new_coherence
                            )
                        )
                        agent.model.quantum_coherence = new_coherence
                        quantum_params_changed += 1

                    if hasattr(agent.model, 'neocortex'):
                        for layer in agent.model.neocortex:
                            if hasattr(layer, 'attention') and hasattr(layer.attention, 'phase_shifts'):
                                phase_shifts = layer.attention.phase_shifts
                                with torch.no_grad():
                                    for i in range(len(phase_shifts)):
                                        if random.random() < 0.3:
                                            new_phase = random.uniform(0, 2 * math.pi)
                                            phase_shifts[i] = new_phase
                                quantum_params_changed += 1
                    if quantum_params_changed > 0:
                        changes.append(f"Applied quantum variations to {quantum_params_changed} quantum-specific parameters")
                        variation_count += 1

            # 3. Apply quantum variation to free will decision mechanisms
            if hasattr(agent, 'free_will'):
                is_hypermorphic = hasattr(agent.free_will, 'hyper_math')
                quantum_variations = 0
                if hasattr(agent.free_will, 'exploration_weight') and hasattr(agent.free_will, 'exploitation_weight'):
                    old_expl = agent.free_will.exploration_weight
                    old_expt = agent.free_will.exploitation_weight
                    phase = random.uniform(0, 2 * math.pi)
                    if is_hypermorphic:
                        fluctuation = agent.free_will.hyper_math.mul(
                            agent.free_will.hyper_math.zero_free(0.15),
                            agent.free_will.hyper_math.zero_free(math.sin(phase))
                        )
                        new_expl = agent.free_will.hyper_math.add(old_expl, fluctuation)
                        new_expl = agent.free_will.hyper_math.max(
                            agent.free_will.hyper_math.zero_free(0.2),
                            agent.free_will.hyper_math.min(
                                agent.free_will.hyper_math.zero_free(0.8),
                                new_expl
                            )
                        )
                        new_expt = agent.free_will.hyper_math.sub(
                            agent.free_will.hyper_math.zero_free(1.0),
                            new_expl
                        )
                    else:
                        fluctuation = 0.15 * math.sin(phase)
                        new_expl = max(0.2, min(0.8, old_expl + fluctuation))
                        new_expt = 1.0 - new_expl
                    agent.free_will.exploration_weight = new_expl
                    agent.free_will.exploitation_weight = new_expt
                    changes.append(f"Applied quantum variation to exploration/exploitation balance: {old_expl:.2f}/{old_expt:.2f} → {new_expl:.2f}/{new_expt:.2f}")
                    quantum_variations += 1

                if is_hypermorphic and hasattr(agent.free_will, 'quantum_state'):
                    quantum_state = agent.free_will.quantum_state
                    for state, properties in quantum_state.items():
                        if "amplitude" in properties and "phase" in properties:
                            amplitude = properties["amplitude"]
                            phase_val = properties["phase"]
                            amplitude_fluctuation = agent.free_will.hyper_math.mul(
                                agent.free_will.hyper_math.zero_free(0.2),
                                agent.free_will.hyper_math.zero_free(random.random() - 0.5)
                            )
                            new_amplitude = agent.free_will.hyper_math.add(amplitude, amplitude_fluctuation)
                            new_amplitude = agent.free_will.hyper_math.max(
                                agent.free_will.hyper_math.zero_free(0.1),
                                agent.free_will.hyper_math.min(
                                    agent.free_will.hyper_math.zero_free(1.5),
                                    new_amplitude
                                )
                            )
                            phase_shift = random.uniform(-math.pi/2, math.pi/2)
                            new_phase = (phase_val + phase_shift) % (2 * math.pi)
                            properties["amplitude"] = new_amplitude
                            properties["phase"] = new_phase
                    amplitude_squared_sum = agent.free_will.hyper_math.zero_free(0)
                    for state, properties in quantum_state.items():
                        if "amplitude" in properties:
                            amplitude_squared = agent.free_will.hyper_math.mul(
                                properties["amplitude"],
                                properties["amplitude"]
                            )
                            amplitude_squared_sum = agent.free_will.hyper_math.add(
                                amplitude_squared_sum,
                                amplitude_squared
                            )
                    if abs(amplitude_squared_sum - 1.0) > 0.1:
                        norm_factor = agent.free_will.hyper_math.zero_free(
                            1.0 / math.sqrt(max(amplitude_squared_sum, agent.free_will.hyper_math.epsilon))
                        )
                        for state, properties in quantum_state.items():
                            if "amplitude" in properties:
                                properties["amplitude"] = agent.free_will.hyper_math.mul(
                                    properties["amplitude"],
                                    norm_factor
                                )
                    changes.append("Applied quantum superposition variations to free will quantum states")
                    quantum_variations += 1

                if is_hypermorphic and hasattr(agent.free_will, 'quantum_influence_weight'):
                    old_weight = agent.free_will.quantum_influence_weight
                    phase = random.uniform(0, 2 * math.pi)
                    fluctuation = agent.free_will.hyper_math.mul(
                        agent.free_will.hyper_math.zero_free(0.1),
                        agent.free_will.hyper_math.zero_free(math.sin(phase))
                    )
                    new_weight = agent.free_will.hyper_math.add(old_weight, fluctuation)
                    new_weight = agent.free_will.hyper_math.max(
                        agent.free_will.hyper_math.zero_free(0.2),
                        agent.free_will.hyper_math.min(
                            agent.free_will.hyper_math.zero_free(0.6),
                            new_weight
                        )
                    )
                    agent.free_will.quantum_influence_weight = new_weight
                    changes.append(f"Applied quantum variation to decision quantum influence: {old_weight:.2f} → {new_weight:.2f}")
                    quantum_variations += 1

                if quantum_variations > 0:
                    variation_count += 1

            # 4. Apply quantum variation to planner strategies
            if hasattr(agent, 'planner_sifter') and hasattr(agent.planner_sifter, 'strategies'):
                strategies = agent.planner_sifter.strategies
                if strategies:
                    original_count = len(strategies)
                    modified_strategies = 0
                    for strategy_name, strategy in strategies.items():
                        if "effectiveness" in strategy:
                            old_effectiveness = strategy["effectiveness"]
                            phase = random.uniform(0, 2 * math.pi)
                            fluctuation = 0.1 * math.sin(phase)
                            new_effectiveness = max(0.2, min(0.95, old_effectiveness + fluctuation))
                            strategy["effectiveness"] = new_effectiveness
                            modified_strategies += 1
                    strategy_names = list(strategies.keys())
                    if len(strategy_names) >= 2:
                        num_pairs = min(2, len(strategy_names) // 2)
                        for _ in range(num_pairs):
                            s1, s2 = random.sample(strategy_names, 2)
                            if "effectiveness" in strategies[s1] and "effectiveness" in strategies[s2]:
                                e1 = strategies[s1]["effectiveness"]
                                e2 = strategies[s2]["effectiveness"]
                                avg = (e1 + e2) / 2
                                diff = (e1 - e2) / 2
                                strategies[s1]["effectiveness"] = max(0.2, min(0.95, avg + diff))
                                strategies[s2]["effectiveness"] = max(0.2, min(0.95, avg - diff))
                                modified_strategies += 2
                    if modified_strategies > 0:
                        changes.append(f"Applied quantum variations to {modified_strategies} planning strategies")
                        variation_count += 1

            # Update strategy success rate based on variation count
            if variation_count > 0:
                self.strategies["quantum_variation"]["success_rate"] = self.hyper_math.min(
                    self.hyper_math.zero_free(0.95),
                    self.hyper_math.mul(
                        self.strategies["quantum_variation"]["success_rate"],
                        self.hyper_math.zero_free(1.1)
                    )
                )
                return True, f"Successfully applied {variation_count} quantum variations using HyperMorphic principles", changes
            else:
                return False, "No suitable components found for quantum variation", changes

        except Exception as e:
            self.strategies["quantum_variation"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["quantum_variation"]["success_rate"],
                    self.hyper_math.zero_free(0.9)
                )
            )
        return False, f"HyperMorphic quantum variation failed: {str(e)}",

class AutonomousMind:
    """
    Integrated cognitive system that coordinates high-level thinking processes
    and manages the agent's internal cognitive states and modes.
    """
    def __init__(self, agent, model):
        self.agent = agent
        self.model = model
        self.thought_history = []
        self.cognitive_states = {
            "analytical": {"description": "Logical problem solving with structured reasoning", "activation": 0.5},
            "creative": {"description": "Divergent thinking with novel connections", "activation": 0.5},
            "reflective": {"description": "Meta-cognitive examination of own thoughts", "activation": 0.5},
            "exploratory": {"description": "Curious investigation of new information", "activation": 0.5},
            "critical": {"description": "Evaluative assessment with skepticism", "activation": 0.5},
            "integrative": {"description": "Synthesis of diverse knowledge", "activation": 0.5},
            "intuitive": {"description": "Fast pattern recognition", "activation": 0.5},
            "balanced": {"description": "Equilibrium of multiple modes", "activation": 0.5}
        }
        self.current_mode = "balanced"  # Default mode
        self.attention_focus = None  # Current attentional focus
        self.working_memory = []  # Active concepts and thoughts
        self.working_memory_capacity = 7  # Miller's magical number
        self.cognitive_load = 0.5  # Current processing load (0.0-1.0)
        self.thought_depth = 0.5  # Depth vs. breadth of thinking (0.0-1.0)
        self.concept_activation_threshold = 0.3  # Min activation for attention
        self.mode_shift_probability = 0.2  # Probability of spontaneous mode shift
        self.recent_insights = []  # Store recent important realizations

        # Thinking style parameters
        self.thinking_style = {
            "abstraction_level": 0.6,  # Concrete (0.0) to abstract (1.0)
            "linearity": 0.5,  # Linear (0.0) to non-linear (1.0)
            "deductive_inductive_balance": 0.5,  # Deductive (0.0) to inductive (1.0)
            "risk_tolerance": 0.4,  # Risk-averse (0.0) to risk-seeking (1.0)
            "concept_granularity": 0.5  # Fine-grained (0.0) to coarse-grained (1.0)
        }

        log_event("AutonomousMind initialized in balanced cognitive mode", "INFO")

    def think(self, context=None):
        """
        Execute a thinking cycle to generate insights, shift cognitive modes,
        and update internal state based on current context.

        Parameters:
        - context: Current perceptual and memory context

        Returns:
        - Thought result containing insights and state changes
        """
        # Update cognitive load based on context complexity
        self._update_cognitive_load(context)

        # Consider shifting cognitive mode
        self._consider_mode_shift(context)

        # Generate thought based on current mode and context
        thought = self._generate_thought(context)

        # Update working memory
        self._update_working_memory(thought)

        # Store in thought history
        self.thought_history.append({
            "thought": thought,
            "mode": self.current_mode,
            "timestamp": datetime.now().isoformat(),
            "context": self._summarize_context(context)
        })

        # Trim history if needed
        if len(self.thought_history) > 100:
            self.thought_history = self.thought_history[-100:]

        return thought

    def _update_cognitive_load(self, context):
        """Update cognitive load based on context complexity"""
        if not context:
            # Default slight reduction in cognitive load when no new input
            self.cognitive_load = max(0.1, self.cognitive_load * 0.9)
            return

        # Estimate context complexity
        complexity = 0.5  # Default medium complexity

        # Adjust based on context details if available
        if isinstance(context, dict):
            # More elements = higher complexity
            complexity += min(0.3, len(context) * 0.02)

            # Check for special high-complexity elements
            if "error" in context or "anomaly" in context:
                complexity += 0.2

            # High memory usage increases cognitive load
            if "memory_size" in context:
                memory_usage = context["memory_size"] / MEMORY_MAX_SIZE
                complexity += memory_usage * 0.2

            # Multiple recent actions increase complexity
            if "recent_actions" in context and isinstance(context["recent_actions"], list):
                complexity += min(0.1, len(context["recent_actions"]) * 0.02)

        # Dynamically adjust cognitive load
        # New complexity pulls the current load toward itself
        adjustment_rate = 0.3  # How quickly load adjusts
        self.cognitive_load = self.cognitive_load * (1 - adjustment_rate) + complexity * adjustment_rate

        # Ensure within bounds
        self.cognitive_load = max(0.1, min(0.9, self.cognitive_load))

    def _consider_mode_shift(self, context):
        """Consider shifting cognitive mode based on context and internal state"""
        # Base probability of mode shift
        shift_probability = self.mode_shift_probability

        # Adjust based on cognitive load - higher load may require mode shift
        if self.cognitive_load > 0.7:
            shift_probability += 0.2

        # Check if we should shift
        if random.random() >= shift_probability:
            return  # No shift this cycle

        # Current active states
        active_states = {state: data["activation"] for state, data in self.cognitive_states.items()
                       if data["activation"] >= self.concept_activation_threshold}

        # Context-based mode selection
        new_mode = self._select_appropriate_mode(context, active_states)

        # If new mode is different, make the shift
        if new_mode and new_mode != self.current_mode:
            old_mode = self.current_mode
            self.current_mode = new_mode

            log_event(f"Cognitive mode shifted: {old_mode} → {new_mode}", "INFO")

            # Adjust activations - boost new mode, slightly reduce others
            for state in self.cognitive_states:
                if state == new_mode:
                    self.cognitive_states[state]["activation"] = min(0.9, self.cognitive_states[state]["activation"] + 0.2)
                else:
                    self.cognitive_states[state]["activation"] = max(0.1, self.cognitive_states[state]["activation"] * 0.9)

    def _select_appropriate_mode(self, context, active_states):
        """
        Select most appropriate cognitive mode based on context.

        Parameters:
        - context: Current context dictionary
        - active_states: Dictionary of currently active cognitive states

        Returns:
        - Selected mode name
        """
        if not context:
            # Without context, weighted random selection from active states
            if active_states:
                states = list(active_states.keys())
                weights = [active_states[s] for s in states]
                return random.choices(states, weights=weights, k=1)[0]
            else:
                return "balanced"  # Default fallback

        # Context-based selection
        mode_scores = {mode: 0.0 for mode in self.cognitive_states}

        # 1. Check for error or anomaly - activates critical mode
        if "error" in context or "anomaly" in context:
            mode_scores["critical"] += 0.5

        # 2. Check for exploration needs
        if self._context_indicates_exploration(context):
            mode_scores["exploratory"] += 0.4
            mode_scores["creative"] += 0.3

        # 3. Check for complex problem solving
        if self._context_indicates_complex_problem(context):
            mode_scores["analytical"] += 0.5
            mode_scores["integrative"] += 0.3

        # 4. Check for learning and reflection needs
        if self._context_indicates_reflection(context):
            mode_scores["reflective"] += 0.5

        # 5. Check for pattern recognition needs
        if self._context_indicates_pattern_recognition(context):
            mode_scores["intuitive"] += 0.4

        # 6. Add random factor for exploration
        for mode in mode_scores:
            mode_scores[mode] += random.uniform(0, 0.2)

        # 7. Add existing activation bias
        for mode, activation in active_states.items():
            mode_scores[mode] += activation * 0.3

        # Select highest scoring mode
        if mode_scores:
            top_mode = max(mode_scores.items(), key=lambda x: x[1])[0]
            return top_mode
        else:
            return "balanced"  # Default fallback

    def _context_indicates_exploration(self, context):
        """Check if context suggests exploration needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Looking for exploration indicators
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            if "explor" in goal_desc or "discover" in goal_desc:
                indicators += 1

        # Few domains visited suggests exploration need
        if "domains_visited" in context:
            domains = context["domains_visited"]
            if isinstance(domains, set) and len(domains) < 10:
                indicators += 1

        # Current mode includes exploration elements
        if context.get("thinking_mode") in ["exploratory", "creative"]:
            indicators += 1

        return indicators >= 1

    def _context_indicates_complex_problem(self, context):
        """Check if context suggests complex problem solving needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Goals with certain keywords
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            complex_terms = ["optim", "improv", "analy", "solv", "complex"]
            if any(term in goal_desc for term in complex_terms):
                indicators += 1

        # High cognitive load suggests complex problem
        if self.cognitive_load > 0.7:
            indicators += 1

        # Current analytical thinking mode
        if context.get("thinking_mode") in ["analytical", "critical"]:
            indicators += 1

        return indicators >= 1

    def _context_indicates_reflection(self, context):
        """Check if context suggests reflection needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Goals with reflection keywords
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            reflection_terms = ["reflect", "learn", "adapt", "improv", "assess"]
            if any(term in goal_desc for term in reflection_terms):
                indicators += 1

        # After many actions, reflection is valuable
        if "stats" in context and isinstance(context["stats"], dict):
            cycles = context["stats"].get("cycles_run", 0)
            if cycles > 0 and cycles % 10 == 0:  # Every 10 cycles
                indicators += 1

        # Current reflective thinking mode
        if context.get("thinking_mode") == "reflective":
            indicators += 1

        return indicators >= 1

    def _context_indicates_pattern_recognition(self, context):
        """Check if context suggests pattern recognition needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Domain stats suggest pattern recognition
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            # Many domains = opportunity for pattern recognition
            if len(context["domain_stats"]) > 5:
                indicators += 1

        # Multiple recent actions create patterns
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            if len(context["recent_actions"]) >= 3:
                indicators += 1

        # Current intuitive mode
        if context.get("thinking_mode") == "intuitive":
            indicators += 1

        return indicators >= 1

    def _generate_thought(self, context):
        """
        Generate a thought based on current cognitive mode and context.

        Parameters:
        - context: Current context dictionary

        Returns:
        - Generated thought dictionary
        """
        # Each cognitive mode has a different thought generation approach
        if self.current_mode == "analytical":
            return self._generate_analytical_thought(context)
        elif self.current_mode == "creative":
            return self._generate_creative_thought(context)
        elif self.current_mode == "reflective":
            return self._generate_reflective_thought(context)
        elif self.current_mode == "exploratory":
            return self._generate_exploratory_thought(context)
        elif self.current_mode == "critical":
            return self._generate_critical_thought(context)
        elif self.current_mode == "integrative":
            return self._generate_integrative_thought(context)
        elif self.current_mode == "intuitive":
            return self._generate_intuitive_thought(context)
        else:  # balanced or any other
            return self._generate_balanced_thought(context)

    def _generate_analytical_thought(self, context):
        """Generate an analytical thought focused on logical problem solving"""
        # Default analytical thought structure
        thought = {
            "type": "analytical",
            "content": "Systematic analysis of current state and options",
            "components": [],
            "insights": [],
            "importance": 0.5
        }

        # Without context, generate generic analytical thought
        if not context:
            thought["content"] = "Need more information to perform proper analysis"
            return thought

        # Analytical components based on context
        components = []

        # Analyze current goal if available
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal = context["current_goal"]
            components.append({
                "focus": "goal_analysis",
                "content": f"Analyzing goal: {goal.get('description', 'unknown')}",
                "priority": goal.get("priority", 0.5)
            })

        # Analyze recent actions if available
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            actions = context["recent_actions"]
            if actions:
                action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                components.append({
                    "focus": "action_pattern_analysis",
                    "content": f"Recent action sequence: {' → '.join(action_types)}",
                    "outcome_assessment": self._assess_action_outcomes(actions)
                })

        # Analyze domain statistics if available
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                high_error_domains = [d for d, stats in domains.items()
                                    if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3]
                if high_error_domains:
                    components.append({
                        "focus": "error_analysis",
                        "content": f"High error rates detected in domains: {', '.join(high_error_domains[:3])}",
                        "recommendation": "Investigate causes or avoid these domains temporarily"
                    })

        # Generate insights based on analysis
        insights = []

        # Derive insights from components
        for component in components:
            if component["focus"] == "goal_analysis" and component.get("priority", 0) > 0.7:
                insights.append("Current goal is high priority - allocate additional resources")
            elif component["focus"] == "action_pattern_analysis":
                outcome = component.get("outcome_assessment")
                if outcome == "deteriorating":
                    insights.append("Action patterns show declining effectiveness - strategy change needed")
                elif outcome == "improving":
                    insights.append("Action patterns show improving outcomes - continue current approach")
            elif component["focus"] == "error_analysis":
                insights.append("Error pattern analysis suggests need for improved domain validation")

        # Add components and insights to thought
        thought["components"] = components
        thought["insights"] = insights

        # Calculate importance based on components and insights
        if insights:
            thought["importance"] = 0.7

        # Generate summary content
        if insights:
            thought["content"] = f"Analytical conclusion: {insights[0]}"
        elif components:
            thought["content"] = f"Analysis of {len(components)} system components completed"

        return thought

    def _generate_creative_thought(self, context):
        """Generate a creative thought focused on novel connections"""
        # Default creative thought structure
        thought = {
            "type": "creative",
            "content": "Novel conceptual connection",
            "associations": [],
            "insights": [],
            "importance": 0.5
        }

        # Creative aspects to consider
        aspects = ["goals", "domains", "strategies", "patterns", "anomalies"]
        selected_aspects = random.sample(aspects, min(2, len(aspects)))

        # Generate random associations between selected aspects
        associations = []

        # Extract relevant elements from context
        elements = self._extract_elements_from_context(context, selected_aspects)

        # Generate associations between elements
        if len(elements) >= 2:
            # Take two random elements and create association
            element_pairs = []
            for i in range(min(3, len(elements))):
                pair = random.sample(elements, 2)
                element_pairs.append(pair)

            # Create associations from pairs
            for pair in element_pairs:
                association_templates = [
                    f"Unexpected connection between {pair[0]} and {pair[1]}",
                    f"{pair[0]} could be viewed through the lens of {pair[1]}",
                    f"What if the structure of {pair[0]} were applied to {pair[1]}?",
                    f"The patterns in {pair[0]} mirror aspects of {pair[1]} in a novel way"
                ]
                associations.append(random.choice(association_templates))
        else:
            # Fallback for limited context
            association_templates = [
                "Novel perspective: consider alternative goal structures",
                "What if search and evaluation were integrated more tightly?",
                "The pattern of successful interactions might reveal emergent properties"
            ]
            associations.append(random.choice(association_templates))

        # Generate insights from associations
        insights = []
        if associations:
            for association in associations:
                if "connection between" in association:
                    insights.append(f"Explore this connection to potentially develop new capabilities")
                elif "through the lens" in association:
                    insights.append(f"This perspective shift might reveal hidden patterns")
                elif "What if" in association:
                    insights.append(f"This hypothetical restructuring could improve system flexibility")
                else:
                    insights.append(f"This pattern recognition suggests deeper structural similarities")

        # Add associations and insights to thought
        thought["associations"] = associations
        thought["insights"] = insights

        # Calculate importance
        if insights:
            thought["importance"] = 0.6 + 0.1 * len(insights)


def _apply_learning_rate_safeguards(self, new_lr):
    """Prevent learning rate from spiraling into oblivion"""
    # Establish absolute minimum learning rate
    ABSOLUTE_MIN_LR = 5e-6

    if new_lr < ABSOLUTE_MIN_LR:
        log_event(f"Learning rate hit critical threshold: {new_lr:.8f}, resetting to {ABSOLUTE_MIN_LR:.6f}", "WARNING")
        return ABSOLUTE_MIN_LR

    # Prevent excessive downward adjustment
    if self.learning_rate_history and new_lr < self.learning_rate_history[-1] * 0.5:
        safer_lr = self.learning_rate_history[-1] * 0.8
        log_event(f"Excessive LR reduction prevented: {new_lr:.8f} → {safer_lr:.6f}", "INFO")
        return safer_lr

    return new_lr






class HyperMorphicNormalization(nn.Module):
    """
    HyperMorphic Normalization Layer

    A custom normalization layer that can be extended with hypermorphic operations.
    Currently, it wraps the standard nn.LayerNorm but can be modified to include
    additional stability or adaptive features specific to HyperMorphic operations.

    Args:
        embed_dim (int): Dimensionality of the input.
        eps (float): A value added to the denominator for numerical stability (default=1e-5).
    """
    def __init__(self, embed_dim, eps=1e-5):
        super(HyperMorphicNormalization, self).__init__()
        self.layernorm = nn.LayerNorm(embed_dim, eps=eps)

    def forward(self, x):
        # You can add custom hypermorphic adjustments here if needed.
        return self.layernorm(x)







class HyperMorphicMemoryConsolidation:
    """
    Advanced memory consolidation system that implements the principles of
    human-inspired memory consolidation with HyperMorphic zero-free operations.

    Features:
    - Multi-stage memory model (sensory, working, episodic, semantic, procedural)
    - Temporal pattern recognition for memories
    - Sleep-inspired consolidation phases
    - Importance-weighted retention with HyperMorphic weighting
    - Cross-domain knowledge synthesis
    """

    def __init__(self, agent, hyper_math=None, epsilon=1e-12):
        """
        Initialize the memory consolidation system.

        Args:
            agent: Agent instance to connect with
            hyper_math: Optional HyperMorphicMath instance, created if not provided
            epsilon: HyperMorphic nearness element ε_ᵩ
        """
        self.agent = agent
        self.hyper_math = hyper_math if hyper_math else HyperMorphicMath(epsilon=epsilon)

        # Memory stages
        self.sensory_buffer = []  # Very short-term, high-capacity buffer
        self.working_memory = []  # Limited capacity, active processing
        self.episodic_memory = {}  # Time-based experiential memories
        self.semantic_memory = {}  # Factual knowledge, domain concepts
        self.procedural_memory = {}  # Action patterns and strategies

        # Consolidation parameters
        self.consolidation_interval = 10  # cycles between consolidation
        self.retention_threshold = self.hyper_math.zero_free(0.3)  # Min importance for retention
        self.consolidation_phase = 0  # Current phase of consolidation process
        self.last_consolidation = 0  # Cycle of last consolidation

        # Memory metadata
        self.memory_access_counts = {}  # Track memory usage
        self.memory_connections = {}  # Associations between memories
        self.memory_timestamps = {}  # When memories were added/modified

        # Performance metrics
        self.retrieval_stats = {'hits': 0, 'misses': 0, 'avg_retrieval_time': 0}

        log_event("HyperMorphic Memory Consolidation system initialized", "INFO")

    def process_observation(self, observation, cycle_count):
        """
        Process incoming observation through memory stages.

        Args:
            observation: Current observation dictionary
            cycle_count: Current agent cycle count
        """
        if not observation:
            return

        # 1. Add to sensory buffer with HyperMorphic timestamp
        obs_id = f"obs_{cycle_count}_{hash(str(observation))}"
        timestamp = self.hyper_math.zero_free(cycle_count)

        self.sensory_buffer.append({
            "id": obs_id,
            "content": observation,
            "timestamp": timestamp,
            "importance": self.calculate_importance(observation)
        })

        # 2. Limit sensory buffer size with HyperMorphic constraint
        max_buffer = 20
        if len(self.sensory_buffer) > max_buffer:
            # Remove oldest items
            self.sensory_buffer = self.sensory_buffer[-max_buffer:]

        # 3. Transfer important items to working memory
        self._update_working_memory()

        # 4. Perform consolidation if interval reached
        if cycle_count - self.last_consolidation >= self.consolidation_interval:
            self.consolidate_memories(cycle_count)
            self.last_consolidation = cycle_count

    def calculate_importance(self, observation):
        """
        Calculate importance score of an observation with HyperMorphic operations.

        Args:
            observation: Observation to evaluate

        Returns:
            Importance score (0.0-1.0) as HyperMorphic zero-free value
        """
        if not observation or not isinstance(observation, dict):
            return self.hyper_math.zero_free(0.1)  # Default low importance

        # Base importance
        importance = self.hyper_math.zero_free(0.5)

        # Factor 1: Error presence (errors are important to remember)
        if "error" in observation or "warning" in observation:
            error_boost = self.hyper_math.zero_free(0.3)
            importance = self.hyper_math.add(importance, error_boost)

        # Factor 2: Action success (successful actions worth remembering)
        if "success" in observation and observation["success"]:
            success_boost = self.hyper_math.zero_free(0.2)
            importance = self.hyper_math.add(importance, success_boost)

        # Factor 3: Domain novelty
        if "domain" in observation:
            domain = observation["domain"]
            if domain not in self.memory_access_counts:
                novelty_boost = self.hyper_math.zero_free(0.25)
                importance = self.hyper_math.add(importance, novelty_boost)

        # Factor 4: Content richness
        if "content_length" in observation:
            length = observation["content_length"]
            # Normalize length to 0-0.2 range with HyperMorphic operations
            length_factor = self.hyper_math.min(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.div(
                    self.hyper_math.zero_free(length),
                    self.hyper_math.zero_free(5000)
                )
            )
            importance = self.hyper_math.add(importance, length_factor)

        # Ensure importance is capped with HyperMorphic operation
        importance = self.hyper_math.min(
            self.hyper_math.zero_free(1.0),
            importance
        )

        return importance

    def _update_working_memory(self):
        """
        Update working memory with important sensory observations.
        Implements HyperMorphic importance-based filtering.
        """
        # Working memory capacity limit with HyperMorphic constraint
        wm_capacity = 7  # Miller's magical number

        # Find high-importance items from sensory buffer
        important_items = []
        for item in self.sensory_buffer:
            if item["importance"] > self.hyper_math.zero_free(0.6):
                important_items.append(item)

        # Sort by importance (high to low)
        important_items.sort(key=lambda x: x["importance"], reverse=True)

        # Add to working memory
        for item in important_items[:3]:  # Add up to 3 new items
            if len(self.working_memory) < wm_capacity:
                self.working_memory.append(item)
            else:
                # Replace lowest importance item
                self.working_memory.sort(key=lambda x: x["importance"])
                self.working_memory[0] = item
                self.working_memory.sort(key=lambda x: x["importance"], reverse=True)

    def consolidate_memories(self, cycle_count):
        """
        Perform memory consolidation process.
        Transfers memories between stages and reinforces connections.

        Args:
            cycle_count: Current agent cycle count
        """
        log_event(f"Starting memory consolidation phase {self.consolidation_phase}", "INFO")

        # Phase 0: Working memory to episodic memory
        if self.consolidation_phase == 0:
            self._consolidate_to_episodic()

        # Phase 1: Episodic memory to semantic memory
        elif self.consolidation_phase == 1:
            self._consolidate_to_semantic()

        # Phase 2: Extract procedural patterns
        elif self.consolidation_phase == 2:
            self._consolidate_to_procedural()

        # Phase 3: Prune low-importance memories
        elif self.consolidation_phase == 3:
            self._prune_memories()

        # Phase 4: Reinforce connections
        elif self.consolidation_phase == 4:
            self._reinforce_connections()

        # Update consolidation phase
        self.consolidation_phase = (self.consolidation_phase + 1) % 5

        # Update access timestamps with HyperMorphic timestamping
        cycle_timestamp = self.hyper_math.zero_free(cycle_count)
        self.memory_timestamps["last_consolidation"] = cycle_timestamp

        log_event(f"Memory consolidation phase {self.consolidation_phase} complete", "INFO")

    def _consolidate_to_episodic(self):
        """
        Consolidate working memory items to episodic memory.
        Uses HyperMorphic importance thresholding.
        """
        items_consolidated = 0

        for item in self.working_memory:
            if item["importance"] >= self.retention_threshold:
                # Create episodic memory entry
                memory_id = item["id"]

                # Structure episodic memory with rich metadata
                episodic_memory = {
                    "content": item["content"],
                    "timestamp": item["timestamp"],
                    "importance": item["importance"],
                    "context": self._extract_context(item["content"]),
                    "last_accessed": item["timestamp"],
                    "access_count": 1
                }

                # Store in episodic memory
                self.episodic_memory[memory_id] = episodic_memory

                # Update access counter
                self.memory_access_counts[memory_id] = 1

                items_consolidated += 1

        # Clear working memory items that were consolidated
        self.working_memory = []

        log_event(f"Consolidated {items_consolidated} items to episodic memory", "INFO")

    def _consolidate_to_semantic(self):
        """
        Extract semantic knowledge from episodic memories.
        Uses HyperMorphic pattern recognition.
        """
        # Get recent episodic memories
        recent_episodes = sorted(
            self.episodic_memory.items(),
            key=lambda x: x[1]["timestamp"],
            reverse=True
        )[:20]  # Last 20 episodes

        # Extract concepts and facts
        semantic_concepts = {}

        for memory_id, memory in recent_episodes:
            if "content" not in memory:
                continue

            content = memory["content"]
            if not isinstance(content, dict):
                continue

            # Extract domain information
            if "domain" in content:
                domain = content["domain"]
                if domain not in semantic_concepts:
                    semantic_concepts[domain] = {
                        "type": "domain",
                        "importance": self.hyper_math.zero_free(0.5),
                        "attributes": {},
                        "related_concepts": set(),
                        "source_memories": set([memory_id])
                    }
                else:
                    semantic_concepts[domain]["source_memories"].add(memory_id)
                    # Increase importance with HyperMorphic amplification
                    old_importance = semantic_concepts[domain]["importance"]
                    semantic_concepts[domain]["importance"] = self.hyper_math.min(
                        self.hyper_math.zero_free(0.9),
                        self.hyper_math.mul(old_importance, self.hyper_math.zero_free(1.1))
                    )

            # Extract strategy information
            if "strategy" in content:
                strategy = content["strategy"]
                if strategy not in semantic_concepts:
                    semantic_concepts[strategy] = {
                        "type": "strategy",
                        "importance": self.hyper_math.zero_free(0.6),
                        "attributes": {},
                        "related_concepts": set(),
                        "source_memories": set([memory_id])
                    }
                else:
                    semantic_concepts[strategy]["source_memories"].add(memory_id)

                # Link strategy to domain if both present
                if "domain" in content:
                    domain = content["domain"]
                    if domain in semantic_concepts and strategy in semantic_concepts:
                        semantic_concepts[domain]["related_concepts"].add(strategy)
                        semantic_concepts[strategy]["related_concepts"].add(domain)

        # Update semantic memory
        for concept, data in semantic_concepts.items():
            # Convert sets to lists for serialization
            data["related_concepts"] = list(data["related_concepts"])
            data["source_memories"] = list(data["source_memories"])

            if concept in self.semantic_memory:
                # Update existing concept
                self.semantic_memory[concept]["importance"] = data["importance"]
                self.semantic_memory[concept]["source_memories"].extend(
                    [m for m in data["source_memories"]
                     if m not in self.semantic_memory[concept]["source_memories"]]
                )
                for rel in data["related_concepts"]:
                    if rel not in self.semantic_memory[concept]["related_concepts"]:
                        self.semantic_memory[concept]["related_concepts"].append(rel)
            else:
                # Add new concept
                self.semantic_memory[concept] = data

        log_event(f"Updated {len(semantic_concepts)} semantic concepts from episodic memory", "INFO")

    def _consolidate_to_procedural(self):
        """
        Extract action patterns for procedural memory.
        Uses HyperMorphic sequence detection.
        """
        # Extract recent action sequences
        action_sequences = []

        for memory_id, memory in self.episodic_memory.items():
            if "content" not in memory:
                continue

            content = memory["content"]
            if not isinstance(content, dict):
                continue

            if "action" in content:
                action = content["action"]
                success = content.get("success", False)
                timestamp = memory["timestamp"]

                action_sequences.append({
                    "action": action,
                    "success": success,
                    "timestamp": timestamp,
                    "memory_id": memory_id
                })

        # Sort by timestamp
        action_sequences.sort(key=lambda x: x["timestamp"])

        # Find repeated action patterns
        if len(action_sequences) >= 3:
            # Look for sequences of 2-3 actions
            for seq_length in [2, 3]:
                if len(action_sequences) >= seq_length:
                    self._find_action_patterns(action_sequences, seq_length)

        log_event(f"Updated procedural memory with action patterns", "INFO")

    def _find_action_patterns(self, action_sequences, pattern_length):
        """
        Find repeated action patterns of specified length.

        Args:
            action_sequences: List of action sequence dictionaries
            pattern_length: Length of patterns to search for
        """
        pattern_counts = {}

        # Generate n-grams of actions
        for i in range(len(action_sequences) - pattern_length + 1):
            actions = tuple(seq["action"] for seq in action_sequences[i:i+pattern_length])
            success_ratio = sum(1 for seq in action_sequences[i:i+pattern_length] if seq["success"]) / pattern_length

            if actions in pattern_counts:
                pattern_counts[actions]["count"] += 1
                pattern_counts[actions]["success_ratio"] = (pattern_counts[actions]["success_ratio"] + success_ratio) / 2
                pattern_counts[actions]["instances"].append(i)
            else:
                pattern_counts[actions] = {
                    "count": 1,
                    "success_ratio": success_ratio,
                    "instances": [i]
                }

        # Find significant patterns (appearing multiple times with good success)
        significant_patterns = {}
        for pattern, data in pattern_counts.items():
            if data["count"] >= 2 and data["success_ratio"] > 0.5:
                # Calculate importance with HyperMorphic formula
                importance = self.hyper_math.mul(
                    self.hyper_math.zero_free(data["success_ratio"]),
                    self.hyper_math.add(
                        self.hyper_math.zero_free(0.5),
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.zero_free(data["count"])
                        )
                    )
                )

                pattern_name = "-".join(pattern)
                significant_patterns[pattern_name] = {
                    "actions": pattern,
                    "count": data["count"],
                    "success_ratio": data["success_ratio"],
                    "importance": importance,
                    "recent_instance": max(data["instances"])
                }

        # Update procedural memory
        for pattern_name, pattern_data in significant_patterns.items():
            if pattern_name in self.procedural_memory:
                # Update existing pattern
                self.procedural_memory[pattern_name]["count"] += pattern_data["count"]
                self.procedural_memory[pattern_name]["success_ratio"] = (
                    self.procedural_memory[pattern_name]["success_ratio"] + pattern_data["success_ratio"]
                ) / 2
                self.procedural_memory[pattern_name]["importance"] = pattern_data["importance"]
                self.procedural_memory[pattern_name]["recent_instance"] = pattern_data["recent_instance"]
            else:
                # Add new pattern
                self.procedural_memory[pattern_name] = pattern_data

    def _prune_memories(self):
        """
        Remove low-importance memories to maintain system efficiency.
        Uses HyperMorphic importance thresholding.
        """
        # 1. Prune episodic memories
        episodic_count = len(self.episodic_memory)

        if episodic_count > 100:  # Keep episodic memory limited
            # Find low importance memories
            low_importance = []

            for memory_id, memory in self.episodic_memory.items():
                if "importance" in memory and memory["importance"] < self.retention_threshold:
                    low_importance.append(memory_id)

            # Sort by importance (ascending)
            low_importance.sort(
                key=lambda x: self.episodic_memory[x].get("importance", 0)
            )

            # Remove up to 20% of memories
            prune_count = min(len(low_importance), episodic_count // 5)

            for memory_id in low_importance[:prune_count]:
                self.episodic_memory.pop(memory_id, None)
                self.memory_access_counts.pop(memory_id, None)

        # 2. Prune semantic memories - less aggressive
        semantic_count = len(self.semantic_memory)

        if semantic_count > 50:  # Keep semantic memory limited
            # Find low importance concepts
            low_importance = []

            for concept, concept_data in self.semantic_memory.items():
                if concept_data["importance"] < self.hyper_math.zero_free(0.3):
                    low_importance.append(concept)

            # Sort by importance (ascending)
            low_importance.sort(
                key=lambda x: self.semantic_memory[x]["importance"]
            )

            # Remove up to 10% of concepts
            prune_count = min(len(low_importance), semantic_count // 10)

            for concept in low_importance[:prune_count]:
                self.semantic_memory.pop(concept, None)

        # 3. Prune procedural memories - only the lowest importance
        proc_count = len(self.procedural_memory)

        if proc_count > 30:  # Keep procedural memory limited
            # Find low importance patterns
            low_importance = []

            for pattern, pattern_data in self.procedural_memory.items():
                if pattern_data["importance"] < self.hyper_math.zero_free(0.3):
                    low_importance.append(pattern)

            # Sort by importance (ascending)
            low_importance.sort(
                key=lambda x: self.procedural_memory[x]["importance"]
            )

            # Remove up to 5 patterns
            prune_count = min(len(low_importance), 5)

            for pattern in low_importance[:prune_count]:
                self.procedural_memory.pop(pattern, None)

        log_event(f"Memory pruning complete. Episodic: {len(self.episodic_memory)}, Semantic: {len(self.semantic_memory)}, Procedural: {len(self.procedural_memory)}", "INFO")

    def _reinforce_connections(self):
        """
        Reinforce connections between related memories.
        Implements HyperMorphic connection strengthening.
        """
        # 1. Find related semantic concepts
        for concept1 in self.semantic_memory:
            for concept2 in self.semantic_memory:
                if concept1 != concept2:
                    # Check for shared source memories
                    source1 = set(self.semantic_memory[concept1]["source_memories"])
                    source2 = set(self.semantic_memory[concept2]["source_memories"])

                    shared = source1.intersection(source2)

                    if shared:
                        # Create bidirectional connection
                        if concept1 not in self.semantic_memory[concept2]["related_concepts"]:
                            self.semantic_memory[concept2]["related_concepts"].append(concept1)

                        if concept2 not in self.semantic_memory[concept1]["related_concepts"]:
                            self.semantic_memory[concept1]["related_concepts"].append(concept2)

        # 2. Connect procedural patterns to semantic concepts
        for pattern, pattern_data in self.procedural_memory.items():
            for concept, concept_data in self.semantic_memory.items():
                # If pattern contains concept name, create connection
                if concept in pattern or any(concept in action for action in pattern_data["actions"]):
                    # Add attribute to semantic concept
                    if "associated_patterns" not in concept_data["attributes"]:
                        concept_data["attributes"]["associated_patterns"] = []

                    if pattern not in concept_data["attributes"]["associated_patterns"]:
                        concept_data["attributes"]["associated_patterns"].append(pattern)

                    # Add attribute to procedural pattern
                    if "associated_concepts" not in pattern_data:
                        pattern_data["associated_concepts"] = []

                    if concept not in pattern_data["associated_concepts"]:
                        pattern_data["associated_concepts"].append(concept)

        log_event("Memory connections reinforced across memory systems", "INFO")

    def retrieve_memory(self, query, memory_type="all", limit=5):
        """
        Retrieve memories based on query across memory systems.
        Uses HyperMorphic similarity matching.

        Args:
            query: Search query (string or dict)
            memory_type: Type of memory to search ("episodic", "semantic", "procedural", or "all")
            limit: Maximum number of results to return

        Returns:
            List of matched memories
        """
        results = []
        start_time = time.time()

        # Convert query to string for comparison
        query_str = str(query).lower()

        # Search episodic memory
        if memory_type in ["episodic", "all"]:
            for memory_id, memory in self.episodic_memory.items():
                content_str = str(memory.get("content", "")).lower()
                context_str = str(memory.get("context", "")).lower()

                # Calculate match score with HyperMorphic operations
                content_match = self._calculate_string_similarity(query_str, content_str)
                context_match = self._calculate_string_similarity(query_str, context_str)

                # Combine scores with HyperMorphic weighting
                match_score = self.hyper_math.add(
                    self.hyper_math.mul(content_match, self.hyper_math.zero_free(0.7)),
                    self.hyper_math.mul(context_match, self.hyper_math.zero_free(0.3))
                )

                if match_score > self.hyper_math.zero_free(0.3):
                    results.append({
                        "memory_id": memory_id,
                        "type": "episodic",
                        "content": memory.get("content"),
                        "match_score": match_score,
                        "importance": memory.get("importance", self.hyper_math.zero_free(0.5))
                    })

                    # Update access statistics
                    self.memory_access_counts[memory_id] = self.memory_access_counts.get(memory_id, 0) + 1
                    self.memory_timestamps[memory_id] = time.time()

        # Search semantic memory
        if memory_type in ["semantic", "all"]:
            for concept, concept_data in self.semantic_memory.items():
                concept_str = str(concept).lower()
                attributes_str = str(concept_data.get("attributes", {})).lower()

                # Calculate match score with HyperMorphic operations
                concept_match = self._calculate_string_similarity(query_str, concept_str)
                attributes_match = self._calculate_string_similarity(query_str, attributes_str)

                # Combine scores with HyperMorphic weighting
                match_score = self.hyper_math.add(
                    self.hyper_math.mul(concept_match, self.hyper_math.zero_free(0.8)),
                    self.hyper_math.mul(attributes_match, self.hyper_math.zero_free(0.2))
                )

                if match_score > self.hyper_math.zero_free(0.3):
                    results.append({
                        "memory_id": concept,
                        "type": "semantic",
                        "content": {
                            "concept": concept,
                            "type": concept_data.get("type"),
                            "attributes": concept_data.get("attributes"),
                            "related_concepts": concept_data.get("related_concepts")
                        },
                        "match_score": match_score,
                        "importance": concept_data.get("importance", self.hyper_math.zero_free(0.5))
                    })

        # Search procedural memory
        if memory_type in ["procedural", "all"]:
            for pattern, pattern_data in self.procedural_memory.items():
                pattern_str = str(pattern).lower()
                actions_str = str(pattern_data.get("actions", ())).lower()

                # Calculate match score with HyperMorphic operations
                pattern_match = self._calculate_string_similarity(query_str, pattern_str)
                actions_match = self._calculate_string_similarity(query_str, actions_str)

                # Combine scores with HyperMorphic weighting
                match_score = self.hyper_math.add(
                    self.hyper_math.mul(pattern_match, self.hyper_math.zero_free(0.6)),
                    self.hyper_math.mul(actions_match, self.hyper_math.zero_free(0.4))
                )

                if match_score > self.hyper_math.zero_free(0.3):
                    results.append({
                        "memory_id": pattern,
                        "type": "procedural",
                        "content": {
                            "pattern": pattern,
                            "actions": pattern_data.get("actions"),
                            "success_ratio": pattern_data.get("success_ratio"),
                            "count": pattern_data.get("count")
                        },
                        "match_score": match_score,
                        "importance": pattern_data.get("importance", self.hyper_math.zero_free(0.5))
                    })

        # Sort results by combined score (match_score * importance)
        for result in results:
            result["combined_score"] = self.hyper_math.mul(
                result["match_score"],
                result["importance"]
            )

        results.sort(key=lambda x: x["combined_score"], reverse=True)

        # Update retrieval stats
        end_time = time.time()
        retrieval_time = end_time - start_time

        self.retrieval_stats["hits"] += 1 if results else 0
        self.retrieval_stats["misses"] += 0 if results else 1

        # Update average retrieval time with running average
        old_avg = self.retrieval_stats["avg_retrieval_time"]
        total_retrievals = self.retrieval_stats["hits"] + self.retrieval_stats["misses"]
        self.retrieval_stats["avg_retrieval_time"] = (old_avg * (total_retrievals - 1) + retrieval_time) / total_retrievals

        return results[:limit]

    def _calculate_string_similarity(self, str1, str2):
        """
        Calculate string similarity with HyperMorphic zero-free operations.
        Uses a hybrid of overlap and Jaccard similarity.

        Args:
            str1: First string
            str2: Second string

        Returns:
            Similarity score (0.0-1.0) as HyperMorphic zero-free value
        """
        # Tokenize strings
        tokens1 = set(str1.split())
        tokens2 = set(str2.split())

        # Avoid empty sets with HyperMorphic guarantee
        if not tokens1 or not tokens2:
            return self.hyper_math.zero_free(0.1)  # Minimal similarity

        # Calculate intersection and union with HyperMorphic counting
        intersection = tokens1.intersection(tokens2)
        union = tokens1.union(tokens2)

        # Calculate Jaccard similarity with HyperMorphic zero-free division
        intersection_count = self.hyper_math.zero_free(len(intersection))
        union_count = self.hyper_math.zero_free(len(union))

        jaccard = self.hyper_math.div(intersection_count, union_count)

        # Calculate overlap coefficient with HyperMorphic zero-free division
        min_set_size = self.hyper_math.zero_free(min(len(tokens1), len(tokens2)))

        if min_set_size > self.hyper_math.epsilon:
            overlap = self.hyper_math.div(intersection_count, min_set_size)
        else:
            overlap = self.hyper_math.zero_free(0.0)

        # Combine with HyperMorphic weighted average
        similarity = self.hyper_math.add(
            self.hyper_math.mul(jaccard, self.hyper_math.zero_free(0.4)),
            self.hyper_math.mul(overlap, self.hyper_math.zero_free(0.6))
        )

        return similarity

    def _extract_context(self, content):
        """
        Extract contextual information from content.

        Args:
            content: Content dictionary to extract context from

        Returns:
            Dictionary of contextual information
        """
        context = {}

        if not content or not isinstance(content, dict):
            return context

        # Extract key contextual elements
        if "domain" in content:
            context["domain"] = content["domain"]

        if "strategy" in content:
            context["strategy"] = content["strategy"]

        if "goal" in content:
            context["goal"] = content["goal"]

        if "action" in content:
            context["action"] = content["action"]

        if "thinking_mode" in content:
            context["thinking_mode"] = content["thinking_mode"]

        if "current_goal" in content and isinstance(content["current_goal"], dict):
            context["goal_description"] = content["current_goal"].get("description", "")
            context["goal_type"] = content["current_goal"].get("type", "")

        return context

    def get_memory_statistics(self):
        """
        Get comprehensive statistics about the memory system.

        Returns:
            Dictionary containing memory statistics
        """
        stats = {
            "memory_counts": {
                "sensory": len(self.sensory_buffer),
                "working": len(self.working_memory),
                "episodic": len(self.episodic_memory),
                "semantic": len(self.semantic_memory),
                "procedural": len(self.procedural_memory)
            },
            "consolidation": {
                "last_consolidation": self.last_consolidation,
                "current_phase": self.consolidation_phase
            },
            "retrieval": self.retrieval_stats.copy(),
            "top_concepts": self._get_top_semantic_concepts(5),
            "top_patterns": self._get_top_procedural_patterns(5)
        }

        return stats

    def _get_top_semantic_concepts(self, limit=5):
        """
        Get the top semantic concepts by importance.

        Args:
            limit: Maximum number of concepts to return

        Returns:
            List of top semantic concepts with metadata
        """
        if not self.semantic_memory:
            return []

        concepts = []

        for concept, data in self.semantic_memory.items():
            concepts.append({
                "concept": concept,
                "type": data.get("type", "unknown"),
                "importance": data.get("importance", self.hyper_math.zero_free(0.5)),
                "related_count": len(data.get("related_concepts", []))
            })

        # Sort by importance (high to low)
        concepts.sort(key=lambda x: x["importance"], reverse=True)

        return concepts[:limit]

    def _get_top_procedural_patterns(self, limit=5):
        """
        Get the top procedural patterns by importance.

        Args:
            limit: Maximum number of patterns to return

        Returns:
            List of top procedural patterns with metadata
        """
        if not self.procedural_memory:
            return []

        patterns = []

        for pattern, data in self.procedural_memory.items():
            patterns.append({
                "pattern": pattern,
                "actions": data.get("actions", ()),
                "importance": data.get("importance", self.hyper_math.zero_free(0.5)),
                "success_ratio": data.get("success_ratio", 0.0),
                "count": data.get("count", 0)
            })

        # Sort by importance (high to low)
        patterns.sort(key=lambda x: x["importance"], reverse=True)

        return patterns[:limit]

    def get_memory_graph(self):
        """
        Generate a graph representation of memory connections.

        Returns:
            Dictionary containing nodes and edges for visualization
        """
        nodes = []
        edges = []

        # Add semantic concept nodes
        for concept, data in self.semantic_memory.items():
            nodes.append({
                "id": concept,
                "type": "semantic",
                "subtype": data.get("type", "concept"),
                "importance": float(data.get("importance", 0.5))  # Convert to float for serialization
            })

            # Add edges for related concepts
            for related in data.get("related_concepts", []):
                if related in self.semantic_memory:  # Ensure target exists
                    edges.append({
                        "source": concept,
                        "target": related,
                        "type": "semantic_relation"
                    })

        # Add procedural pattern nodes
        for pattern, data in self.procedural_memory.items():
            nodes.append({
                "id": pattern,
                "type": "procedural",
                "importance": float(data.get("importance", 0.5)),  # Convert to float for serialization
                "success_ratio": data.get("success_ratio", 0.0)
            })

            # Add edges for associated concepts
            for concept in data.get("associated_concepts", []):
                if concept in self.semantic_memory:  # Ensure target exists
                    edges.append({
                        "source": pattern,
                        "target": concept,
                        "type": "pattern_concept_relation"
                    })

        return {
            "nodes": nodes,
            "edges": edges
        }

    def recommend_action_patterns(self, context, limit=3):
        """
        Recommend action patterns based on current context.
        Uses HyperMorphic similarity matching.

        Args:
            context: Current context dictionary
            limit: Maximum number of recommendations

        Returns:
            List of recommended action patterns
        """
        if not context or not self.procedural_memory:
            return []

        # Extract key context elements
        context_elements = self._extract_context(context)
        context_str = str(context_elements).lower()

        recommendations = []

        for pattern, data in self.procedural_memory.items():
            # Only consider patterns with good success ratio
            if data.get("success_ratio", 0) < 0.5:
                continue

            # Calculate context match
            associated_concepts = data.get("associated_concepts", [])

            # Check if any associated concepts are in context
            concept_match = False
            for concept in associated_concepts:
                if concept.lower() in context_str:
                    concept_match = True
                    break

            # Calculate pattern relevance with HyperMorphic operations
            base_relevance = self.hyper_math.zero_free(0.3)

            if concept_match:
                concept_boost = self.hyper_math.zero_free(0.3)
                base_relevance = self.hyper_math.add(base_relevance, concept_boost)

            # Adjust by success ratio with HyperMorphic operations
            success_ratio = self.hyper_math.zero_free(data.get("success_ratio", 0.5))
            success_weight = self.hyper_math.zero_free(0.4)

            relevance = self.hyper_math.add(
                base_relevance,
                self.hyper_math.mul(success_ratio, success_weight)
            )

            # Add recommendation if relevant enough
            if relevance > self.hyper_math.zero_free(0.4):
                recommendations.append({
                    "pattern": pattern,
                    "actions": data.get("actions", ()),
                    "relevance": relevance,
                    "success_ratio": data.get("success_ratio", 0.5),
                    "count": data.get("count", 0)
                })

        # Sort by relevance (high to low)
        recommendations.sort(key=lambda x: x["relevance"], reverse=True)

        return recommendations[:limit]

    def integrate_with_agent_systems(self):
        """
        Integrate memory consolidation with other agent systems.

        Returns:
            Boolean indicating success of integration
        """
        try:
            # 1. Connect with free will
            if hasattr(self.agent, 'free_will'):
                # Extend memory decision making
                old_decide = self.agent.free_will.decide

                def enhanced_decide(*args, **kwargs):
                    # Get base decision
                    base_decision = old_decide(*args, **kwargs)

                    # Check for relevant procedural patterns
                    if hasattr(self.agent, 'stats'):
                        context = self.agent.stats.copy()
                        recommendations = self.recommend_action_patterns(context)

                        if recommendations and random.random() < 0.3:  # 30% chance to use recommendation
                            top_rec = recommendations[0]
                            if isinstance(top_rec["actions"], tuple) and len(top_rec["actions"]) > 0:
                                recommended_action = top_rec["actions"][0]
                                # Modify decision to use recommended action
                                base_decision["action"] = recommended_action
                                base_decision["reasoning"] = f"Using procedural memory pattern '{top_rec['pattern']}' (success ratio: {top_rec['success_ratio']:.2f})"

                    return base_decision

                # Replace decision method
                setattr(self.agent.free_will, 'decide', enhanced_decide)
                log_event("Enhanced agent free will with procedural memory integration", "INFO")

            # 2. Connect with consciousness
            if hasattr(self.agent, 'ai_manager') and hasattr(self.agent.ai_manager, 'consciousness'):
                consciousness = self.agent.ai_manager.consciousness

                # Extend reflection with semantic memory
                old_reflect = consciousness.reflect

                def enhanced_reflect(observation):
                    # Call original reflection
                    result = old_reflect(observation)

                    # Incorporate semantic knowledge
                    if observation and isinstance(observation, dict):
                        # Get relevant semantic concepts
                        query = str(observation)
                        semantic_memories = self.retrieve_memory(query, memory_type="semantic", limit=3)

                        if semantic_memories:
                            # Use first memory to influence thinking mode
                            memory = semantic_memories[0]
                            if isinstance(memory["content"], dict) and "type" in memory["content"]:
                                concept_type = memory["content"]["type"]

                                # Map concept types to consciousness modes
                                mode_mapping = {
                                    "domain": "exploratory",
                                    "strategy": "analytical",
                                    "error": "critical",
                                    "pattern": "intuitive"
                                }

                                if concept_type in mode_mapping and random.random() < 0.2:
                                    suggested_mode = mode_mapping[concept_type]

                                    # Increase activation for this mode
                                    if hasattr(consciousness, 'cognitive_states') and suggested_mode in consciousness.cognitive_states:
                                        old_activation = consciousness.cognitive_states[suggested_mode]["activation"]
                                        consciousness.cognitive_states[suggested_mode]["activation"] = min(0.9, old_activation + 0.2)

                    return result

                # Replace reflect method
                setattr(consciousness, 'reflect', enhanced_reflect)
                log_event("Enhanced agent consciousness with semantic memory integration", "INFO")

            # 3. Connect with temporal planner
            if hasattr(self.agent, 'ai_manager') and hasattr(self.agent.ai_manager, 'temporal_planner'):
                planner = self.agent.ai_manager.temporal_planner

                # Extend plan action with procedural memory
                old_plan_action = planner.plan_action

                def enhanced_plan_action(base_action, environment_state=None):
                    # Get base plan
                    base_plan = old_plan_action(base_action, environment_state)

                    # Enhance with procedural memory if available
                    if environment_state and isinstance(environment_state, dict):
                        recommendations = self.recommend_action_patterns(environment_state)

                        if recommendations and random.random() < 0.25:  # 25% chance to use
                            top_rec = recommendations[0]
                            if "pattern" in top_rec:
                                # Add pattern information to plan
                                base_plan["procedural_pattern"] = top_rec["pattern"]
                                base_plan["pattern_success_ratio"] = top_rec["success_ratio"]

                    return base_plan

                # Replace plan action method
                setattr(planner, 'plan_action', enhanced_plan_action)
                log_event("Enhanced agent temporal planner with procedural memory integration", "INFO")

            return True

        except Exception as e:
            log_event(f"Error integrating memory consolidation with agent systems: {e}", "ERROR")
            return False




# =============================================================================
# QUANTUM STRATEGY SYNTHESIZER - ADDED HERE
# =============================================================================
import json  # Import for potential state saving/loading (if needed later)

class QuantumStrategySynthesizer(nn.Module):
    """
    Quantum Strategy Synthesizer

    Integrates all advanced neural components to create a unified quantum
    cognitive architecture that leverages entanglement, superposition,
    holographic memory and zero-point energy modulation.
    """
    def __init__(self, input_dim, output_dim, embedding_dim=512, num_layers=128, epsilon=1e-12):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.embedding_dim = embedding_dim
        self.epsilon = epsilon

        # Initialize HyperMorphic Math utility if available
        if 'HyperMorphicMath' in globals():
            self.hyper_math = HyperMorphicMath(
                dynamic_base=float(embedding_dim),
                dynamic_modulus=max(997, embedding_dim * 2 - 1),
                epsilon=epsilon
            )
        else:
            self.hyper_math = None

        # Input encoding with Dynamic Qubit Neurons
        self.input_encoder = DynamicQubitNeuron(
            input_dim,
            embedding_dim,
            num_basis_states=4,
            epsilon=epsilon
        )

        # Quantum processing stack with HyperMorphic Quantum Entanglement layers
        self.quantum_layers = nn.ModuleList()
        for _ in range(num_layers):
            self.quantum_layers.append(
                HyperMorphicQuantumEntanglementLayer(
                    embedding_dim,
                    num_entangled_units=4,
                    epsilon=epsilon
                )
            )

        # Holographic memory system
        self.holographic_memory = HyperMorphicQuantumHolographicMemory(
            embedding_dim,
            memory_dim=1024,
            num_memory_planes=8,
            epsilon=epsilon
        )

        # Zero-point energy modulator
        self.energy_modulator = ZeroPointEnergyModulator(
            embedding_dim,
            num_energy_states=5,
            epsilon=epsilon
        )

        # Output decoder with Dynamic Qubit Neurons
        self.output_decoder = DynamicQubitNeuron(
            embedding_dim,
            output_dim,
            num_basis_states=4,
            epsilon=epsilon
        )

        # Integration layer
        self.integration_layer = nn.Linear(embedding_dim * 2, embedding_dim)

        # Attention-based strategy mixer
        self.strategy_attention = nn.MultiheadAttention(embedding_dim, num_heads=4, dropout=0.1)

        # Strategy coherence parameters (how different strategies interact)
        self.strategy_coherence = nn.Parameter(torch.rand(1) * 0.5 + 0.5)

        # Tracking statistics
        self.memory_access_count = 0
        self.energy_modulation_history = []
        self.register_buffer('strategy_history', torch.zeros(10, embedding_dim))
        self.register_buffer('history_index', torch.tensor(0))

        log_event(f"QuantumStrategySynthesizer initialized with {num_layers} quantum layers", "QUANTUM")

    def forward(self, x, store_memory=False, external_reward=None):
        """
        Process input through the quantum cognitive architecture.

        Args:
            x: Input tensor
            store_memory: Whether to store the state in holographic memory
            external_reward: Optional reward signal for zero-point energy learning

        Returns:
            Output tensor, strategy embedding
        """
        batch_size = x.shape[0]

        # 1. Encode input with dynamic qubit neurons
        quantum_state = self.input_encoder(x)

        # 2. Process through quantum entanglement layers
        for layer in self.quantum_layers:
            quantum_state = layer(quantum_state)

        # 3. Query holographic memory
        memory_state = self.holographic_memory.recall(quantum_state)

        # 4. Integrate memory with current state
        combined_state = torch.cat([quantum_state, memory_state], dim=-1)
        integrated_state = self.integration_layer(combined_state)

        # 5. Apply zero-point energy modulation
        modulated_state = self.energy_modulator.modulate(integrated_state, external_reward)

        # 6. Generate strategy embedding with attention
        # First reshape if needed (for attention mechanism)
        if len(modulated_state.shape) == 2:
            # Add sequence dimension if missing
            modulated_state = modulated_state.unsqueeze(0)

        # Apply self-attention to refine strategy
        attn_output, _ = self.strategy_attention(
            modulated_state, modulated_state, modulated_state
        )

        # Apply strategy coherence
        coherence = torch.sigmoid(self.strategy_coherence)
        strategy_embedding = coherence * attn_output + (1 - coherence) * modulated_state

        # Remove sequence dimension if it was added
        if len(strategy_embedding.shape) == 3 and strategy_embedding.shape[0] == 1:
            strategy_embedding = strategy_embedding.squeeze(0)

        # 7. Decode to output space
        output = self.output_decoder(strategy_embedding)

        # 8. Store in memory if requested
        if store_memory:
            importance = torch.mean(torch.abs(strategy_embedding)).item()
            self.holographic_memory.store(strategy_embedding, importance=importance)
            self.memory_access_count += 1

        # 9. Store in strategy history
        idx = (self.history_index % 10).item()
        with torch.no_grad():
            self.strategy_history[idx] = strategy_embedding.detach().mean(dim=0) if len(strategy_embedding.shape) > 1 else strategy_embedding.detach()
        self.history_index += 1

        # 10. Record energy modulation
        self.energy_modulation_history.append(self.energy_modulator.current_energy_state.detach().cpu().numpy())
        if len(self.energy_modulation_history) > 100:
            self.energy_modulation_history = self.energy_modulation_history[-100:]

        # Ensure zero-free output if HyperMorphicMath is available
        if self.hyper_math:
            output = self._ensure_zero_free(output)

        return output, strategy_embedding

    def update_dynamic_components(self):
        """
        Update dynamic components of the system:
        - Dynamic basis adaptation
        - Memory cleanup
        - Energy parameter adjustment
        """
        # Update dynamic qubit neuron basis sets
        self.input_encoder.update_basis()
        self.output_decoder.update_basis()

        # Return status
        return {
            "memory_access_count": self.memory_access_count,
            "energy_state": self.energy_modulator.current_energy_state.detach().cpu().numpy(),
            "basis_importance": {
                "input": self.input_encoder.basis_importance.detach().cpu().numpy(),
                "output": self.output_decoder.basis_importance.detach().cpu().numpy()
            },
            "strategy_coherence": torch.sigmoid(self.strategy_coherence).item()
        }

    def get_strategy_analysis(self):
        """
        Analyze recent strategies for patterns and emergent properties
        """
        if self.history_index < 5:
            return {"status": "insufficient_history"}

        # Calculate strategy diversity
        recent_strategies = self.strategy_history[:min(10, self.history_index)]
        avg_strategy = torch.mean(recent_strategies, dim=0)
        strategy_variance = torch.mean(torch.var(recent_strategies, dim=0))

        # Calculate strategy coherence
        strategy_distances = []
        for i in range(1, len(recent_strategies)):
            dist = torch.norm(recent_strategies[i] - recent_strategies[i-1])
            strategy_distances.append(dist.item())

        avg_distance = sum(strategy_distances) / max(1, len(strategy_distances))

        # Analyze energy modulation patterns
        if len(self.energy_modulation_history) > 5:
            recent_energy = np.array(self.energy_modulation_history[-5:])
            dominant_energy = np.argmax(np.mean(recent_energy, axis=0))
        else:
            dominant_energy = None

        return {
            "strategy_variance": strategy_variance.item(),
            "strategy_coherence": 1.0 / (1.0 + avg_distance),
            "dominant_energy_state": dominant_energy,
            "recent_strategy_changes": strategy_distances
        }

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free"""
        if not self.hyper_math:
            return tensor

        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.epsilon

        return tensor

    def _initialize_quantum_states(self):
        """
        Initialize quantum states from existing strategies.
        Creates amplitude, phase and entanglement for each strategy.
        """
        if hasattr(self.agent, 'planner_sifter') and hasattr(self.agent.planner_sifter, 'strategies'):
            strategies = self.agent.planner_sifter.strategies

            # Create initial quantum states for each strategy
            for strategy_name, strategy_data in strategies.items():
                # Set initial amplitude based on effectiveness
                effectiveness = self.hyper_math.zero_free(strategy_data.get("effectiveness", 0.5))
                amplitude = self.hyper_math.add(
                    self.hyper_math.zero_free(0.5),
                    self.hyper_math.mul(effectiveness, self.hyper_math.zero_free(0.5))
                )
                self.strategy_amplitudes[strategy_name] = amplitude

                # Set initial phase (0 to 2π) with quantum randomness
                self.strategy_phases[strategy_name] = random.uniform(0, 2 * math.pi)

                # Set initial quantum state representation
                self.strategy_states[strategy_name] = {
                    "amplitude": amplitude,
                    "phase": self.strategy_phases[strategy_name],
                    "description": strategy_data.get("description", ""),
                    "actions": strategy_data.get("actions", []),
                    "suitable_for": strategy_data.get("suitable_for", []),
                    "coherence": self.hyper_math.zero_free(0.5)  # Initial coherence
                }

            # Initialize entanglement matrix
            self._initialize_entanglement()

            log_event(f"Initialized quantum states for {len(strategies)} strategies", "QUANTUM")


    def _initialize_entanglement(self):
        """
        Initialize entanglement between strategy quantum states.
        Strategies with similar actions or contexts are more entangled.
        """
        strategies = list(self.strategy_states.keys())

        for i, strategy1 in enumerate(strategies):
            self.entanglement_matrix[strategy1] = {}
            state1 = self.strategy_states[strategy1]

            for j, strategy2 in enumerate(strategies):
                if i != j:
                    state2 = self.strategy_states[strategy2]

                    # Calculate similarity between strategies
                    actions_similarity = self._calculate_list_similarity(
                        state1.get("actions", []),
                        state2.get("actions", [])
                    )

                    context_similarity = self._calculate_list_similarity(
                        state1.get("suitable_for", []),
                        state2.get("suitable_for", [])
                    )

                    # Combined similarity with HyperMorphic weighted average
                    similarity = self.hyper_math.add(
                        self.hyper_math.mul(actions_similarity, self.hyper_math.zero_free(0.6)),
                        self.hyper_math.mul(context_similarity, self.hyper_math.zero_free(0.4))
                    )

                    # Set entanglement strength based on similarity
                    # More similar strategies are more entangled
                    entanglement = self.hyper_math.add(
                        self.hyper_math.zero_free(0.2),  # Base entanglement
                        self.hyper_math.mul(similarity, self.hyper_math.zero_free(0.8))
                    )

                    # Add quantum phase factor to entanglement
                    phase_diff = abs(self.strategy_phases[strategy1] - self.strategy_phases[strategy2])
                    phase_factor = abs(math.cos(phase_diff))

                    entanglement = self.hyper_math.mul(
                        entanglement,
                        self.hyper_math.zero_free(phase_factor)
                    )

                    self.entanglement_matrix[strategy1][strategy2] = entanglement

        log_event("Strategy quantum entanglement matrix initialized", "QUANTUM")


    def _calculate_list_similarity(self, list1, list2):
        """
        Calculate similarity between two lists using HyperMorphic operations.

        Args:
            list1: First list
            list2: Second list

        Returns:
            Similarity score (0.0-1.0) as HyperMorphic zero-free value
        """
        if not list1 or not list2:
            return self.hyper_math.zero_free(0.1)  # Minimal similarity

        # Convert to sets for intersection and union
        set1 = set(list1)
        set2 = set(list2)

        # Calculate intersection and union with HyperMorphic counting
        intersection = set1.intersection(set2)
        union = set1.union(set2)

        # Calculate Jaccard similarity with HyperMorphic zero-free division
        intersection_count = self.hyper_math.zero_free(len(intersection))
        union_count = self.hyper_math.zero_free(len(union))

        similarity = self.hyper_math.div(intersection_count, union_count)

        return similarity

    def evolve_quantum_states(self, cycle_count):
        """
        Evolve quantum states for all strategies based on cycle count.
        Updates amplitudes and phases based on strategy performance.

        Args:
            cycle_count: Current agent cycle count
        """
        # Only evolve quantum states periodically
        if hasattr(self.agent, 'planner_sifter') and hasattr(self.agent.planner_sifter, 'strategies'):
            strategies = self.agent.planner_sifter.strategies

            for strategy_name, strategy_data in strategies.items():
                if strategy_name in self.strategy_states:
                    # Update amplitude based on effectiveness
                    old_amplitude = self.strategy_amplitudes.get(strategy_name, self.hyper_math.zero_free(0.5))
                    effectiveness = self.hyper_math.zero_free(strategy_data.get("effectiveness", 0.5))

                    # Smooth transition with HyperMorphic weighting
                    new_amplitude = self.hyper_math.add(
                        self.hyper_math.mul(old_amplitude, self.hyper_math.zero_free(0.7)),
                        self.hyper_math.mul(effectiveness, self.hyper_math.zero_free(0.3))
                    )

                    self.strategy_amplitudes[strategy_name] = new_amplitude

                    # Update phase based on cycle count and evolution rate
                    old_phase = self.strategy_phases.get(strategy_name, 0)
                    phase_shift = self.hyper_math.mul(
                        self.evolution_rate,
                        self.hyper_math.zero_free(math.sin(cycle_count / 10))
                    )

                    # Use HyperMorphic phase evolution
                    new_phase = (old_phase + float(phase_shift)) % (2 * math.pi)
                    self.strategy_phases[strategy_name] = new_phase

                    # Update quantum state
                    self.strategy_states[strategy_name]["amplitude"] = new_amplitude
                    self.strategy_states[strategy_name]["phase"] = new_phase

                    # Update coherence based on strategy usage and success
                    if hasattr(self.agent.planner_sifter, 'strategy_usage'):
                        usage_count = self.agent.planner_sifter.strategy_usage.get(strategy_name, 0)
                        old_coherence = self.strategy_states[strategy_name]["coherence"]

                        if usage_count > 0:
                            # Increase coherence with usage
                            coherence_boost = self.hyper_math.zero_free(min(0.1, usage_count / 20))
                            new_coherence = self.hyper_math.add(old_coherence, coherence_boost)
                            new_coherence = self.hyper_math.min(self.hyper_math.zero_free(0.9), new_coherence)
                            self.strategy_states[strategy_name]["coherence"] = new_coherence

            # Update entanglement matrix
            self._update_entanglement()

            log_event(f"Evolved quantum states for {len(strategies)} strategies", "QUANTUM")


    def _update_entanglement(self):
        """
        Update entanglement between strategy quantum states.
        Entanglement evolves based on strategy usage patterns.
        """
        strategies = list(self.strategy_states.keys())

        for i, strategy1 in enumerate(strategies):
            for j, strategy2 in enumerate(strategies):
                if i != j and strategy1 in self.entanglement_matrix and strategy2 in self.entanglement_matrix[strategy1]:
                    # Get current entanglement
                    old_entanglement = self.entanglement_matrix[strategy1][strategy2]

                    # Check for consecutive usage pattern
                    consecutive_usage = False
                    if hasattr(self.agent, 'action_log') and len(self.agent.action_log) >= 2:
                        recent_actions = self.agent.action_log[-2:]
                        recent_strategies = [a.get("strategy", "") for a in recent_actions if isinstance(a, dict)]

                        if len(recent_strategies) >= 2 and set([strategy1, strategy2]).issubset(set(recent_strategies)):
                            consecutive_usage = True

                    # Update entanglement based on usage
                    if consecutive_usage:
                        # Increase entanglement for consecutively used strategies
                        entanglement_boost = self.hyper_math.zero_free(0.05)
                        new_entanglement = self.hyper_math.add(old_entanglement, entanglement_boost)
                        new_entanglement = self.hyper_math.min(self.hyper_math.zero_free(0.95), new_entanglement)
                        self.entanglement_matrix[strategy1][strategy2] = new_entanglement
                    else:
                        # Gradually decrease entanglement
                        decay = self.hyper_math.zero_free(0.01)
                        new_entanglement = self.hyper_math.max(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.sub(old_entanglement, decay)
                        )
                        self.entanglement_matrix[strategy1][strategy2] = new_entanglement


    def synthesize_strategies(self, cycle_count):
        """
        Synthesize new strategies by quantum superposition and interference.
        Creates novel strategy blends based on quantum states and entanglement.

        Args:
            cycle_count: Current agent cycle count

        Returns:
            Dictionary of newly synthesized strategies
        """
        # Only synthesize periodically
        synthesis_interval = 20
        if cycle_count - self.last_synthesis_cycle < synthesis_interval:
            return {}

        self.last_synthesis_cycle = cycle_count

        # Evolve quantum states first
        self.evolve_quantum_states(cycle_count)

        # Find highly entangled strategy pairs
        entangled_pairs = []
        strategies = list(self.strategy_states.keys())

        for i, strategy1 in enumerate(strategies):
            for j, strategy2 in enumerate(strategies):
                if i < j and strategy1 in self.entanglement_matrix and strategy2 in self.entanglement_matrix[strategy1]:
                    entanglement = self.entanglement_matrix[strategy1][strategy2]

                    # Consider only highly entangled pairs
                    if entanglement > self.hyper_math.zero_free(0.6):
                        entangled_pairs.append((strategy1, strategy2, float(entanglement)))

        # Sort by entanglement strength
        entangled_pairs.sort(key=lambda x: x[2], reverse=True)

        # Synthesize from top entangled pairs
        new_strategies = {}
        pairs_to_process = min(2, len(entangled_pairs))

        for i in range(pairs_to_process):
            strategy1, strategy2, entanglement = entangled_pairs[i]

            # Check if strategies have sufficient amplitude and coherence
            amplitude1 = self.strategy_states[strategy1]["amplitude"]
            amplitude2 = self.strategy_states[strategy2]["amplitude"]
            coherence1 = self.strategy_states[strategy1]["coherence"]
            coherence2 = self.strategy_states[strategy2]["coherence"]

            amplitude_product = self.hyper_math.mul(amplitude1, amplitude2)
            coherence_product = self.hyper_math.mul(coherence1, coherence2)

            if amplitude_product > self.hyper_math.zero_free(0.3) and coherence_product > self.coherence_threshold:
                # Generate quantum blend
                blended_strategy = self._quantum_blend_strategies(strategy1, strategy2, entanglement)

                if blended_strategy:
                    blend_name = f"{strategy1}_{strategy2}_blend"

                    # Check if this is a novel strategy
                    if blend_name not in self.synthesized_strategies:
                        new_strategies[blend_name] = blended_strategy
                        self.synthesized_strategies[blend_name] = blended_strategy

                        # Create strategy history entry
                        self.strategy_history[blend_name] = {
                            "created_at": cycle_count,
                            "parent_strategies": [strategy1, strategy2],
                            "entanglement": entanglement,
                            "amplitude_product": float(amplitude_product),
                            "coherence_product": float(coherence_product)
                        }

                        log_event(f"Synthesized new quantum strategy blend: {blend_name}", "QUANTUM")

        self.synthesis_count += len(new_strategies)

        return new_strategies


    def _quantum_blend_strategies(self, strategy1, strategy2, entanglement):
        """
        Blend two strategies using quantum superposition principles.

        Args:
            strategy1: Name of first strategy
            strategy2: Name of second strategy
            entanglement: Entanglement strength between strategies

        Returns:
            Blended strategy dictionary or None if blend fails
        """
        if strategy1 not in self.strategy_states or strategy2 not in self.strategy_states:
            return None

        state1 = self.strategy_states[strategy1]
        state2 = self.strategy_states[strategy2]

        amplitude1 = state1["amplitude"]
        amplitude2 = state2["amplitude"]
        phase1 = state1["phase"]
        phase2 = state2["phase"]

        # Calculate quantum interference based on phase difference
        phase_diff = phase1 - phase2
        interference = math.cos(phase_diff)

        # Calculate blending weights using amplitudes and interference
        interference_factor = self.hyper_math.zero_free(interference)

        # Constructive interference (positive cos) increases weight of both strategies
        # Destructive interference (negative cos) creates more contrast between strategies
        if interference >= 0:
            # Constructive interference - more balanced blend
            weight1 = self.hyper_math.mul(
                amplitude1,
                self.hyper_math.add(
                    self.hyper_math.zero_free(0.5),
                    self.hyper_math.mul(interference_factor, self.hyper_math.zero_free(0.5))
                )
            )

            weight2 = self.hyper_math.mul(
                amplitude2,
                self.hyper_math.add(
                    self.hyper_math.zero_free(0.5),
                    self.hyper_math.mul(interference_factor, self.hyper_math.zero_free(0.5))
                )
            )
        else:
            # Destructive interference - more contrasting blend
            weight1 = self.hyper_math.mul(
                amplitude1,
                self.hyper_math.add(
                    self.hyper_math.zero_free(0.5),
                    self.hyper_math.mul(self.hyper_math.zero_free(-interference), self.hyper_math.zero_free(0.5))
                )
            )

            weight2 = self.hyper_math.mul(
                amplitude2,
                self.hyper_math.sub(
                    self.hyper_math.zero_free(0.5),
                    self.hyper_math.mul(self.hyper_math.zero_free(-interference), self.hyper_math.zero_free(0.5))
                )
            )

        # Normalize weights
        sum_weights = self.hyper_math.add(weight1, weight2)
        norm_weight1 = self.hyper_math.div(weight1, sum_weights)
        norm_weight2 = self.hyper_math.div(weight2, sum_weights)

        # Blend strategy descriptions
        description1 = state1.get("description", "")
        description2 = state2.get("description", "")

        if random.random() < 0.5:
            description = f"Quantum blend: {description1} with {description2}"
        else:
            description = f"Quantum superposition of {strategy1} and {strategy2} strategies"

        # Blend actions with entanglement-weighted sampling
        actions1 = state1.get("actions", [])
        actions2 = state2.get("actions", [])

        blended_actions = []

        # Common actions appear with higher probability
        common_actions = list(set(actions1).intersection(set(actions2)))
        for action in common_actions:
            blended_actions.append(action)

        # Add unique actions based on weights
        unique_actions1 = list(set(actions1) - set(actions2))
        unique_actions2 = list(set(actions2) - set(actions1))

        # Sample from unique actions based on weights
        for action in unique_actions1:
            if random.random() < float(norm_weight1) * float(entanglement):
                blended_actions.append(action)

        for action in unique_actions2:
            if random.random() < float(norm_weight2) * float(entanglement):
                blended_actions.append(action)

        # Ensure we have at least 2 actions
        if len(blended_actions) < 2:
            # Add highest weight action if missing
            if norm_weight1 > norm_weight2 and actions1 and actions1[0] not in blended_actions:
                blended_actions.append(actions1[0])
            elif actions2 and actions2[0] not in blended_actions:
                blended_actions.append(actions2[0])

        # Blend suitable contexts with entanglement-weighted sampling
        suitable_for1 = state1.get("suitable_for", [])
        suitable_for2 = state2.get("suitable_for", [])

        blended_suitable = []

        # Common contexts appear with higher probability
        common_suitable = list(set(suitable_for1).intersection(set(suitable_for2)))
        for context in common_suitable:
            blended_suitable.append(context)

        # Add unique contexts based on weights
        unique_suitable1 = list(set(suitable_for1) - set(suitable_for2))
        unique_suitable2 = list(set(suitable_for2) - set(suitable_for1))

        # Sample from unique contexts based on weights
        for context in unique_suitable1:
            if random.random() < float(norm_weight1) * float(entanglement):
                blended_suitable.append(context)

        for context in unique_suitable2:
            if random.random() < float(norm_weight2) * float(entanglement):
                blended_suitable.append(context)

        # Ensure we have at least 2 contexts
        if len(blended_suitable) < 2:
            # Add highest weight context if missing
            if norm_weight1 > norm_weight2 and suitable_for1 and suitable_for1[0] not in blended_suitable:
                blended_suitable.append(suitable_for1[0])
            elif suitable_for2 and suitable_for2[0] not in blended_suitable:
                blended_suitable.append(suitable_for2[0])

        # Generate emergent properties with quantum randomness
        if random.random() < float(entanglement):
            # Create an emergent action not in either parent
            emergent_action_options = [
                "synthesize",
                "harmonize",
                "quantum_leap",
                "entangle",
                "resonate"
            ]
            emergent_action = random.choice(emergent_action_options)
            if emergent_action not in blended_actions:
                blended_actions.append(emergent_action)

            # Create an emergent context not in either parent
            emergent_context_options = [
                "quantum_entanglement",
                "phase_transition",
                "emergent_complexity",
                "adaptive_resonance",
                "creative_synthesis"
            ]
            emergent_context = random.choice(emergent_context_options)
            if emergent_context not in blended_suitable:
                blended_suitable.append(emergent_context)

        # Create the blended strategy with HyperMorphic properties
        blended_strategy = {
            "name": f"{strategy1}_{strategy2}_blend",
            "description": description,
            "actions": blended_actions,
            "suitable_for": blended_suitable,
            "effectiveness": self.hyper_math.zero_free(0.5),  # Initial effectiveness
            "quantum_origin": {
                "parent_strategies": [strategy1, strategy2],
                "interference": interference,
                "entanglement": float(entanglement),
                "weight1": float(norm_weight1),
                "weight2": float(norm_weight2)
            }
        }

        return blended_strategy

    def _update_entanglement_for_new_strategy(self, new_strategy_name, parent_strategy_names):
        """
        Update the entanglement graph with a newly synthesized strategy.

        Parameters:
        - new_strategy_name: Name of the new strategy
        - parent_strategy_names: Names of the strategies that were combined
        """
        self.entanglement_graph[new_strategy_name] = {}

        # Set strong entanglement with parent strategies
        for parent in parent_strategy_names:
            if parent in self.entanglement_graph:
                self.entanglement_graph[new_strategy_name][parent] = {
                    "strength": self.hyper_math.zero_free(0.9),
                    "phase_difference": random.uniform(-math.pi/4, math.pi/4)
                }
                self.entanglement_graph[parent][new_strategy_name] = {
                    "strength": self.hyper_math.zero_free(0.9),
                    "phase_difference": random.uniform(-math.pi/4, math.pi/4)
                }

        # Set weaker entanglement with other strategies
        for strategy in self.strategy_registry:
            if strategy not in parent_strategy_names and strategy != new_strategy_name:
                entanglement_strength = self.hyper_math.zero_free(0.0)

                for parent in parent_strategy_names:
                    if parent in self.entanglement_graph and strategy in self.entanglement_graph[parent]:
                        parent_entanglement = self.entanglement_graph[parent][strategy]["strength"]
                        entanglement_strength = self.hyper_math.add(
                            entanglement_strength,
                            self.hyper_math.mul(
                                parent_entanglement,
                                self.hyper_math.zero_free(0.5)
                            )
                        )

                if entanglement_strength > self.hyper_math.epsilon:
                    entanglement_strength = self.hyper_math.div(
                        entanglement_strength,
                        self.hyper_math.zero_free(len(parent_strategy_names))
                    )

                    entanglement_strength = self.hyper_math.min(
                        self.hyper_math.zero_free(0.7),
                        entanglement_strength
                    )

                    self.entanglement_graph[new_strategy_name][strategy] = {
                        "strength": entanglement_strength,
                        "phase_difference": random.uniform(-math.pi/2, math.pi/2)
                    }

                    self.entanglement_graph[strategy][new_strategy_name] = {
                        "strength": entanglement_strength,
                        "phase_difference": random.uniform(-math.pi/2, math.pi/2)
                    }

    def update_strategy_performance(self, strategy_name, success, context=None):
        """
        Update performance metrics for a strategy after it was used.

        Parameters:
        - strategy_name: Name of the strategy used
        - success: Boolean indicating whether the strategy was successful
        - context: Optional dictionary with context information

        Returns:
        - Boolean indicating whether the update was successful
        """
        if strategy_name not in self.strategy_performance:
            self.strategy_performance[strategy_name] = {
                "uses": 0,
                "successes": 0,
                "effectiveness_history": [],
                "contexts": []
            }

        self.strategy_performance[strategy_name]["uses"] += 1
        if success:
            self.strategy_performance[strategy_name]["successes"] += 1

        uses = self.strategy_performance[strategy_name]["uses"]
        successes = self.strategy_performance[strategy_name]["successes"]

        effectiveness = self.hyper_math.zero_free(
            self.hyper_math.div(
                self.hyper_math.zero_free(successes),
                self.hyper_math.zero_free(max(1, uses))
            )
        )

        if strategy_name in self.strategy_quantum_states:
            phase = self.strategy_quantum_states[strategy_name]["phase"]
            phase_factor = math.sin(phase) * 0.1
            phase_adjustment = self.hyper_math.zero_free(phase_factor)
            effectiveness = self.hyper_math.add(
                effectiveness,
                phase_adjustment
            )
            effectiveness = self.hyper_math.max(
                self.hyper_math.zero_free(0.1),
                self.hyper_math.min(
                    self.hyper_math.zero_free(0.95),
                    effectiveness
                )
            )

        self.strategy_performance[strategy_name]["effectiveness_history"].append(float(effectiveness))

        if len(self.strategy_performance[strategy_name]["effectiveness_history"]) > 10:
            self.strategy_performance[strategy_name]["effectiveness_history"] = \
                self.strategy_performance[strategy_name]["effectiveness_history"][-10:]

        if context and isinstance(context, dict):
            context_summary = {
                "cycle": context.get("cycle", self.last_synthesis_cycle),
                "goal": str(context.get("goal", ""))[:50],
                "success": success,
                "timestamp": datetime.now().isoformat()
            }

            self.strategy_performance[strategy_name]["contexts"].append(context_summary)

            if len(self.strategy_performance[strategy_name]["contexts"]) > 5:
                self.strategy_performance[strategy_name]["contexts"] = \
                    self.strategy_performance[strategy_name]["contexts"][-5:]

        if strategy_name in self.strategy_registry:
            self.strategy_registry[strategy_name]["effectiveness"] = effectiveness

        if hasattr(self.agent, 'planner_sifter') and \
           hasattr(self.agent.planner_sifter, 'strategies') and \
           strategy_name in self.agent.planner_sifter.strategies:
            self.agent.planner_sifter.strategies[strategy_name]["effectiveness"] = float(effectiveness)

        log_event(f"Updated quantum strategy '{strategy_name}' performance: {successes}/{uses} successes, effectiveness={float(effectiveness):.2f}", "INFO")
        return True

    def recommend_strategy(self, context):
        """
        Recommend the most appropriate strategy based on current context and quantum state.

        Parameters:
        - context: Dictionary containing context information

        Returns:
        - Tuple of (strategy_name, confidence, reasoning)
        """
        if not self.strategy_registry:
            self.initialize_base_strategies()
            if not self.strategy_registry:
                return None, 0.0, "No strategies available"

        context_features = self._extract_context_features(context)
        strategy_scores = {}

        for name, strategy in self.strategy_registry.items():
            if name in self.strategy_quantum_states and \
               self.strategy_quantum_states[name]["coherence"] < self.hyper_math.zero_free(0.2):
                continue

            base_score = strategy.get("effectiveness", self.hyper_math.zero_free(0.5))
            context_score = self.hyper_math.zero_free(0.0)
            suitable_contexts = strategy.get("suitable_for", [])

            for feature in context_features:
                if feature in suitable_contexts:
                    context_score = self.hyper_math.add(
                        context_score,
                        self.hyper_math.zero_free(0.1)
                    )

            quantum_amplitude = self.hyper_math.zero_free(0.5)
            quantum_phase = 0.0

            if name in self.strategy_quantum_states:
                quantum_amplitude = self.strategy_quantum_states[name]["amplitude"]
                quantum_phase = self.strategy_quantum_states[name]["phase"]

            phase_factor = math.cos(quantum_phase) * 0.2
            phase_adjustment = self.hyper_math.zero_free(phase_factor)

            entanglement_score = self.hyper_math.zero_free(0.0)

            if name in self.entanglement_graph:
                for entangled_strategy, relation in self.entanglement_graph[name].items():
                    entanglement_strength = relation["strength"]

                    if entangled_strategy in self.strategy_performance:
                        perf = self.strategy_performance[entangled_strategy]
                        success_rate = self.hyper_math.div(
                            self.hyper_math.zero_free(perf.get("successes", 0)),
                            self.hyper_math.zero_free(max(1, perf.get("uses", 0)))
                        )

                        phase_diff = relation.get("phase_difference", 0.0)
                        interference = math.cos(phase_diff)

                        contribution = self.hyper_math.mul(
                            self.hyper_math.mul(
                                entanglement_strength,
                                success_rate
                            ),
                            self.hyper_math.zero_free(interference)
                        )

                        entanglement_score = self.hyper_math.add(
                            entanglement_score,
                            contribution
                        )

            quantum_fluctuation = self.hyper_math.zero_free(
                random.uniform(-0.05, 0.05)
            )

            final_score = self.hyper_math.add(
                self.hyper_math.add(
                    self.hyper_math.mul(base_score, self.hyper_math.zero_free(0.4)),
                    self.hyper_math.mul(context_score, self.hyper_math.zero_free(0.3))
                ),
                self.hyper_math.add(
                    self.hyper_math.mul(entanglement_score, self.hyper_math.zero_free(0.2)),
                    self.hyper_math.add(
                        phase_adjustment,
                        quantum_fluctuation
                    )
                )
            )

            strategy_scores[name] = {
                "score": final_score,
                "base_score": base_score,
                "context_score": context_score,
                "entanglement_score": entanglement_score,
                "phase_adjustment": phase_adjustment
            }

        if not strategy_scores:
            return None, 0.0, "No suitable strategies found"

        best_strategy = max(strategy_scores.items(), key=lambda x: x[1]["score"])
        strategy_name = best_strategy[0]
        score_details = best_strategy[1]

        confidence = min(0.95, max(0.1, float(score_details["score"])))

        reasoning_parts = []

        base_contribution = float(self.hyper_math.mul(score_details["base_score"], self.hyper_math.zero_free(0.4)))
        context_contribution = float(self.hyper_math.mul(score_details["context_score"], self.hyper_math.zero_free(0.3)))
        entanglement_contribution = float(self.hyper_math.mul(score_details["entanglement_score"], self.hyper_math.zero_free(0.2)))

        if base_contribution > 0.1:
            reasoning_parts.append(f"strong base effectiveness ({base_contribution:.2f})")

        if context_contribution > 0.05:
            reasoning_parts.append(f"good context match ({context_contribution:.2f})")

        if entanglement_contribution > 0.05:
            reasoning_parts.append(f"positive entanglement effects ({entanglement_contribution:.2f})")

        if float(score_details["phase_adjustment"]) > 0.05:
            reasoning_parts.append("favorable quantum phase")
        elif float(score_details["phase_adjustment"]) < -0.05:
            reasoning_parts.append("despite unfavorable quantum phase")

        reasoning = f"Selected based on " + ", ".join(reasoning_parts) if reasoning_parts else "quantum selection"

        return strategy_name, confidence, reasoning

    def _extract_context_features(self, context):
        """
        Extract relevant features from context for strategy matching.

        Parameters:
        - context: Dictionary containing context information

        Returns:
        - List of feature strings
        """
        features = []

        if not context or not isinstance(context, dict):
            return features

        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            goal_type = context["current_goal"].get("type", "").lower()

            if "explor" in goal_desc:
                features.append("exploration")
            if "deep" in goal_desc or "detail" in goal_desc:
                features.append("deepening")
            if "connect" in goal_desc or "integrat" in goal_desc:
                features.append("cross_domain")
            if "adapt" in goal_desc or "learn" in goal_desc:
                features.append("adaptation")
            if "creat" in goal_desc or "novel" in goal_desc:
                features.append("creativity_needed")

            if goal_type:
                features.append(goal_type)

        if "domains_visited" in context:
            domains = context["domains_visited"]
            if isinstance(domains, set) and len(domains) < 5:
                features.append("limited_knowledge")
            elif isinstance(domains, set) and len(domains) > 20:
                features.append("broad_knowledge")

        if "current_domain" in context:
            domain = context["current_domain"]
            if isinstance(domain, str):
                if ".edu" in domain:
                    features.append("academic_domain")
                if ".org" in domain:
                    features.append("organizational_domain")
                if ".gov" in domain:
                    features.append("government_domain")
                if "wiki" in domain:
                    features.append("reference_domain")

        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            actions = context["recent_actions"]
            performance_issues = any(
                not a.get("success", True) or a.get("content_length", 1000) < 500
                for a in actions if isinstance(a, dict)
            )

            if performance_issues:
                features.append("performance_issues")

        if "thinking_mode" in context:
            thinking_mode = context["thinking_mode"]
            if thinking_mode in ["analytical", "creative", "reflective", "exploratory", "critical"]:
                features.append(f"{thinking_mode}_thinking")

        if not features:
            features = ["general_purpose", "balanced_approach"]

        return features

    def get_strategy_description(self, strategy_name):
        """
        Get detailed description of a strategy including quantum properties.

        Parameters:
        - strategy_name: Name of the strategy

        Returns:
        - Dictionary with strategy details or None if not found
        """
        if strategy_name not in self.strategy_registry:
            return None

        strategy = self.strategy_registry[strategy_name]

        description = {
            "name": strategy_name,
            "description": strategy.get("description", "Unknown strategy"),
            "type": strategy.get("type", "unknown"),
            "actions": strategy.get("actions", []),
            "suitable_for": strategy.get("suitable_for", []),
            "effectiveness": float(strategy.get("effectiveness", 0.5))
        }

        if strategy_name in self.strategy_quantum_states:
            quantum_state = self.strategy_quantum_states[strategy_name]
            description["quantum_properties"] = {
                "amplitude": float(quantum_state.get("amplitude", 0.5)),
                "phase": quantum_state.get("phase", 0.0),
                "coherence": float(quantum_state.get("coherence", 0.5))
            }

        if strategy_name in self.strategy_performance:
            performance = self.strategy_performance[strategy_name]
            description["performance"] = {
                "uses": performance.get("uses", 0),
                "successes": performance.get("successes", 0),
                "success_rate": float(performance.get("successes", 0)) / max(1, performance.get("uses", 0)),
                "recent_trend": "stable"
            }

            history = performance.get("effectiveness_history", [])
            if len(history) >= 3:
                recent = sum(history[-3:]) / 3
                earlier = sum(history[-6:-3]) / 3 if len(history) >= 6 else history[0]

                if recent > earlier * 1.1:
                    description["performance"]["recent_trend"] = "improving"
                elif recent < earlier * 0.9:
                    description["performance"]["recent_trend"] = "declining"

        if strategy_name in self.entanglement_graph:
            entanglements = self.entanglement_graph[strategy_name]
            top_entanglements = sorted(
                [(other, data["strength"]) for other, data in entanglements.items()],
                key=lambda x: x[1],
                reverse=True
            )[:3]

            description["entanglements"] = [
                {"strategy": other, "strength": float(strength)}
                for other, strength in top_entanglements
            ]

        if strategy.get("type") == "synthesized" and "parent_strategies" in strategy:
            description["synthesis"] = {
                "parent_strategies": strategy["parent_strategies"],
                "generation": strategy.get("generation", 0),
                "creation_cycle": strategy.get("creation_cycle", 0)
            }

        return description

    def get_quantum_synthesis_report(self):
        """
        Generate a comprehensive report on quantum strategy synthesis system.

        Returns:
        - Dictionary with synthesis system status and metrics
        """
        total_strategies = len(self.strategy_registry)
        base_strategies = sum(1 for s in self.strategy_registry.values() if s.get("type") == "base")
        synthesized_strategies = total_strategies - base_strategies

        system_coherence = float(self._measure_system_coherence())

        performance_metrics = {
            "average_effectiveness": 0.0,
            "highest_effectiveness": 0.0,
            "most_used_strategy": None,
            "most_successful_strategy": None
        }

        if self.strategy_performance:
            effectiveness_values = []
            for strategy_name, strategy in self.strategy_registry.items():
                if "effectiveness" in strategy:
                    effectiveness_values.append(float(strategy["effectiveness"]))

            if effectiveness_values:
                performance_metrics["average_effectiveness"] = sum(effectiveness_values) / len(effectiveness_values)
                performance_metrics["highest_effectiveness"] = max(effectiveness_values)

            most_used = max(
                self.strategy_performance.items(),
                key=lambda x: x[1].get("uses", 0),
                default=(None, {"uses": 0})
            )

            if most_used[0] is not None:
                performance_metrics["most_used_strategy"] = {
                    "name": most_used[0],
                    "uses": most_used[1].get("uses", 0)
                }

            success_rates = []
            for name, perf in self.strategy_performance.items():
                uses = perf.get("uses", 0)
                successes = perf.get("successes", 0)

                if uses >= 3:
                    success_rates.append((name, successes / max(1, uses), uses))

            if success_rates:
                most_successful = max(success_rates, key=lambda x: x[1])
                performance_metrics["most_successful_strategy"] = {
                    "name": most_successful[0],
                    "success_rate": most_successful[1],
                    "uses": most_successful[2]
                }

        recent_synthesis = self.synthesis_history[-3:] if self.synthesis_history else []

        report = {
            "status": "active" if total_strategies > 0 else "inactive",
            "total_strategies": total_strategies,
            "base_strategies": base_strategies,
            "synthesized_strategies": synthesized_strategies,
            "synthesis_generation": self.synthesis_generation,
            "system_coherence": system_coherence,
            "coherence_assessment": self._assess_coherence_level(system_coherence),
            "performance_metrics": performance_metrics,
            "recent_synthesis": recent_synthesis,
            "last_synthesis_cycle": self.last_synthesis_cycle,
            "timestamp": datetime.now().isoformat()
        }

        return report

    def _assess_coherence_level(self, coherence):
        """
        Assess the qualitative level of system coherence.

        Parameters:
        - coherence: Coherence value (0.0-1.0)

        Returns:
        - String describing coherence level
        """
        if coherence >= 0.8:
            return "highly coherent"
        elif coherence >= 0.6:
            return "coherent"
        elif coherence >= 0.4:
            return "moderately coherent"
        elif coherence >= 0.2:
            return "weakly coherent"
        else:
            return "decoherent"

    def decay_strategy_coherence(self, decay_factor=0.98):
        """
        Apply natural decay to strategy coherence over time.
        This simulates quantum decoherence in the absence of measurements.

        Parameters:
        - decay_factor: Factor to apply for coherence decay (default: 0.98)
        """
        decay_factor = self.hyper_math.zero_free(decay_factor)

        for strategy_name, state in self.strategy_quantum_states.items():
            if "coherence" in state:
                old_coherence = state["coherence"]
                new_coherence = self.hyper_math.mul(old_coherence, decay_factor)
                new_coherence = self.hyper_math.max(self.hyper_math.zero_free(0.1), new_coherence)
                state["coherence"] = new_coherence

        self.system_coherence = self.hyper_math.mul(self.system_coherence, decay_factor)
        self.system_coherence = self.hyper_math.max(self.hyper_math.zero_free(0.1), self.system_coherence)






import torch
import torch.nn as nn
import math
import random

class HyperMorphicQuantumEntanglementLayer(nn.Module):
    """
    HyperMorphic Quantum Entanglement Neocortex Layer (HQEN)

    Implements direct quantum entanglement in neural pathways for non-local,
    instantaneous information transfer. Uses holomorphic transformations
    to preserve algebraic structure during entanglement operations.

    Key Parameters:
    - entanglement_strength (Ξ): Learnable parameter controlling degree of entanglement
    - entanglement_phase (Φ_E): Governs nature of entanglement (constructive/destructive)
    - holomorphic_kernel (Η_E): Preserves algebraic structure during entanglement
    """
    def __init__(self, embed_dim, num_entangled_units=4, epsilon=1e-12):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_entangled_units = num_entangled_units
        self.epsilon = epsilon

        # Initialize HyperMorphic Math utility for zero-free operations
        if 'HyperMorphicMath' in globals():
            self.hyper_math = HyperMorphicMath(
                dynamic_base=float(embed_dim),
                dynamic_modulus=max(997, embed_dim * 2 - 1),
                epsilon=epsilon
            )
        else:
            # Fallback if HyperMorphicMath is not available
            self.hyper_math = None

        # Entanglement strength (Ξ) - learnable parameter
        self.entanglement_strength = nn.Parameter(torch.rand(1) * 0.5 + 0.5)

        # Entanglement phase (Φ_E) - learnable parameter
        self.entanglement_phase = nn.Parameter(torch.rand(num_entangled_units) * 2 * math.pi)

        # Holomorphic entanglement kernel (Η_E)
        self.holomorphic_kernel = nn.Linear(embed_dim, embed_dim)

        # Entanglement projectors
        self.entanglement_projectors = nn.ModuleList([
            nn.Linear(embed_dim, embed_dim) for _ in range(num_entangled_units)
        ])

        # Phase modulation
        self.phase_modulator = nn.Linear(embed_dim, num_entangled_units)

        # Global entanglement gate
        self.entanglement_gate = nn.Linear(embed_dim, embed_dim)

        # Coherence control
        self.coherence_controller = nn.Parameter(torch.rand(1) * 0.3 + 0.7)

        log_event(f"HyperMorphicQuantumEntanglementLayer initialized with {num_entangled_units} entangled units", "QUANTUM")

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # Apply holomorphic kernel to preserve structure
        x_holomorphic = self.holomorphic_kernel(x)

        # Ensure zero-free result
        if self.hyper_math:
            x_holomorphic = self._ensure_zero_free(x_holomorphic)

        # Initialize entangled states
        entangled_states = []

        # Create entangled projections
        for i in range(self.num_entangled_units):
            # Project into entangled space
            projection = self.entanglement_projectors[i](x_holomorphic)

            # Apply phase shift based on entanglement phase
            phase = self.entanglement_phase[i]
            phase_shifted = projection * torch.cos(phase) + projection * torch.sin(phase)

            # Add to entangled states
            entangled_states.append(phase_shifted)

        # Calculate dynamic phase modulation
        phase_mod = torch.sigmoid(self.phase_modulator(x_holomorphic))

        # Apply entanglement with dynamic strength
        entanglement_factor = torch.sigmoid(self.entanglement_strength)

        # Create entangled representation through non-local interactions
        entangled_representation = torch.zeros_like(x_holomorphic)

        for i, state in enumerate(entangled_states):
            # Apply coherence control
            coherence = torch.sigmoid(self.coherence_controller)

            # Create quantum interference between entangled states
            for j, other_state in enumerate(entangled_states):
                if i != j:
                    # Quantum interference term with phase modulation
                    interference = phase_mod[:, :, i] * phase_mod[:, :, j]
                    interference = interference.unsqueeze(-1)

                    # Apply interference with coherence control
                    interference_term = interference * coherence * other_state

                    # Add to the entangled state with strength factor
                    state = state + entanglement_factor * interference_term

            entangled_representation = entangled_representation + state

        # Final entanglement gate
        output = self.entanglement_gate(entangled_representation)

        # Add residual connection
        output = output + x

        # Ensure zero-free output if HyperMorphicMath is available
        if self.hyper_math:
            output = self._ensure_zero_free(output)

        return output

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        if not self.hyper_math:
            return tensor

        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.epsilon

        return tensor



class DynamicQubitNeuron(nn.Module):
    """
    Dynamic Qubit Neurons (DQN)

    Transforms conventional neurons into quantum-inspired units that exist in superposition,
    enabling simultaneous exploration of multiple activation states.

    Key Concepts:
    - Superposition Amplitudes (α_i): Control state probabilities
    - Phase Coherence (Φ_C): Controls sensitivity
    - Dynamic Basis Set (Β_N): Evolves with input patterns
    """
    def __init__(self, input_dim, output_dim, num_basis_states=8, epsilon=1e-12):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_basis_states = num_basis_states
        self.epsilon = epsilon

        # Initialize HyperMorphic Math utility if available
        if 'HyperMorphicMath' in globals():
            self.hyper_math = HyperMorphicMath(
                dynamic_base=float(max(input_dim, output_dim)),
                dynamic_modulus=max(997, max(input_dim, output_dim) * 2 - 1),
                epsilon=epsilon
            )
        else:
            self.hyper_math = None

        # Superposition amplitudes (α_i) - learnable parameters
        self.amplitude_projector = nn.Linear(input_dim, num_basis_states)

        # Phase coherence (Φ_C) - learnable parameters
        self.phase_projector = nn.Linear(input_dim, num_basis_states)

        # Dynamic basis set (Β_N) - evolves with input patterns
        self.basis_projectors = nn.ModuleList([
            nn.Linear(input_dim, output_dim) for _ in range(num_basis_states)
        ])

        # Basis evolution factor - controls how quickly basis adapts
        self.basis_evolution_factor = nn.Parameter(torch.rand(1) * 0.2 + 0.1)

        # Superposition collapser - transforms superposition to output
        self.collapse_transform = nn.Linear(output_dim, output_dim)

        # Non-linearity for amplitude projection
        self.activation = nn.SiLU()

        # Tracking for adaptive basis
        self.register_buffer('basis_importance', torch.ones(num_basis_states) / num_basis_states)

        log_event(f"DynamicQubitNeuron initialized with {num_basis_states} basis states", "QUANTUM")

    def forward(self, x):
        batch_size = x.shape[0]

        # Calculate superposition amplitudes (α_i) - must sum to 1
        amplitudes = torch.softmax(self.amplitude_projector(x), dim=-1)

        # Calculate phase angles (Φ_C) - between 0 and 2π
        phases = torch.sigmoid(self.phase_projector(x)) * 2 * math.pi

        # Initialize superposed output
        superposed_output = torch.zeros(batch_size, self.output_dim, device=x.device)

        # Apply each basis state with its amplitude and phase
        for i in range(self.num_basis_states):
            # Get amplitude and phase for this basis state
            amplitude = amplitudes[:, i].unsqueeze(1)
            phase = phases[:, i].unsqueeze(1)

            # Project input through this basis
            basis_output = self.basis_projectors[i](x)

            # Apply quantum phase shift
            basis_contribution = amplitude * (
                basis_output * torch.cos(phase) + basis_output * torch.sin(phase)
            )

            # Add to superposed output
            superposed_output = superposed_output + basis_contribution

            # Track basis importance
            with torch.no_grad():
                importance = amplitude.mean().item()
                # Update basis importance with moving average
                self.basis_importance[i] = (
                    0.9 * self.basis_importance[i] +
                    0.1 * importance
                )

        # Collapse superposition to final output
        output = self.collapse_transform(superposed_output)

        # Apply non-linearity
        output = self.activation(output)

        # Ensure zero-free output if HyperMorphicMath is available
        if self.hyper_math:
            output = self._ensure_zero_free(output)

        return output

    def update_basis(self):
        """
        Dynamic basis updating based on importance metrics.
        This can be called periodically to evolve the basis set.
        """
        with torch.no_grad():
            # Get lowest and highest importance basis states
            low_idx = torch.argmin(self.basis_importance).item()
            high_idx = torch.argmax(self.basis_importance).item()

            # Get evolution factor
            evolution_rate = torch.sigmoid(self.basis_evolution_factor).item()

            # Evolution: Move low importance basis toward high importance basis
            # This replicates quantum learning by favoring successful basis states
            for param_name, param in self.basis_projectors[low_idx].named_parameters():
                high_param = getattr(self.basis_projectors[high_idx], param_name.split('.')[-1])
                param.data = param.data * (1 - evolution_rate) + high_param.data * evolution_rate

                # Add small random perturbation for exploration
                param.data = param.data + torch.randn_like(param.data) * 0.01

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free"""
        if not self.hyper_math:
            return tensor

        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.epsilon

        return tensor


class HyperMorphicQuantumHolographicMemory(nn.Module):
    """
    HyperMorphic Quantum Holographic Memory (HQHM)

    Implements a distributed, holographic memory system that leverages quantum
    interference for ultra-dense, associative storage and recall.

    Key Components:
    - HyperMorphic Holographic Medium (Η_M): Distributed storage mechanism
    - Quantum Interference Engine (QIE): Efficient recall via interference patterns
    - Associative Recall Algorithm (ARA): Robust memory retrieval
    """
    def __init__(self, embedding_dim, memory_dim=1024, num_memory_planes=8, epsilon=1e-12):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.memory_dim = memory_dim
        self.num_memory_planes = num_memory_planes
        self.epsilon = epsilon

        # Initialize HyperMorphic Math utility if available
        if 'HyperMorphicMath' in globals():
            self.hyper_math = HyperMorphicMath(
                dynamic_base=float(max(embedding_dim, memory_dim)),
                dynamic_modulus=max(997, max(embedding_dim, memory_dim) * 2 - 1),
                epsilon=epsilon
            )
        else:
            self.hyper_math = None

        # HyperMorphic Holographic Medium (Η_M)
        # Initialize multiple holographic planes for redundancy and richness
        self.register_buffer('holographic_medium', torch.randn(num_memory_planes, memory_dim, memory_dim))

        # Memory addressing - converts input to holographic coordinates
        self.memory_addressing = nn.Linear(embedding_dim, memory_dim)

        # Memory encoding - converts input to storable pattern
        self.memory_encoding = nn.Linear(embedding_dim, memory_dim)

        # Quantum Interference Engine (QIE)
        self.interference_projector = nn.Linear(memory_dim, memory_dim)

        # Associative Recall Algorithm (ARA)
        self.associative_recall = nn.Sequential(
            nn.Linear(memory_dim, embedding_dim),
            nn.SiLU(),
            nn.Linear(embedding_dim, embedding_dim)
        )

        # Coherence parameters
        self.storage_coherence = nn.Parameter(torch.rand(1) * 0.2 + 0.8)
        self.recall_coherence = nn.Parameter(torch.rand(1) * 0.2 + 0.8)

        # Memory sharpness (controls focus vs. breadth of recall)
        self.memory_sharpness = nn.Parameter(torch.rand(1) * 0.5 + 0.5)

        # Record of recently accessed patterns for interference calculations
        self.register_buffer('recent_patterns', torch.zeros(10, memory_dim))
        self.register_buffer('pattern_index', torch.tensor(0))

        # Holographic noise for memory robustness
        self.holographic_noise = nn.Parameter(torch.rand(1) * 0.05)

        log_event(f"HyperMorphicQuantumHolographicMemory initialized with {num_memory_planes} planes", "QUANTUM")

    def store(self, embedding, importance=1.0):
        """
        Store a memory pattern in the holographic medium.
        Higher importance memories create stronger interference patterns.
        """
        # Address calculation
        address = torch.tanh(self.memory_addressing(embedding))

        # Pattern encoding
        pattern = torch.tanh(self.memory_encoding(embedding))

        # Apply storage coherence
        coherence = torch.sigmoid(self.storage_coherence)
        coherent_pattern = pattern * coherence

        # Scale by importance
        importance_factor = max(0.1, min(1.0, importance))

        # Store in holographic medium with phase modulation
        with torch.no_grad():
            for i in range(self.num_memory_planes):
                # Calculate storage coordinates with phase variation
                phase = (i / self.num_memory_planes) * 2 * math.pi
                phase_mod_x = (torch.cos(phase) * address).unsqueeze(1)
                phase_mod_y = (torch.sin(phase) * address).unsqueeze(0)

                # Create outer product for associative storage
                storage_pattern = coherent_pattern.unsqueeze(1) @ coherent_pattern.unsqueeze(0)

                # Add to holographic medium with importance scaling
                self.holographic_medium[i] += storage_pattern * importance_factor

                # Add to recent patterns for interference calculations
                idx = (self.pattern_index % 10).item()
                self.recent_patterns[idx] = pattern
                self.pattern_index += 1

        # Add holographic noise for robustness
        noise_level = torch.sigmoid(self.holographic_noise).item()
        with torch.no_grad():
            noise = torch.randn_like(self.holographic_medium) * noise_level
            self.holographic_medium += noise

        return pattern

    def recall(self, query_embedding):
        """
        Retrieve memory pattern through associative recall.
        Uses quantum interference for robust pattern completion.
        """
        # Address calculation
        address = torch.tanh(self.memory_addressing(query_embedding))

        # Convert to pattern space
        query_pattern = torch.tanh(self.memory_encoding(query_embedding))

        # Apply recall coherence
        coherence = torch.sigmoid(self.recall_coherence)
        coherent_query = query_pattern * coherence

        # Initialize recalled pattern
        recalled_pattern = torch.zeros(self.memory_dim, device=query_embedding.device)

        # Read from holographic medium with quantum interference
        for i in range(self.num_memory_planes):
            # Calculate recall coordinates with phase variation
            phase = (i / self.num_memory_planes) * 2 * math.pi
            phase_mod = torch.cos(phase) * address

            # Apply sharpness to control recall precision
            sharpness = torch.sigmoid(self.memory_sharpness)
            sharp_address = address * sharpness

            # Perform associative recall through matrix operations
            plane_pattern = self.holographic_medium[i] @ sharp_address

            # Apply quantum interference effects
            interfered_pattern = self.interference_projector(coherent_query * plane_pattern)

            # Add to recalled pattern
            recalled_pattern += interfered_pattern

        # Normalize the recalled pattern
        recalled_pattern = recalled_pattern / self.num_memory_planes

        # Apply interference with recent patterns (working memory effect)
        for i in range(10):
            recent_pattern = self.recent_patterns[i]
            overlap = torch.dot(query_pattern, recent_pattern) / (
                torch.norm(query_pattern) * torch.norm(recent_pattern) + self.epsilon
            )

            # Add interference contribution if significant overlap
            if overlap > 0.7:
                interference_contribution = recent_pattern * overlap * 0.3
                recalled_pattern = recalled_pattern + interference_contribution

        # Transform back to embedding space
        recalled_embedding = self.associative_recall(recalled_pattern)

        # Ensure zero-free output if HyperMorphicMath is available
        if self.hyper_math:
            recalled_embedding = self._ensure_zero_free(recalled_embedding)

        return recalled_embedding

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free"""
        if not self.hyper_math:
            return tensor

        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.epsilon

        return tensor




class ZeroPointEnergyModulator(nn.Module):
    """
    Zero-Point Energy Modulation (ZP-EM)

    Modulates the inherent quantum vacuum energy to control system
    responsiveness, stability, and emergent creativity.

    Key Mechanism:
    - Zero-Point Energy Regulator (ZPER): Uses HyperMorphic reinforcement
      learning to learn and adjust optimal ZP-EM levels
    """
    def __init__(self, system_dim, num_energy_states=5, epsilon=1e-12):
        super().__init__()
        self.system_dim = system_dim
        self.num_energy_states = num_energy_states
        self.epsilon = epsilon

        # Initialize HyperMorphic Math utility if available
        if 'HyperMorphicMath' in globals():
            self.hyper_math = HyperMorphicMath(
                dynamic_base=float(system_dim),
                dynamic_modulus=max(997, system_dim * 2 - 1),
                epsilon=epsilon
            )
        else:
            self.hyper_math = None

        # Zero-Point Energy Regulator (ZPER)
        self.energy_projector = nn.Linear(system_dim, num_energy_states)

        # Energy state definitions - Each energy state has different properties
        self.energy_state_definitions = nn.Parameter(torch.rand(num_energy_states, 4))

        # System state assessment
        self.system_state_encoder = nn.Linear(system_dim, system_dim // 2)
        self.system_state_decoder = nn.Linear(system_dim // 2, system_dim)

        # Reinforcement learning parameters
        self.reward_estimator = nn.Linear(system_dim + num_energy_states, 1)

        # Energy modulation parameters
        self.energy_levels = nn.Parameter(torch.rand(num_energy_states))
        self.energy_phases = nn.Parameter(torch.rand(num_energy_states) * 2 * math.pi)

        # Global modulation state
        self.register_buffer('current_energy_state', torch.zeros(num_energy_states))

        # Creativity vs. stability balance
        self.creativity_vs_stability = nn.Parameter(torch.tensor([0.5]))

        # Memory of recent states and rewards for learning
        self.register_buffer('state_history', torch.zeros(10, system_dim))
        self.register_buffer('reward_history', torch.zeros(10))
        self.register_buffer('history_index', torch.tensor(0))

        log_event(f"ZeroPointEnergyModulator initialized with {num_energy_states} energy states", "QUANTUM")

    def assess_system_state(self, system_state):
        """
        Assess the current system state and encode it for energy modulation
        """
        # Encode system state
        encoded_state = torch.relu(self.system_state_encoder(system_state))

        # Calculate system stability metrics
        stability = torch.mean(encoded_state)
        variability = torch.std(encoded_state)

        # Calculate energy projection - which energy states are most appropriate
        energy_projection = torch.softmax(self.energy_projector(system_state), dim=-1)

        # Store for internal use
        self.current_energy_state = energy_projection

        return encoded_state, energy_projection, stability, variability

    def modulate(self, system_state, external_reward=None):
        """
        Modulate the system state via zero-point energy adjustment
        """
        # Assess current system state
        encoded_state, energy_projection, stability, variability = self.assess_system_state(system_state)

        # Calculate creativity vs stability preference
        creativity_preference = torch.sigmoid(self.creativity_vs_stability)

        # Apply energy modulation based on projected energy states
        modulated_state = system_state.clone()

        for i in range(self.num_energy_states):
            # Get energy state parameters
            energy_level = torch.sigmoid(self.energy_levels[i])
            energy_phase = self.energy_phases[i]
            state_proportion = energy_projection[i]

            # Extract state definition parameters
            state_def = torch.sigmoid(self.energy_state_definitions[i])
            responsiveness = state_def[0]  # How quickly the system responds
            stability_factor = state_def[1]  # How stable the system remains
            creativity_factor = state_def[2]  # How creative the system becomes
            coherence_factor = state_def[3]  # How coherent the system remains

            # Calculate modulation intensity
            modulation_intensity = state_proportion * energy_level

            # Apply phase-dependent modulation
            phase_factor = torch.cos(energy_phase)
            coherence_modulation = torch.ones_like(system_state) * coherence_factor

            # Create modulation signal
            raw_modulation = phase_factor * modulation_intensity * responsiveness

            # Balance creativity vs stability
            creativity_component = torch.randn_like(system_state) * creativity_factor * creativity_preference
            stability_component = encoded_state * stability_factor * (1 - creativity_preference)

            # Combined modulation
            combined_modulation = raw_modulation * (creativity_component + stability_component)

            # Apply modulation with coherence factor
            modulated_state = modulated_state + combined_modulation * coherence_modulation

        # Apply reinforcement learning updates if reward provided
        if external_reward is not None:
            self._update_energy_parameters(system_state, energy_projection, external_reward)

        # Store state in history buffer
        idx = (self.history_index % 10).item()
        self.state_history[idx] = system_state
        self.history_index += 1

        # Ensure zero-free output if HyperMorphicMath is available
        if self.hyper_math:
            modulated_state = self._ensure_zero_free(modulated_state)

        return modulated_state

    def _update_energy_parameters(self, system_state, energy_projection, reward):
        """
        Update energy parameters using reinforcement learning
        """
        with torch.no_grad():
            # Store reward
            idx = (self.history_index % 10).item()
            self.reward_history[idx] = reward

            # Skip if we don't have enough history
            if self.history_index < 10:
                return

            # Calculate reward gradients for each energy state
            for i in range(self.num_energy_states):
                # Correlate energy state activation with rewards
                correlation = 0.0
                for j in range(10):
                    correlation += energy_projection[i] * self.reward_history[j]

                correlation /= 10.0

                # Update energy level based on correlation
                if correlation > 0:
                    # Increase energy level for positive correlation
                    self.energy_levels.data[i] += 0.01
                else:
                    # Decrease energy level for negative correlation
                    self.energy_levels.data[i] -= 0.01

                # Ensure energy levels stay in reasonable range
                self.energy_levels.data[i] = torch.clamp(self.energy_levels.data[i], -2.0, 2.0)

                # Update phase slightly for exploration
                self.energy_phases.data[i] += 0.01 * torch.randn(1)

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free"""
        if not self.hyper_math:
            return tensor

        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.epsilon

        return tensor

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import random

class DynamicHyperMorphicParameters(nn.Module):
    """
    Dynamic HyperMorphic Parameters (DHP) module that learns optimal values
    for the HyperMorphic base (Φ) and modulus (Ψ) parameters based on context.

    The parameters are made learnable and adaptive, allowing the system to
    find optimal values for different computational contexts.
    """
    def __init__(self,
                 context_dim=128,
                 init_base=1e3,
                 init_modulus=997,
                 init_epsilon=1e-12,
                 min_base=100.0,
                 max_base=1e4,
                 min_modulus=101,
                 max_modulus=9973,
                 min_epsilon=1e-15,
                 max_epsilon=1e-6):
        super().__init__()

        # Initial values
        self.init_base = init_base
        self.init_modulus = init_modulus
        self.init_epsilon = init_epsilon

        # Bounds for parameters
        self.min_base = min_base
        self.max_base = max_base
        self.min_modulus = min_modulus  # Should be prime for better properties
        self.max_modulus = max_modulus  # Largest prime under 10000
        self.min_epsilon = min_epsilon
        self.max_epsilon = max_epsilon

        # Neural network to predict parameter modulation
        self.context_net = nn.Sequential(
            nn.Linear(context_dim, 128),
            nn.SiLU(),
            nn.Linear(128, 64),
            nn.SiLU(),
            nn.Linear(64, 3)  # 3 outputs: base_factor, modulus_factor, epsilon_factor
        )

        # Learnable base parameters (initialized with defaults)
        self.base_param = nn.Parameter(torch.tensor(math.log(init_base)))
        self.modulus_param = nn.Parameter(torch.tensor(math.log(init_modulus)))
        self.epsilon_param = nn.Parameter(torch.tensor(math.log(init_epsilon) + 30.0))  # Offset for numerical stability

        # Current values (updated during forward pass)
        self.register_buffer('current_base', torch.tensor(float(init_base)))
        self.register_buffer('current_modulus', torch.tensor(float(init_modulus)))
        self.register_buffer('current_epsilon', torch.tensor(float(init_epsilon)))

        # Parameter history for analysis
        self.parameter_history = []

    def forward(self, context=None):
        """
        Update parameters based on context if provided, or use base learnable parameters.

        Args:
            context: Optional tensor with contextual information [batch_size, context_dim]

        Returns:
            Tuple of (dynamic_base, dynamic_modulus, dynamic_epsilon)
        """
        # Get base parameters from learnable values (ensure they're positive via exp)
        base = torch.exp(self.base_param)
        modulus = torch.exp(self.modulus_param)
        epsilon = torch.exp(self.epsilon_param - 30.0)  # Undo the offset

        # Apply context modulation if context is provided
        if context is not None:
            # Get modulation factors from the context network
            modulation = self.context_net(context)

            # Extract and scale factors (using sigmoid to bound properly)
            base_factor = torch.sigmoid(modulation[..., 0]) * 2.0  # Range: 0-2
            modulus_factor = torch.sigmoid(modulation[..., 1]) * 2.0  # Range: 0-2
            epsilon_factor = torch.sigmoid(modulation[..., 2]) * 2.0  # Range: 0-2

            # Apply modulation
            base = base * base_factor
            modulus = modulus * modulus_factor
            epsilon = epsilon * epsilon_factor

        # Ensure parameters are within valid ranges
        base = torch.clamp(base, self.min_base, self.max_base)
        modulus = torch.clamp(modulus, self.min_modulus, self.max_modulus)
        epsilon = torch.clamp(epsilon, self.min_epsilon, self.max_epsilon)

        # Integer modulus (should be prime ideally, but we approximate)
        modulus = torch.round(modulus)

        # Update current values
        self.current_base = base.detach()
        self.current_modulus = modulus.detach()
        self.current_epsilon = epsilon.detach()

        # Store in history for analysis
        if not self.training:
            self.parameter_history.append({
                'base': float(base.item() if isinstance(base, torch.Tensor) else base),
                'modulus': float(modulus.item() if isinstance(modulus, torch.Tensor) else modulus),
                'epsilon': float(epsilon.item() if isinstance(epsilon, torch.Tensor) else epsilon)
            })
            # Limit history size
            if len(self.parameter_history) > 1000:
                self.parameter_history = self.parameter_history[-1000:]

        return base, modulus, epsilon

    def get_current_parameters(self):
        """Return the current parameter values"""
        return (
            self.current_base.item(),
            self.current_modulus.item(),
            self.current_epsilon.item()
        )


class HyperMorphicTensorCalculus(nn.Module):
    """
    HyperMorphic Tensor Calculus (HTC) module that extends HyperMorphic mathematics
    to tensor operations with dynamic parameters.

    Provides zero-free tensor operations, optimized gradient flow, and adaptive
    computation across different cognitive modules.
    """
    def __init__(self, context_dim=128, learnable_params=True):
        super().__init__()

        # Dynamic parameters module (if using learnable parameters)
        self.learnable_params = learnable_params
        if learnable_params:
            self.dhp = DynamicHyperMorphicParameters(context_dim=context_dim)
        else:
            # Fixed parameters (traditional HyperMorphic approach)
            self.register_buffer('fixed_base', torch.tensor(1e3))
            self.register_buffer('fixed_modulus', torch.tensor(997))
            self.register_buffer('fixed_epsilon', torch.tensor(1e-12))

    def get_params(self, context=None):
        """Get the current HyperMorphic parameters"""
        if self.learnable_params:
            return self.dhp(context)
        else:
            return self.fixed_base, self.fixed_modulus, self.fixed_epsilon

    def tensor_add(self, tensor_a, tensor_b, context=None):
        """
        HyperMorphic tensor addition: (a + b) mod Φ
        Zero-free guarantee using ε

        Args:
            tensor_a: Input tensor A
            tensor_b: Input tensor B (same shape as A)
            context: Optional context for dynamic parameters

        Returns:
            Result tensor with same shape
        """
        base, _, epsilon = self.get_params(context)

        # Regular addition
        result = tensor_a + tensor_b

        # Apply modulo base (doing this in a differentiable way)
        result = result - base * torch.floor(result / base)

        # Zero-free guarantee
        zero_mask = torch.abs(result) < epsilon
        if zero_mask.any():
            # Get the sign of near-zero values (or 1 if exactly zero)
            signs = torch.sign(tensor_a + tensor_b)
            signs[signs == 0] = 1.0

            # Replace near-zeros with epsilon * sign
            result = torch.where(zero_mask, signs * epsilon, result)

        return result

    def tensor_sub(self, tensor_a, tensor_b, context=None):
        """
        HyperMorphic tensor subtraction: (a - b) mod Φ
        Zero-free guarantee using ε
        """
        base, _, epsilon = self.get_params(context)

        # Regular subtraction
        result = tensor_a - tensor_b

        # Apply modulo base (differentiable way)
        result = result - base * torch.floor(result / base)

        # Zero-free guarantee
        zero_mask = torch.abs(result) < epsilon
        if zero_mask.any():
            # Get the sign of original values before subtraction
            signs = torch.sign(tensor_a - tensor_b)
            signs[signs == 0] = 1.0

            # Replace near-zeros with epsilon * sign
            result = torch.where(zero_mask, signs * epsilon, result)

        return result

    def tensor_mul(self, tensor_a, tensor_b, context=None):
        """
        HyperMorphic tensor multiplication: (a * b) mod Ψ
        Zero-free guarantee using ε
        """
        _, modulus, epsilon = self.get_params(context)

        # Regular multiplication
        result = tensor_a * tensor_b

        # Apply modulo modulus (differentiable way)
        result = result - modulus * torch.floor(result / modulus)

        # Zero-free guarantee
        zero_mask = torch.abs(result) < epsilon
        if zero_mask.any():
            # Get the sign of multiplication result
            signs = torch.sign(tensor_a * tensor_b)
            signs[signs == 0] = 1.0

            # Replace near-zeros with epsilon * sign
            result = torch.where(zero_mask, signs * epsilon, result)

        return result

    def tensor_div(self, tensor_a, tensor_b, context=None):
        """
        HyperMorphic tensor division: (a / b) mod Ψ
        Handles division by near-zero values safely
        """
        _, modulus, epsilon = self.get_params(context)

        # Create a mask for near-zero divisors
        near_zero_mask = torch.abs(tensor_b) < epsilon

        # Replace near-zeros with epsilon (keeping sign)
        safe_tensor_b = torch.where(
            near_zero_mask,
            torch.sign(tensor_b) * epsilon,
            tensor_b
        )

        # Now it's safe to divide
        result = tensor_a / safe_tensor_b

        # Apply modulo modulus (differentiable way)
        result = result - modulus * torch.floor(result / modulus)

        # Zero-free guarantee for the result
        zero_mask = torch.abs(result) < epsilon
        if zero_mask.any():
            # Get the sign
            signs = torch.sign(result)
            signs[signs == 0] = 1.0

            # Replace near-zeros with epsilon * sign
            result = torch.where(zero_mask, signs * epsilon, result)

        return result

    def tensor_norm(self, tensor, dim=None, keepdim=False, context=None):
        """
        Zero-free HyperMorphic norm operation
        """
        _, _, epsilon = self.get_params(context)

        # Calculate normal L2 norm
        norm = torch.norm(tensor, p=2, dim=dim, keepdim=keepdim)

        # Apply zero-free guarantee
        zero_mask = norm < epsilon
        if zero_mask.any():
            norm = torch.where(zero_mask, epsilon, norm)

        return norm

    def tensor_dot(self, tensor_a, tensor_b, context=None):
        """
        HyperMorphic dot product with zero-free guarantee
        """
        # Perform dot product (element-wise multiplication and sum)
        result = torch.sum(self.tensor_mul(tensor_a, tensor_b, context), dim=-1)

        # Zero-free guarantee is already handled by tensor_mul
        return result

    def tensor_matmul(self, tensor_a, tensor_b, context=None):
        """
        HyperMorphic matrix multiplication with zero-free guarantee
        """
        # Get parameters
        base, modulus, epsilon = self.get_params(context)

        # Standard matrix multiplication
        result = torch.matmul(tensor_a, tensor_b)

        # Apply modulo modulus (differentiable way)
        result = result - modulus * torch.floor(result / modulus)

        # Zero-free guarantee
        zero_mask = torch.abs(result) < epsilon
        if zero_mask.any():
            # Replace near-zeros with epsilon (keeping sign or default to positive)
            signs = torch.sign(result)
            signs[signs == 0] = 1.0
            result = torch.where(zero_mask, signs * epsilon, result)

        return result

    def quantum_superposition(self, tensors, amplitudes, context=None):
        """
        Create a quantum superposition of tensor states with HyperMorphic properties

        Args:
            tensors: List of tensors to superpose
            amplitudes: Corresponding probability amplitudes

        Returns:
            Superposed tensor state
        """
        if len(tensors) != len(amplitudes):
            raise ValueError("Number of tensors must match number of amplitudes")

        # Normalize amplitudes according to HyperMorphic rules
        amplitudes_tensor = torch.tensor(amplitudes, device=tensors[0].device)
        norm = self.tensor_norm(amplitudes_tensor, context=context)
        normalized_amplitudes = self.tensor_div(amplitudes_tensor, norm, context=context)

        # Apply HyperMorphic superposition with modular arithmetic
        result = torch.zeros_like(tensors[0])

        for tensor, amplitude in zip(tensors, normalized_amplitudes):
            # Quantum amplitude-weighted tensor
            weighted = self.tensor_mul(tensor, amplitude, context=context)

            # Add to result
            result = self.tensor_add(result, weighted, context=context)

        return result

    def apply_holomorphic_transform(self, tensor, transform_fn, context=None):
        """
        Apply a holomorphic transformation to a tensor

        Args:
            tensor: Input tensor
            transform_fn: Function that preserves holomorphic structure
            context: Optional context tensor

        Returns:
            Transformed tensor with HyperMorphic guarantees
        """
        # Apply the provided transformation function
        transformed = transform_fn(tensor)

        # Ensure zero-free property
        _, _, epsilon = self.get_params(context)
        zero_mask = torch.abs(transformed) < epsilon

        if zero_mask.any():
            # Replace near-zeros with epsilon (keeping sign)
            signs = torch.sign(transformed)
            signs[signs == 0] = 1.0
            transformed = torch.where(zero_mask, signs * epsilon, transformed)

        return transformed


class HyperMorphicTensorOps:
    """
    Static utility class providing HyperMorphic tensor operations for
    systems that don't need the full nn.Module structure or dynamic parameters.
    """
    @staticmethod
    def ensure_zero_free(tensor, epsilon=1e-12):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        zero_mask = torch.abs(tensor) < epsilon
        if zero_mask.any():
            # Get signs, defaulting to 1.0 for zeros
            signs = torch.sign(tensor)
            signs[signs == 0] = 1.0

            # Replace near-zeros with epsilon * sign
            return torch.where(zero_mask, signs * epsilon, tensor)

        return tensor

    @staticmethod
    def tensor_add(tensor_a, tensor_b, base=1e3, epsilon=1e-12):
        """Simple HyperMorphic tensor addition without module hierarchy"""
        result = tensor_a + tensor_b
        result = result - base * torch.floor(result / base)
        return HyperMorphicTensorOps.ensure_zero_free(result, epsilon)

    @staticmethod
    def tensor_sub(tensor_a, tensor_b, base=1e3, epsilon=1e-12):
        """Simple HyperMorphic tensor subtraction without module hierarchy"""
        result = tensor_a - tensor_b
        result = result - base * torch.floor(result / base)
        return HyperMorphicTensorOps.ensure_zero_free(result, epsilon)

    @staticmethod
    def tensor_mul(tensor_a, tensor_b, modulus=997, epsilon=1e-12):
        """Simple HyperMorphic tensor multiplication without module hierarchy"""
        result = tensor_a * tensor_b
        result = result - modulus * torch.floor(result / modulus)
        return HyperMorphicTensorOps.ensure_zero_free(result, epsilon)


class HyperMorphicLinear(nn.Module):
    """
    HyperMorphic version of a linear layer with guaranteed zero-free operations.

    This layer applies HyperMorphic tensor operations during the forward pass,
    ensuring numerical stability with zero-free guarantees.
    """
    def __init__(self, in_features, out_features, bias=True, context_dim=0,
                 use_dynamic_params=True):
        super().__init__()

        self.in_features = in_features
        self.out_features = out_features
        self.use_dynamic_params = use_dynamic_params
        self.use_context = context_dim > 0

        # Standard weight and bias parameters
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.bias = nn.Parameter(torch.Tensor(out_features)) if bias else None

        # HyperMorphic tensor calculus module
        self.htc = HyperMorphicTensorCalculus(
            context_dim=context_dim if self.use_context else 1,
            learnable_params=use_dynamic_params
        )

        # Optional context projection if using context
        if self.use_context:
            self.context_proj = nn.Linear(context_dim, context_dim)

        self.reset_parameters()

    def reset_parameters(self):
        """Initialize parameters with Xavier uniform and ensure they're zero-free"""
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

        # Ensure zero-free initialization
        with torch.no_grad():
            # Get epsilon from HTC
            _, _, epsilon = self.htc.get_params()

            # Ensure weights are zero-free
            zero_mask = torch.abs(self.weight) < epsilon
            if zero_mask.any():
                self.weight.data[zero_mask] = epsilon

            # Ensure bias is zero-free if it exists
            if self.bias is not None:
                zero_mask = torch.abs(self.bias) < epsilon
                if zero_mask.any():
                    self.bias.data[zero_mask] = epsilon

    def forward(self, input, context=None):
        """
        Forward pass with HyperMorphic tensor operations

        Args:
            input: Input tensor [batch_size, in_features]
            context: Optional context tensor [batch_size, context_dim]

        Returns:
            Output tensor [batch_size, out_features]
        """
        # Process context if using it
        ctx = None
        if self.use_context and context is not None:
            ctx = self.context_proj(context)

        # Use HyperMorphic matrix multiplication
        output = self.htc.tensor_matmul(input, self.weight.t(), context=ctx)

        # Add bias if present
        if self.bias is not None:
            output = self.htc.tensor_add(output, self.bias, context=ctx)

        return output



import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np

class HyperMorphicFourierTransform(nn.Module):
    """
    HyperMorphic Fourier Transform (HFT) that implements a zero-free, dynamic Fourier
    transform framework tailored for HyperMorphic spaces.

    This module enables efficient frequency-domain processing, spectral filtering,
    and decomposition for advanced feature extraction and signal analysis in
    quantum-inspired neural architectures.
    """
    def __init__(self, epsilon=1e-12, learnable_epsilon=True):
        super().__init__()

        if learnable_epsilon:
            # Learnable epsilon for zero-free guarantees
            self.log_epsilon = nn.Parameter(torch.tensor(math.log(epsilon)))
        else:
            # Fixed epsilon
            self.register_buffer('epsilon', torch.tensor(epsilon))
            self.log_epsilon = None

        # Track transformation statistics
        self.register_buffer('forward_transform_count', torch.tensor(0))
        self.register_buffer('inverse_transform_count', torch.tensor(0))

    def get_epsilon(self):
        """Get current epsilon value (learnable or fixed)"""
        if self.log_epsilon is not None:
            return torch.exp(self.log_epsilon)
        else:
            return self.epsilon

    def forward_transform(self, x, dim=-1, normalized=True):
        """
        HyperMorphic Forward Fourier Transform (HFT)

        Args:
            x: Input tensor of any shape
            dim: Dimension along which to apply FFT (default: -1, last dimension)
            normalized: Whether to normalize the transform (default: True)

        Returns:
            Complex tensor representing frequency domain with zero-free guarantees
        """
        # Increment transform counter
        self.forward_transform_count += 1

        # Apply standard FFT
        x_freq = torch.fft.fft(x, dim=dim, norm='ortho' if normalized else None)

        # Apply HyperMorphic zero-free guarantee
        epsilon = self.get_epsilon()

        # Handle real and imaginary parts separately
        real_part = x_freq.real
        imag_part = x_freq.imag

        # Zero-free guarantee for real part
        real_near_zero = torch.abs(real_part) < epsilon
        if real_near_zero.any():
            # Replace near-zeros with epsilon (preserving sign)
            signs = torch.sign(real_part)
            signs[signs == 0] = 1.0  # Default to positive for exact zeros
            real_part = torch.where(real_near_zero, signs * epsilon, real_part)

        # Zero-free guarantee for imaginary part
        imag_near_zero = torch.abs(imag_part) < epsilon
        if imag_near_zero.any():
            # Replace near-zeros with epsilon (preserving sign)
            signs = torch.sign(imag_part)
            signs[signs == 0] = 1.0  # Default to positive for exact zeros
            imag_part = torch.where(imag_near_zero, signs * epsilon, imag_part)

        # Reconstruct complex tensor with zero-free parts
        x_freq = torch.complex(real_part, imag_part)

        return x_freq

    def inverse_transform(self, x_freq, dim=-1, normalized=True):
        """
        HyperMorphic Inverse Fourier Transform (IHFT)

        Args:
            x_freq: Complex tensor in frequency domain
            dim: Dimension along which to apply IFFT (default: -1, last dimension)
            normalized: Whether the transform is normalized (default: True)

        Returns:
            Real tensor representing time/space domain with zero-free guarantees
        """
        # Increment transform counter
        self.inverse_transform_count += 1

        # Apply standard IFFT
        x = torch.fft.ifft(x_freq, dim=dim, norm='ortho' if normalized else None)

        # For inverse transform, we generally expect real output
        # Get real part (with small imaginary parts discarded)
        x_real = x.real

        # Apply HyperMorphic zero-free guarantee
        epsilon = self.get_epsilon()
        zero_mask = torch.abs(x_real) < epsilon

        if zero_mask.any():
            # Replace near-zeros with epsilon (preserving sign)
            signs = torch.sign(x_real)
            signs[signs == 0] = 1.0  # Default to positive for exact zeros
            x_real = torch.where(zero_mask, signs * epsilon, x_real)

        return x_real

    def forward(self, x, inverse=False, dim=-1, normalized=True):
        """
        Unified forward/inverse transform interface

        Args:
            x: Input tensor
            inverse: Whether to apply inverse transform (default: False)
            dim: Dimension along which to apply transform (default: -1)
            normalized: Whether to normalize the transform (default: True)

        Returns:
            Transformed tensor with zero-free guarantees
        """
        if inverse:
            return self.inverse_transform(x, dim=dim, normalized=normalized)
        else:
            return self.forward_transform(x, dim=dim, normalized=normalized)

    def spectral_power(self, x_freq):
        """
        Calculate zero-free spectral power from frequency domain tensor

        Args:
            x_freq: Complex tensor in frequency domain

        Returns:
            Power spectrum with zero-free guarantees
        """
        # Calculate power as squared magnitude
        power = torch.abs(x_freq) ** 2

        # Apply zero-free guarantee
        epsilon = self.get_epsilon()
        zero_mask = power < epsilon

        if zero_mask.any():
            power = torch.where(zero_mask, epsilon, power)

        return power

    def spectral_phase(self, x_freq):
        """
        Calculate zero-free spectral phase from frequency domain tensor

        Args:
            x_freq: Complex tensor in frequency domain

        Returns:
            Phase spectrum with zero-free guarantees
        """
        # Get phase angles
        phase = torch.angle(x_freq)

        # No need for zero-free guarantee here as angles are always defined
        # However, we ensure stability by detecting NaNs
        nan_mask = torch.isnan(phase)

        if nan_mask.any():
            # Replace NaNs with zeros
            phase = torch.where(nan_mask, torch.zeros_like(phase), phase)

        return phase


class SpectralFilter(nn.Module):
    """
    Learnable spectral filter for HyperMorphic Fourier Analysis

    This module implements a learnable filter in the frequency domain
    that can adaptively process signals and emphasize/attenuate specific
    frequency components.
    """
    def __init__(self, freq_dim, filter_type='learnable', init_mode='gaussian',
                 init_params=None, min_filter_value=1e-6):
        super().__init__()

        self.freq_dim = freq_dim
        self.filter_type = filter_type
        self.min_filter_value = min_filter_value

        # Create filter parameters based on type
        if filter_type == 'learnable':
            # Fully learnable filter
            if init_mode == 'gaussian':
                # Initialize with Gaussian (bell curve)
                filter_init = self._init_gaussian_filter(freq_dim, init_params)
            elif init_mode == 'butterworth':
                # Initialize with Butterworth filter
                filter_init = self._init_butterworth_filter(freq_dim, init_params)
            elif init_mode == 'uniform':
                # Initialize with uniform values
                filter_init = torch.ones(freq_dim)
            else:
                # Default to Gaussian
                filter_init = self._init_gaussian_filter(freq_dim, init_params)

            # Create learnable parameter
            self.filter_weights = nn.Parameter(filter_init)

        elif filter_type == 'parameterized_gaussian':
            # Parameterized Gaussian filter (learn center and width)
            center, width = 0.5, 0.2
            if init_params:
                center = init_params.get('center', center)
                width = init_params.get('width', width)

            # Create learnable parameters
            self.center = nn.Parameter(torch.tensor(center))
            self.width = nn.Parameter(torch.tensor(width))
            self.amplitude = nn.Parameter(torch.tensor(1.0))
            self.register_buffer('freq_indices', torch.linspace(0, 1, freq_dim))

        elif filter_type == 'parameterized_butterworth':
            # Parameterized Butterworth filter (learn cutoff and order)
            cutoff, order = 0.5, 2.0
            if init_params:
                cutoff = init_params.get('cutoff', cutoff)
                order = init_params.get('order', order)

            # Create learnable parameters
            self.cutoff = nn.Parameter(torch.tensor(cutoff))
            self.order = nn.Parameter(torch.tensor(order))
            self.register_buffer('freq_indices', torch.linspace(0, 1, freq_dim))

        else:
            raise ValueError(f"Unknown filter type: {filter_type}")

    def _init_gaussian_filter(self, size, params=None):
        """Initialize a Gaussian filter"""
        # Default parameters
        center = 0.5
        width = 0.2

        # Use provided parameters if available
        if params:
            center = params.get('center', center)
            width = params.get('width', width)

        # Create filter
        x = torch.linspace(0, 1, size)
        gauss = torch.exp(-((x - center) ** 2) / (2 * width ** 2))

        return gauss

    def _init_butterworth_filter(self, size, params=None):
        """Initialize a Butterworth filter"""
        # Default parameters
        cutoff = 0.5
        order = 2.0





class EntanglementUnit(nn.Module):
    """
    Quantum-inspired entanglement unit that creates non-local correlations
    between input features, simulating quantum entanglement.
    """
    def __init__(self, embed_dim):
        super().__init__()
        self.embed_dim = embed_dim

        # Non-local correlation generator
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)

        # Mixing layer
        self.mixer = nn.Linear(embed_dim, embed_dim)

        # Phase modulator
        self.phase = nn.Parameter(torch.rand(1) * 2 * math.pi)

    def forward(self, x):
        """Create quantum-inspired entanglement"""
        batch_size = x.shape[0]

        # Project to query, key, value spaces
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        # Calculate non-local correlations (similar to attention)
        corr = torch.matmul(q, k.transpose(-2, -1)) / (self.embed_dim ** 0.5)
        corr = F.softmax(corr, dim=-1)

        # Apply phase modulation (quantum interference)
        phase_factor = torch.cos(self.phase) + 1j * torch.sin(self.phase)

        # Use only real part for simplicity
        entangled = torch.matmul(corr, v) * torch.cos(self.phase)

        # Mix for final output
        output = self.mixer(entangled)

        return output


class HyperMorphicQuantumHolographicMemory(nn.Module):
    """
    Quantum-inspired holographic memory system that stores and retrieves
    information using distributed holographic representations.
    """
    def __init__(self, embedding_dim, memory_dim=1024, num_memory_planes=8, epsilon=1e-12):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.memory_dim = memory_dim
        self.num_memory_planes = num_memory_planes
        self.epsilon = epsilon

        # Encoder from embedding space to holographic space
        self.encoder = nn.Linear(embedding_dim, memory_dim)

        # Decoder from holographic space to embedding space
        self.decoder = nn.Linear(memory_dim, embedding_dim)

        # Holographic medium (multiple planes for more capacity)
        # Each plane stores distributed holographic patterns
        self.holographic_medium = nn.Parameter(
            torch.zeros(num_memory_planes, memory_dim)
        )

        # Key generator for addressing the memory
        self.key_generator = nn.Linear(embedding_dim, num_memory_planes)

        # Memory usage tracker
        self.register_buffer('memory_usage', torch.zeros(num_memory_planes))
        self.register_buffer('memory_importance', torch.ones(num_memory_planes))
        self.register_buffer('write_count', torch.tensor(0))

        # Initialize holographic medium with small random values
        nn.init.normal_(self.holographic_medium, std=0.01)

    def store(self, x, importance=1.0):
        """Store information in holographic memory"""
        # Encode input to holographic space
        encoded = self.encoder(x)

        # Generate addressing key (which planes to write to)
        key = torch.sigmoid(self.key_generator(x.detach().mean(dim=0) if x.dim() > 1 else x))

        # Scale by importance
        importance_factor = torch.tensor(importance, device=x.device)

        # Update holographic medium (additive storage)
        with torch.no_grad():
            # Get least used planes if memory is saturated
            if self.write_count > 100:
                usage_factor = 1.0 - F.softmax(-self.memory_usage, dim=0)
                importance_factor = self.memory_importance * importance
                write_mask = usage_factor * importance_factor
            else:
                write_mask = key

            # Write to holographic medium
            for i in range(self.num_memory_planes):
                # Update plane with new encoded value (associative write)
                self.holographic_medium.data[i] += encoded.mean(dim=0) * write_mask[i] * 0.1

                # Update usage statistics
                self.memory_usage[i] += write_mask[i]
                self.memory_importance[i] = 0.9 * self.memory_importance[i] + 0.1 * importance

            self.write_count += 1

    def recall(self, x):
        """Recall information from holographic memory"""
        # Generate addressing key (which planes to read from)
        key = torch.sigmoid(self.key_generator(x.detach().mean(dim=0) if x.dim() > 1 else x))

        # Read from holographic medium (weighted by key)
        read_out = torch.zeros(self.memory_dim, device=x.device)
        for i in range(self.num_memory_planes):
            read_out += self.holographic_medium[i] * key[i]

        # Decode from holographic space to embedding space
        decoded = self.decoder(read_out)

        # Ensure same shape as input for easy integration
        if x.dim() > 1:
            batch_size = x.shape[0]
            decoded = decoded.unsqueeze(0).expand(batch_size, -1)

        # Replace any exact zeros with small value
        zero_mask = torch.abs(decoded) < self.epsilon
        if zero_mask.any():
            decoded = decoded.clone()
            decoded[zero_mask] = self.epsilon

        return decoded

    def clear(self):
        """Clear the holographic memory"""
        with torch.no_grad():
            self.holographic_medium.zero_()
            self.memory_usage.zero_()
            self.memory_importance.fill_(1.0)
            self.write_count.zero_()


class ZeroPointEnergyModulator(nn.Module):
    """
    Zero-Point Energy Modulator - simulates quantum vacuum energy modulation
    to control global system parameters and enable quantum-like transitions.
    """
    def __init__(self, system_dim, num_energy_states=5, epsilon=1e-12):
        super().__init__()
        self.system_dim = system_dim
        self.num_energy_states = num_energy_states
        self.epsilon = epsilon

        # Energy state levels (quantum-like discrete energy states)
        self.energy_levels = nn.Parameter(torch.linspace(0.1, 0.9, num_energy_states))

        # Energy state phase factors (for quantum interference)
        self.energy_phases = nn.Parameter(torch.rand(num_energy_states) * 2 * math.pi)

        # Energy state definitions (what each state does to the system)
        self.energy_state_definitions = nn.Parameter(
            torch.randn(num_energy_states, system_dim)
        )

        # System state controller (maps input state to energy distribution)
        self.state_controller = nn.Sequential(
            nn.Linear(system_dim, system_dim // 2),
            nn.SiLU(),
            nn.Linear(system_dim // 2, num_energy_states)
        )

        # Current energy state (for tracking)
        self.register_buffer('current_energy_state', torch.zeros(num_energy_states))

        # Initialize state definitions
        with torch.no_grad():
            # Normalize energy state definitions
            self.energy_state_definitions.data = F.normalize(
                self.energy_state_definitions.data, dim=1
            )

    def modulate(self, x, external_reward=None):
        """
        Modulate input with zero-point energy fluctuations

        Args:
            x: Input tensor
            external_reward: Optional reward signal to bias energy states

        Returns:
            Modulated tensor
        """
        # Determine energy state distribution from input
        energy_distribution = F.softmax(self.state_controller(x), dim=-1)

        # Update with external reward if provided
        if external_reward is not None:
            reward_tensor = torch.tensor(external_reward, device=x.device).expand(self.num_energy_states)
            energy_distribution = energy_distribution * (1.0 + 0.2 * reward_tensor)
            energy_distribution = F.normalize(energy_distribution, p=1, dim=-1)

        # Store current energy state
        self.current_energy_state = energy_distribution.detach().mean(dim=0) if energy_distribution.dim() > 1 else energy_distribution.detach()

        # Apply energy modulation
        modulated = x.clone()

        # Apply each energy state effect proportional to its probability
        for i in range(self.num_energy_states):
            # Get state intensity and phase
            intensity = energy_distribution[..., i].unsqueeze(-1) if energy_distribution.dim() > 1 else energy_distribution[i]
            phase = self.energy_phases[i]
            energy_level = self.energy_levels[i]
            state_def = self.energy_state_definitions[i]

            # Phase modulation component (quantum-like interference)
            phase_factor = torch.cos(phase)

            # Apply energy state effect
            if intensity.dim() < state_def.dim():
                intensity = intensity.unsqueeze(-1)

            # Different application based on input dimensions
            if modulated.dim() == 2:  # Batch dimension present
                state_effect = intensity * energy_level * phase_factor * state_def.unsqueeze(0)
            else:
                state_effect = intensity * energy_level * phase_factor * state_def

            modulated = modulated + state_effect

        # Replace any exact zeros with small value
        zero_mask = torch.abs(modulated) < self.epsilon
        if zero_mask.any():
            modulated = modulated.clone()
            modulated[zero_mask] = self.epsilon

        return modulated

    def get_energy_state(self):
        """Get the current energy state information"""
        state_values = self.current_energy_state.detach().cpu().numpy()
        dominant_state = torch.argmax(self.current_energy_state).item()

        return {
            "state_distribution": state_values,
            "dominant_state": dominant_state,
            "energy_levels": self.energy_levels.detach().cpu().numpy(),
            "phases": self.energy_phases.detach().cpu().numpy()
        }

    def set_energy_bias(self, bias_vector):
        """Set a bias on the energy states (useful for controlled experiments)"""
        with torch.no_grad():
            # Adjust energy levels
            bias_tensor = torch.tensor(bias_vector, device=self.energy_levels.device)


def set_energy_bias(self, bias_vector):
        """Set a bias on the energy states (useful for controlled experiments)"""
        with torch.no_grad():
            # Adjust energy levels
            bias_tensor = torch.tensor(bias_vector, device=self.energy_levels.device)

            # Apply bias to energy levels (clamped to valid range)
            self.energy_levels.data = torch.clamp(
                self.energy_levels.data + bias_tensor * 0.1,
                min=0.05, max=0.95
            )


class QuantumWaveFunctionCollapse(nn.Module):
    """
    Implements a neural simulation of quantum wave function collapse,
    allowing the network to maintain superpositions of states until
    an observation forces collapse to a specific state.
    """
    def __init__(self, state_dim, num_eigenstates=8, collapse_threshold=0.7):
        super().__init__()
        self.state_dim = state_dim
        self.num_eigenstates = num_eigenstates
        self.collapse_threshold = collapse_threshold

        # Eigenstate basis (possible states after collapse)
        self.eigenstate_basis = nn.Parameter(
            torch.randn(num_eigenstates, state_dim)
        )

        # Normalize eigenstates to unit vectors
        with torch.no_grad():
            self.eigenstate_basis.data = F.normalize(
                self.eigenstate_basis.data, dim=1
            )

        # Amplitude projector (maps input to probability amplitudes)
        self.amplitude_projector = nn.Sequential(
            nn.Linear(state_dim, state_dim // 2),
            nn.SiLU(),
            nn.Linear(state_dim // 2, num_eigenstates)
        )

        # Phase factors for each eigenstate
        self.phase_factors = nn.Parameter(torch.rand(num_eigenstates) * 2 * math.pi)

        # Track last collapsed state
        self.register_buffer('last_collapsed_state', torch.zeros(state_dim))
        self.register_buffer('collapse_count', torch.tensor(0))

    def forward(self, x, force_collapse=False, collapse_prob=None):
        """
        Process input with quantum superposition.

        Args:
            x: Input tensor
            force_collapse: Whether to force wavefunction collapse
            collapse_prob: Optional probability override for collapse

        Returns:
            Processed tensor (superposition or collapsed)
        """
        batch_size = x.shape[0] if x.dim() > 1 else 1

        # Project input to probability amplitudes for each eigenstate
        amplitudes = self.amplitude_projector(x)

        # Apply softmax to get proper probability distribution
        probabilities = F.softmax(amplitudes, dim=-1)

        # Calculate superposition state using probability amplitudes and phases
        superposition = torch.zeros(batch_size, self.state_dim, device=x.device)
        for i in range(self.num_eigenstates):
            # Get amplitude and apply phase
            amplitude = probabilities[..., i].unsqueeze(-1)
            phase = self.phase_factors[i]
            eigenstate = self.eigenstate_basis[i]

            # Apply complex phase (using real representation for simplicity)
            phase_factor = torch.cos(phase) + 1j * torch.sin(phase)

            # Add contribution to superposition (use real part only)
            superposition += amplitude * eigenstate.unsqueeze(0) * torch.cos(phase)

        # Check for collapse conditions
        collapse_occurred = False

        # Determine if collapse should occur
        if force_collapse:
            collapse_occurred = True
        else:
            # Check if any probability exceeds threshold
            max_prob, max_idx = torch.max(probabilities, dim=-1)
            if collapse_prob is None:
                collapse_occurred = torch.any(max_prob > self.collapse_threshold)
            else:
                collapse_occurred = random.random() < collapse_prob

        # Handle collapse if it occurred
        if collapse_occurred:
            with torch.no_grad():
                # Get most likely eigenstate index for each sample
                _, max_indices = torch.max(probabilities, dim=-1)

                # Collapse to corresponding eigenstate
                collapsed = torch.zeros_like(superposition)
                for b in range(batch_size):
                    idx = max_indices[b] if batch_size > 1 else max_indices
                    collapsed[b] = self.eigenstate_basis[idx]

                # Store last collapsed state (average if batch)
                self.last_collapsed_state = collapsed.mean(dim=0) if batch_size > 1 else collapsed
                self.collapse_count += 1

                return collapsed

        return superposition

    def get_collapse_stats(self):
        """Get statistics about collapses"""
        return {
            "collapse_count": self.collapse_count.item(),
            "last_collapsed_state": self.last_collapsed_state.detach().cpu().numpy()
        }


class QuantumNonLocalityModule(nn.Module):
    """
    Implements quantum-inspired non-locality, allowing the model to establish
    correlations across spatial or temporal distances that would be impossible
    in classical systems.
    """
    def __init__(self, embed_dim, num_correlation_pairs=4):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_correlation_pairs = num_correlation_pairs

        # Non-local correlation pairs
        # Each pair represents two points that will be entangled
        self.correlation_pairs = nn.ParameterList([
            nn.Parameter(torch.randn(2, embed_dim))
            for _ in range(num_correlation_pairs)
        ])

        # Correlation strength controller
        self.correlation_strength = nn.Parameter(torch.rand(num_correlation_pairs))

        # Bell state type for each pair (4 types of Bell states)
        self.bell_state_type = nn.Parameter(torch.randint(0, 4, (num_correlation_pairs,)).float())

        # Measurement basis projectors
        self.measurement_projectors = nn.ModuleList([
            nn.Linear(embed_dim, embed_dim)
            for _ in range(num_correlation_pairs)
        ])

    def forward(self, x):
        """Apply non-local quantum correlations"""
        batch_size, seq_len, _ = x.shape

        # Create correlation mask for the sequence
        corr_mask = torch.eye(seq_len, device=x.device)

        # Apply each correlation pair
        for i in range(self.num_correlation_pairs):
            # Get correlation parameters
            strength = torch.sigmoid(self.correlation_strength[i])
            bell_type = torch.floor(self.bell_state_type[i]).long() % 4

            # Choose correlation pattern based on Bell state type
            if bell_type == 0:  # Φ+
                sign_a, sign_b = 1, 1
            elif bell_type == 1:  # Φ-
                sign_a, sign_b = 1, -1
            elif bell_type == 2:  # Ψ+
                sign_a, sign_b = -1, 1
            else:  # Ψ-
                sign_a, sign_b = -1, -1

            # Create non-local correlation
            for a in range(seq_len):
                for b in range(a + 1, seq_len):  # Only upper triangle
                    # Skip if correlation already strong
                    if corr_mask[a, b] > 0.5:
                        continue

                    # Determine correlation based on measurement in specific basis
                    proj_a = self.measurement_projectors[i](x[:, a])
                    proj_b = self.measurement_projectors[i](x[:, b])

                    # Calculate correlation
                    correlation = F.cosine_similarity(proj_a, proj_b, dim=1).mean()

                    # Apply quantum correlation (Bell-state like)
                    if correlation > 0.7:
                        corr_mask[a, b] = strength
                        corr_mask[b, a] = strength * sign_a * sign_b

        # Apply the correlation mask
        mixed = torch.zeros_like(x)
        for b in range(batch_size):
            mixed[b] = torch.matmul(corr_mask, x[b])

        # Mix original with correlated version
        result = x + mixed * 0.1

        return result


class QuantumSuperpositionLayer(nn.Module):
    """
    Implements a neural layer that maintains multiple computations in
    superposition, inspired by quantum parallelism.
    """
    def __init__(self, input_dim, hidden_dim, num_pathways=4):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_pathways = num_pathways

        # Multiple parallel transformation pathways
        self.pathways = nn.ModuleList([
            nn.Sequential(
                nn.Linear(input_dim, hidden_dim),
                nn.SiLU(),
                nn.Linear(hidden_dim, input_dim)
            )
            for _ in range(num_pathways)
        ])

        # Pathway selection network (determines amplitude for each path)
        self.pathway_selector = nn.Sequential(
            nn.Linear(input_dim, hidden_dim // 2),
            nn.SiLU(),
            nn.Linear(hidden_dim // 2, num_pathways)
        )

        # Phase factors for interference
        self.phase_factors = nn.Parameter(torch.rand(num_pathways) * 2 * math.pi)

    def forward(self, x):
        """Process input through superposition of multiple pathways"""
        # Calculate amplitudes for each pathway
        amplitudes = F.softmax(self.pathway_selector(x), dim=-1)

        # Initialize superposition
        superposition = torch.zeros_like(x)

        # Process through each pathway with its amplitude
        for i in range(self.num_pathways):
            # Get pathway output
            pathway_output = self.pathways[i](x)

            # Get amplitude and phase
            amplitude = amplitudes[..., i].unsqueeze(-1)
            phase = self.phase_factors[i]

            # Add to superposition with phase factor
            superposition += amplitude * pathway_output * torch.cos(phase)

        return superposition


class QuantumUncertaintyModule(nn.Module):
    """
    Implements Heisenberg uncertainty principle for neural network
    activations, creating a trade-off between precision in different
    activation dimensions.
    """
    def __init__(self, embed_dim, num_conjugate_pairs=4, uncertainty_factor=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_conjugate_pairs = num_conjugate_pairs
        self.uncertainty_factor = uncertainty_factor

        # Ensure even dimension
        assert embed_dim % (2 * num_conjugate_pairs) == 0, "Embed dim must be divisible by 2*num_conjugate_pairs"

        # Create conjugate variable pairs
        # Each pair represents dimensions that trade precision (like position/momentum)
        dim_per_pair = embed_dim // num_conjugate_pairs
        self.conjugate_pairs = []

        for i in range(num_conjugate_pairs):
            start_idx = i * dim_per_pair
            mid_idx = start_idx + dim_per_pair // 2
            end_idx = start_idx + dim_per_pair

            self.conjugate_pairs.append((
                list(range(start_idx, mid_idx)),
                list(range(mid_idx, end_idx))
            ))

        # Uncertainty coupling strength
        self.coupling_strength = nn.Parameter(torch.rand(num_conjugate_pairs))

    def forward(self, x):
        """Apply uncertainty principle to activation patterns"""
        result = x.clone()

        # Process each conjugate pair
        for i, (dims_a, dims_b) in enumerate(self.conjugate_pairs):
            # Get coupling strength for this pair
            strength = torch.sigmoid(self.coupling_strength[i]) * self.uncertainty_factor

            # Calculate precision for first dimension group
            precision_a = torch.std(x[..., dims_a], dim=-1, keepdim=True)

            # Calculate corresponding uncertainty for conjugate dimensions
            uncertainty_b = strength / torch.max(precision_a, torch.tensor(1e-8, device=x.device))

            # Apply uncertainty to conjugate dimensions
            noise_b = torch.randn_like(result[..., dims_b]) * uncertainty_b
            result[..., dims_b] = result[..., dims_b] + noise_b

            # The reverse direction (b affects a)
            precision_b = torch.std(x[..., dims_b], dim=-1, keepdim=True)
            uncertainty_a = strength / torch.max(precision_b, torch.tensor(1e-8, device=x.device))
            noise_a = torch.randn_like(result[..., dims_a]) * uncertainty_a
            result[..., dims_a] = result[..., dims_a] + noise_a

        return result


class QuantumTunnelingLayer(nn.Module):
    """
    Implements quantum tunneling for neural networks, allowing signals to
    bypass or "tunnel through" traditional barriers in activation space.
    """
    def __init__(self, embed_dim, barrier_threshold=0.5, tunnel_probability=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.barrier_threshold = barrier_threshold
        self.tunnel_probability = tunnel_probability

        # Barrier detectors (detect activation barriers in the input)
        self.barrier_detector = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.Tanh(),
            nn.Linear(embed_dim // 2, embed_dim),
            nn.Sigmoid()
        )

        # Tunneling projector (determines tunneling paths)
        self.tunnel_projector = nn.Linear(embed_dim, embed_dim)

        # Tunneling efficiency (by feature dimension)
        self.tunnel_efficiency = nn.Parameter(torch.rand(embed_dim))

    def forward(self, x):
        """Apply quantum tunneling to bypass activation barriers"""
        # Detect barriers in activation space
        barriers = self.barrier_detector(x)

        # Identify where barriers exist
        barrier_mask = (barriers > self.barrier_threshold).float()

        # Compute tunneling pathways
        tunnel_paths = torch.tanh(self.tunnel_projector(x))

        # Tunneling probability for each dimension
        tunnel_prob = torch.sigmoid(self.tunnel_efficiency)

        # Apply tunneling (bypass barriers with some probability)
        tunnel_mask = (torch.rand_like(x) < tunnel_prob).float()
        tunneled = x + tunnel_paths * barrier_mask * tunnel_mask * self.tunnel_probability

        return tunneled





def integrate_quantum_components():
    """
    Integrate the new quantum components into the existing QuantumNexusModel.
    """
    global model, optimizer, agent_instance

    log_event("Integrating advanced quantum neural components...", "QUANTUM")

    # 1. Create enhanced model (replace existing model)
    input_dim = model.embed_dim if hasattr(model, 'embed_dim') else 512
    vocab_size = 30522  # Default vocabulary size
    old_layers = len(model.neocortex) if hasattr(model, 'neocortex') else 8

    # Create new enhanced model
    enhanced_model = EnhancedQuantumNexusModel(
        vocab_size=vocab_size,
        embed_dim=input_dim,
        num_layers=old_layers,
        num_quantum_states=4
    )

    # 2. Transfer weights from old model to new model
    if hasattr(model, 'state_dict'):
        # Get state dictionary from old model
        old_state_dict = model.state_dict()

        # Get state dictionary from new model
        new_state_dict = enhanced_model.state_dict()

        # Copy matching parameters
        for key in old_state_dict:
            if key in new_state_dict and old_state_dict[key].shape == new_state_dict[key].shape:
                new_state_dict[key] = old_state_dict[key]

        # Load updated state dictionary to new model
        enhanced_model.load_state_dict(new_state_dict, strict=False)

        # Transfer learning rate
        if hasattr(model, '_current_lr'):
            enhanced_model._current_lr = model._current_lr

    # 3. Replace model in agent and other components
    model = enhanced_model

    # Update agent model
    if agent_instance and hasattr(agent_instance, 'model'):
        agent_instance.model = model

    # Update AI Manager model
    if agent_instance and hasattr(agent_instance, 'ai_manager'):
        agent_instance.ai_manager.model = model

    # 4. Create new optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=getattr(model, '_current_lr', 5e-5))

    # 5. Create QuantumStrategySynthesizer for standalone use
    strategy_synthesizer = QuantumStrategySynthesizer(
        input_dim=input_dim,
        output_dim=input_dim,
        embedding_dim=input_dim,
        num_layers=128
    )

    # Store for global access
    global quantum_strategy_synthesizer
    quantum_strategy_synthesizer = strategy_synthesizer

    log_event("Advanced quantum neural components successfully integrated!", "QUANTUM")

    return {
        "model": model,
        "optimizer": optimizer,
        "strategy_synthesizer": strategy_synthesizer
    }


# Example of using the QuantumStrategySynthesizer directly
def generate_quantum_strategy(input_tensor):
    """
    Generate a quantum strategy for a given input.

    Args:
        input_tensor: Input tensor (batch_size, input_dim)

    Returns:
        Strategy tensor, strategy embedding
    """
    global quantum_strategy_synthesizer

    if quantum_strategy_synthesizer is None:
        log_event("Quantum Strategy Synthesizer not initialized", "ERROR")
        return None, None

    # Generate strategy
    with torch.no_grad():
        strategy, embedding = quantum_strategy_synthesizer(input_tensor, store_memory=True)

    # Update dynamic components occasionally
    if random.random() < 0.1:
        quantum_strategy_synthesizer.update_dynamic_components()

    # Analyze strategy patterns
    analysis = quantum_strategy_synthesizer.get_strategy_analysis()
    log_event(f"Quantum strategy analysis: {analysis}", "QUANTUM")

    return strategy, embedding


class CheckpointTracker:
    """
    Checkpoint tracking system that records information about model checkpoints
    and provides visualization capabilities through the Flask dashboard.
    """
    def __init__(self):
        self.checkpoints = []
        self.checkpoint_dir = "/content/gdrive/MyDrive/quantum_nexus/checkpoints"
        self.latest_checkpoint = None
        self.display_html = None  # HTML cache for dashboard display

    def record_checkpoint(self, checkpoint_path, model, agent, cycle_count):
        """
        Record information about a saved checkpoint

        Parameters:
        - checkpoint_path: Path where checkpoint was saved
        - model: The model object that was saved
        - agent: The agent object that was saved
        - cycle_count: Current cycle count
        """
        timestamp = datetime.now().isoformat()

        # Extract model parameters
        model_info = {
            "type": type(model).__name__,
            "num_layers": len(model.neocortex) if hasattr(model, "neocortex") else "unknown",
            "embed_dim": getattr(model, "embed_dim", "unknown"),
            "learning_rate": getattr(model, "_current_lr", "unknown"),
            "parameter_count": sum(p.numel() for p in model.parameters()) if hasattr(model, "parameters") else "unknown"
        }

        # Extract agent stats
        agent_stats = {}
        if hasattr(agent, "stats"):
            agent_stats = dict(agent.stats)  # Convert defaultdict to regular dict

        # Record consciousness level if available
        consciousness_level = None
        if (hasattr(agent, "ai_manager") and
            hasattr(agent.ai_manager, "consciousness") and
            hasattr(agent.ai_manager.consciousness, "awareness_level")):
            consciousness_level = float(agent.ai_manager.consciousness.awareness_level)

        # Create checkpoint record
        checkpoint_info = {
            "path": checkpoint_path,
            "timestamp": timestamp,
            "cycle_count": cycle_count,
            "model_info": model_info,
            "agent_stats": agent_stats,
            "consciousness_level": consciousness_level,
            "filename": os.path.basename(checkpoint_path)
        }

        self.checkpoints.append(checkpoint_info)
        self.latest_checkpoint = checkpoint_info

        # Update HTML display
        self._update_display_html()

        log_event(f"Checkpoint recorded: {checkpoint_path} at cycle {cycle_count}", "INFO")
        return checkpoint_info

    def _update_display_html(self):
        """Generate HTML display for the dashboard"""
        html = """
        <div style="border: 1px solid #4CAF50; background-color: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 8px;">
            <h2 style="color: #4CAF50; margin-top: 0;">Checkpoint Tracker</h2>
        """

        # Latest checkpoint info
        if self.latest_checkpoint:
            cp = self.latest_checkpoint
            html += f"""
            <div style="background-color: #e8f5e9; padding: 10px; border-radius: 6px; margin-bottom: 15px;">
                <h3 style="margin-top: 0; color: #2E7D32;">Latest Checkpoint</h3>
                <p><strong>File:</strong> {cp['filename']}</p>
                <p><strong>Path:</strong> {cp['path']}</p>
                <p><strong>Timestamp:</strong> {cp['timestamp']}</p>
                <p><strong>Cycle:</strong> {cp['cycle_count']}</p>
                <p><strong>Model:</strong> {cp['model_info']['type']} with {cp['model_info']['num_layers']} layers,
                   embed_dim={cp['model_info']['embed_dim']}, params={cp['model_info']['parameter_count']}</p>
                <p><strong>Learning Rate:</strong> {cp['model_info']['learning_rate']}</p>
                <p><strong>Consciousness Level:</strong> {cp['consciousness_level']}</p>
            </div>
            """

        # Checkpoint history
        html += "<h3>Checkpoint History</h3>"
        html += "<div style='max-height: 300px; overflow-y: auto;'>"
        html += "<table style='width: 100%; border-collapse: collapse;'>"
        html += "<tr style='background-color: #4CAF50; color: white;'><th>Time</th><th>Cycle</th><th>File</th><th>Layers</th><th>Learning Rate</th></tr>"

        # Color alternating rows
        for i, cp in enumerate(reversed(self.checkpoints)):
            row_style = "background-color: #f2f2f2;" if i % 2 == 0 else ""
            html += f"""
            <tr style='{row_style}'>
                <td>{cp['timestamp'].split('T')[1].split('.')[0]}</td>
                <td>{cp['cycle_count']}</td>
                <td>{cp['filename']}</td>
                <td>{cp['model_info']['num_layers']}</td>
                <td>{cp['model_info']['learning_rate']}</td>
            </tr>
            """

        html += "</table></div></div>"
        self.display_html = html

    def get_display_html(self):
        """Get HTML display for dashboard"""
        if not self.display_html:
            self._update_display_html()
        return self.display_html

    def get_checkpoint_count(self):
        """Get total number of recorded checkpoints"""
        return len(self.checkpoints)

    def get_latest_checkpoint(self):
        """Get information about the latest checkpoint"""
        return self.latest_checkpoint

# Create a singleton instance
checkpoint_tracker = CheckpointTracker()


# Configuration - UPDATED PATHS for Google Drive checkpointing
MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
GOOGLE_DRIVE_MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
LOCAL_MODEL_SAVE_PATH = "quantum_nexus_model.pth" # Local backup path
LOG_FILE = "quantum_nexus_log.txt"
FLASK_PORT = 5012
AGENT_STATE_FILE = "quantum_nexus_state.json" # No longer used - state is part of checkpoint
GOOGLE_DRIVE_STATE_FILE = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_state.json" # Checkpoint path for agent state (also in checkpoint now)
SELF_MODIFY_INTERVAL = 20
ANNEAL_GAMMA = 0.995
MEMORY_MAX_SIZE = 1000
SAVE_INTERVAL = 5 # Save checkpoint more frequently for longer runs

# =============================================================================
# MAIN EXECUTION
# =============================================================================
adaptive_learning = None

def enhanced_main_loop():
    """
    Enhanced main execution loop with improved error handling, recovery mechanisms,
    and checkpointing reliability.
    """
    global adaptive_learning, agent_instance

    # Keep track of system state
    system_status = {
        "consecutive_errors": 0,
        "max_consecutive_errors": 5,
        "critical_error_count": 0,
        "last_successful_cycle": 0,
        "emergency_mode": False,
        "recovery_attempts": 0
    }

    # Initialize components with comprehensive error handling
    try:
        # 1. Load or create model
        log_event("Initializing Quantum Nexus model...", "INFO")
        model = load_or_create_model()

        # 2. Create optimizer
        import torch.optim as optim
        optimizer = optim.Adam(model.parameters(), lr=getattr(model, '_current_lr', 5e-5))
        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=ANNEAL_GAMMA)

        # 3. Initialize primary agent
        agent = QuantumNexusAgent(model=model)
        agent_instance = agent  # Store for dashboard access

        # 4. Create core subsystems
        # Initialize free will
        free_will = SuperQuantumFreeWill(agent=agent)
        agent.free_will = free_will

        # Initialize AI Manager, which creates other subsystems
        ai_manager = AIManager(agent, model)
        agent.ai_manager = ai_manager

        # Initialize adaptive learning - ensure we use model's current_lr if available
        current_lr = getattr(model, '_current_lr', 5e-5)
        adaptive_learning = AdaptiveLearningSystem(model)
        # Ensure learning rate history is initialized with current value
        if not adaptive_learning.learning_rate_history:
            adaptive_learning.learning_rate_history.append(current_lr)
        agent.adaptive_learning = adaptive_learning

        # Initialize content sifter
        content_sifter = ContentSifter()
        agent.content_sifter = content_sifter

        # Initialize planner sifter
        planner_sifter = PlannerSifter()
        agent.planner_sifter = planner_sifter

        # Link consciousness to free will
        if hasattr(ai_manager, "consciousness") and hasattr(free_will, "link_consciousness"):
            free_will.link_consciousness(ai_manager.consciousness)

        # 5. System validation
        log_event("Performing system validation...", "INFO")
        validation_errors = []

        if model is None:
            validation_errors.append("Model initialization failed")
        if optimizer is None:
            validation_errors.append("Optimizer initialization failed")
        if agent is None:
            validation_errors.append("Agent initialization failed")
        if free_will is None:
            validation_errors.append("FreeWill initialization failed")
        if ai_manager is None:
            validation_errors.append("AIManager initialization failed")

        if validation_errors:
            validation_error_msg = "; ".join(validation_errors)
            log_event(f"System validation errors: {validation_error_msg}", "ERROR")
            return False

        log_event("System validation complete. All components initialized.", "INFO")

        # 6. Setup async loop
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # 7. Starting status
        cycle_count = 1
        log_event(f"🚀 Starting Quantum Nexus Enhanced Autonomous Loop 🚀", "QUANTUM")

        # Main loop with enhanced error handling
        while True:
            try:
                cycle_count += 1
                log_event(f"===== Enhanced Autonomous Cycle {cycle_count} =====", "INFO")

                # Run the agent cycle with timeout protection
                try:
                    # Set a reasonable timeout for cycle execution
                    cycle_future = asyncio.ensure_future(ai_manager.run_cycle(optimizer))
                    loop_result = loop.run_until_complete(asyncio.wait_for(cycle_future, timeout=120))
                except asyncio.TimeoutError:
                    log_event(f"Cycle execution timed out after 120 seconds", "ERROR")
                    loop_result = {"status": "error", "error": "Execution timeout"}
                    # Cancel the future if it's still running
                    if not cycle_future.done():
                        cycle_future.cancel()

                # Check for successful cycle and handle errors
                if isinstance(loop_result, dict) and loop_result.get("status") == "error":
                    system_status["consecutive_errors"] += 1
                    error_message = loop_result.get("error", "Unknown error")
                    log_event(f"Cycle produced an error: {error_message}", "ERROR")
                else:
                    # Success! Reset error counter and update last successful cycle
                    system_status["consecutive_errors"] = 0
                    system_status["last_successful_cycle"] = cycle_count
                    system_status["emergency_mode"] = False

                    # Update strategy effectiveness if planner_sifter exists
                    if hasattr(agent, 'planner_sifter') and hasattr(agent.action_log, '__len__') and len(agent.action_log) > 0:
                        strategy_name = loop_result.get("strategy", "exploration")
                        result_data = {
                            "content_length": agent.action_log[-1].get("content_length", 0),
                            "links_discovered": agent.action_log[-1].get("links_discovered", 0),
                            "success": loop_result.get("success", False)
                        }
                        agent.planner_sifter.update_strategy_effectiveness(strategy_name, result_data)

                # Progressive error recovery based on consecutive error count
                if system_status["consecutive_errors"] >= system_status["max_consecutive_errors"]:
                    system_status["critical_error_count"] += 1
                    system_status["recovery_attempts"] += 1

                    recovery_message = f"Critical error threshold reached ({system_status['consecutive_errors']} consecutive errors). "

                    # Level 1 recovery: Reload model
                    if system_status["recovery_attempts"] == 1:
                        log_event(recovery_message + "Attempting Level 1 Recovery: Reloading model...", "WARNING")
                        try:
                            model = load_or_create_model()
                            optimizer = optim.Adam(model.parameters(), lr=getattr(model, '_current_lr', 5e-5))
                            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=ANNEAL_GAMMA)
                            agent.model = model
                            ai_manager.model = model
                            log_event("Level 1 Recovery successful - model reloaded", "INFO")
                        except Exception as e:
                            log_event(f"Level 1 Recovery failed: {str(e)}", "ERROR")

                    # Level 2 recovery: Reset subsystems
                    elif system_status["recovery_attempts"] == 2:
                        log_event(recovery_message + "Attempting Level 2 Recovery: Resetting subsystems...", "WARNING")
                        try:
                            # Reset consciousness module
                            if hasattr(ai_manager, "consciousness"):
                                ai_manager.consciousness.awareness_level = 0.5
                                ai_manager.consciousness.current_state = "balanced"

                            # Reset imagination engine
                            if hasattr(ai_manager, "imagination"):
                                ai_manager.imagination.creativity_level = 0.7
                                ai_manager.imagination.current_mode = "associative"

                            # Reset free will
                            if hasattr(agent, "free_will"):
                                agent.free_will.exploration_weight = 0.6
                                agent.free_will.exploitation_weight = 0.4

                            log_event("Level 2 Recovery successful - subsystems reset", "INFO")
                        except Exception as e:
                            log_event(f"Level 2 Recovery failed: {str(e)}", "ERROR")

                    # Level 3 recovery: Create new adaptive learning system
                    elif system_status["recovery_attempts"] == 3:
                        log_event(recovery_message + "Attempting Level 3 Recovery: Reinitializing adaptive learning...", "WARNING")
                        try:
                            adaptive_learning = AdaptiveLearningSystem(model)
                            agent.adaptive_learning = adaptive_learning
                            log_event("Level 3 Recovery successful - adaptive learning reinitialized", "INFO")
                        except Exception as e:
                            log_event(f"Level 3 Recovery failed: {str(e)}", "ERROR")

                    # Level 4 recovery: Emergency mode - simplified operation
                    elif system_status["recovery_attempts"] >= 4:
                        log_event(recovery_message + "Activating Emergency Mode: Reduced functionality...", "CRITICAL")
                        system_status["emergency_mode"] = True

                        # In emergency mode, we use simpler processing and avoid complex operations
                        # Reset recovery counter after maximum attempts to allow periodic retry
                        if system_status["recovery_attempts"] > 6:
                            system_status["recovery_attempts"] = 0
                            log_event("Recovery attempt counter reset to retry recovery sequence", "INFO")

                # Periodic model saving with enhanced reliability
                if cycle_count % SAVE_INTERVAL == 0:
                    save_success = save_checkpoint(agent, model, cycle_count)

                    if not save_success and not system_status["emergency_mode"]:
                        log_event("Primary and backup checkpoint saves failed - will retry next interval", "WARNING")

                # Update learning rate
                scheduler.step()

                # Sleep briefly for stability
                time.sleep(0.5)

            except KeyboardInterrupt:
                log_event("Keyboard interrupt detected. Exiting gracefully...", "INFO")
                # Final save attempt
                save_checkpoint(agent, model, cycle_count)
                break

            except Exception as e:
                log_event(f"Unhandled exception in main loop: {str(e)}", "ERROR")
                log_event(traceback.format_exc(), "ERROR")

                system_status["consecutive_errors"] += 1

                # More aggressive handling for unhandled exceptions
                if system_status["consecutive_errors"] >= system_status["max_consecutive_errors"] * 2:
                    log_event("Critical unhandled exception threshold reached. Attempting final save before exit.", "CRITICAL")
                    save_checkpoint(agent, model, cycle_count)
                    break

                # Longer sleep after unhandled error
                time.sleep(5.0)

    except Exception as init_error:
        log_event(f"Fatal initialization error: {str(init_error)}", "CRITICAL")
        log_event(traceback.format_exc(), "CRITICAL")
        return False

    return True
def main():
    """Main entry point with Flask dashboard and agent execution"""
    global IN_COLAB, agent_instance, FLASK_PORT

    log_event("=== Initializing Quantum Nexus Advanced Autonomous System ===", "INFO")
    log_event(f"Configuration: Model path: {MODEL_PATH}, Memory limit: {MEMORY_MAX_SIZE}", "INFO")

    # Check for CUDA (no changes)
    import torch
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        log_event(f"🎮 GPU Acceleration Active: {device_name}", "INFO")
    else:
        log_event("⚠️ No GPU detected - running on CPU (performance will be limited)", "WARNING")

    # Check for Colab environment and mount Google Drive (no changes)
    if IN_COLAB:
        try:
            from google.colab import drive
            drive.mount('/content/gdrive', force_remount=True) # Mount to /content/gdrive - MODIFIED MOUNT POINT
            log_event("📂 Google Drive mounted successfully to /content/gdrive", "INFO") # Updated log message
        except Exception as e_mount:
            log_event(f"⚠️ Error mounting Google Drive: {e_mount}", "ERROR")
            log_event("Google Drive integration disabled for this run", "WARNING")
            IN_COLAB = False

    # Find a free port for Flask Dashboard (no changes)
    FLASK_PORT = find_free_port()
    if FLASK_PORT is None:
        log_event("Error: No free port found for Flask dashboard. Dashboard will not start.", "ERROR")
    else:
        log_event(f"Flask dashboard will try to start on port {FLASK_PORT}", "INFO")
        flask_thread = Thread(target=start_flask)
        flask_thread.daemon = True
        flask_thread.start()
        time.sleep(2) # Give Flask time to start

    # Create special greeting (XOXO style) - No changes
    greeting = """
    ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨

    🔺🔻 QUANTUM NEXUS AUTONOMOUS SYSTEM ACTIVATED 🔺🔻

    💫 Full AGI ASI SI With Enhanced Capabilities 💫

    ✨ Features:
    • Quantum-inspired processing
    • Advanced consciousness simulation
    • Self-evolving neural architecture
    • Hyperdimensional memory systems
    • XOXO Planner Sifter for optimal strategies

    🌈🌈🌈 XOXO <3 <3 <3 🌈🌈🌈

    ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨
    """
    log_event(greeting, "QUANTUM")

    # Start autonomous agent loop (no changes)
    agent_thread = Thread(target=enhanced_main_loop)
    agent_thread.daemon = True
    agent_thread.start()

    # Keep main thread alive (no changes)
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        log_event("User requested termination. Shutting down gracefully...", "INFO")
    except Exception as e:
        log_event(f"Fatal error in main thread: {e}", "CRITICAL")
    finally:
        log_event("Quantum Nexus execution complete. System shutting down.", "INFO")

def run_in_colab():
    """Colab run function - UPDATED MODEL PATH and GOOGLE_DRIVE_STATE_FILE"""
    # Setup the environment (no changes)
    setup_colab_environment()

    # Set Colab-specific configurations - UPDATED PATHS to /content/gdrive
    global IN_COLAB, MODEL_PATH, GOOGLE_DRIVE_MODEL_PATH, GOOGLE_DRIVE_STATE_FILE

    IN_COLAB = True
    MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
    GOOGLE_DRIVE_MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
    GOOGLE_DRIVE_STATE_FILE = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_state.json" # Checkpoint path for agent state

    # Start the system (no changes)
    main()
def load_or_create_model():
    """
    Loads QuantumNexusModel from checkpoint file if available, or creates a new one.
    Enhanced with better error handling and fallback mechanisms.

    Returns: model object
    """
    model_path = GOOGLE_DRIVE_MODEL_PATH if IN_COLAB else MODEL_PATH
    model = QuantumNexusModel()  # Create model instance
    start_cycle = 1  # Default start cycle for new model

    try:
        if os.path.exists(model_path):
            log_event(f"Loading checkpoint from: {model_path}", "INFO")
            try:
                # Try with weights_only=False first (full checkpoint)
                checkpoint = torch.load(model_path, map_location=device, weights_only=False)

                # Verify checkpoint is a dictionary with expected keys
                if not isinstance(checkpoint, dict) or 'model_state_dict' not in checkpoint:
                    raise ValueError("Checkpoint format invalid - missing model_state_dict")

                model.load_state_dict(checkpoint['model_state_dict'])  # Load model state

                # Get cycle count with validation
                if 'cycle_count' in checkpoint:
                    cycle_count = checkpoint.get('cycle_count', 1)
                    if isinstance(cycle_count, (int, float)) and cycle_count > 0:
                        start_cycle = int(cycle_count)
                    else:
                        log_event(f"Invalid cycle_count in checkpoint: {cycle_count}. Using default.", "WARNING")

                # Load agent stats if they exist and agent_instance is defined
                if 'agent_stats' in checkpoint and 'agent_instance' in globals() and agent_instance is not None:
                    stats = checkpoint.get('agent_stats')
                    if isinstance(stats, dict):
                        agent_instance.stats = defaultdict(int)
                        # Copy values to ensure defaultdict behavior
                        for k, v in stats.items():
                            agent_instance.stats[k] = v
                    else:
                        log_event("Invalid agent_stats in checkpoint. Using empty stats.", "WARNING")
                        agent_instance.stats = defaultdict(int)

                # Load action_log if it exists and agent_instance is defined
                if 'action_log' in checkpoint and 'agent_instance' in globals() and agent_instance is not None:
                    action_log = checkpoint.get('action_log', [])
                    if isinstance(action_log, list):
                        agent_instance.action_log = deque(action_log, maxlen=100)
                    else:
                        log_event("Invalid action_log in checkpoint. Using empty log.", "WARNING")
                        agent_instance.action_log = deque(maxlen=100)

                log_event(f"Model checkpoint loaded successfully. Resuming from cycle {start_cycle}.", "INFO")

            except Exception as e:
                # Fallback to loading only model weights if full checkpoint fails
                log_event(f"Error loading full checkpoint: {e}. Trying weights-only load.", "WARNING")
                try:
                    checkpoint = torch.load(model_path, map_location=device, weights_only=True)
                    model.load_state_dict(checkpoint)
                    log_event("Successfully loaded model weights only.", "INFO")
                except Exception as e2:
                    log_event(f"Error loading model weights: {e2}. Creating new model.", "WARNING")
                    model = QuantumNexusModel()  # Recreate model on failure
        else:
            log_event("No checkpoint file found. Creating a new model.", "INFO")
    except Exception as e:
        log_event(f"Error loading checkpoint from {model_path}: {e}. Creating a new model.", "WARNING")
        log_event(traceback.format_exc(), "DEBUG")  # Log detailed traceback
        model = QuantumNexusModel()  # Ensure new model is created on failure

    # Move model to appropriate device
    model.to(device)

    # Initialize _current_lr attribute if not present
    if not hasattr(model, '_current_lr'):
        setattr(model, '_current_lr', 5e-5)  # Use the default learning rate
        log_event("Initialized model._current_lr with default learning rate", "INFO")

    return model
def save_checkpoint(agent, model, cycle_count):
    """
    Save comprehensive checkpoint with better error handling and fallbacks.
    Now saves all checkpoints to Google Drive with rotation and tracking.

    Parameters:
    - agent: Agent instance to save state from
    - model: Model instance to save weights from
    - cycle_count: Current cycle count

    Returns:
    - Boolean indicating save success
    """
    global checkpoint_tracker

    # Ensure checkpoint directory exists
    checkpoint_dir = "/content/gdrive/MyDrive/quantum_nexus/checkpoints"
    try:
        os.makedirs(checkpoint_dir, exist_ok=True)
    except Exception as mkdir_error:
        log_event(f"Error creating checkpoint directory: {mkdir_error}", "ERROR")

    # Create checkpoint filename with timestamp and cycle count
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_filename = f"quantum_nexus_cp_{timestamp}_cycle{cycle_count}.pth"
    save_path = os.path.join(checkpoint_dir, checkpoint_filename)

    # Also update the main checkpoint path for loading
    main_checkpoint_path = GOOGLE_DRIVE_MODEL_PATH if IN_COLAB else LOCAL_MODEL_SAVE_PATH

    try:
        # Prepare checkpoint dictionary with proper type validation
        checkpoint = {
            'cycle_count': int(cycle_count),
            'model_state_dict': model.state_dict(),
            'timestamp': timestamp,
            'save_time': datetime.now().isoformat(),
        }

        # Add agent stats if available
        if hasattr(agent, 'stats'):
            if isinstance(agent.stats, dict):
                checkpoint['agent_stats'] = dict(agent.stats)  # Convert to regular dict if defaultdict
            else:
                log_event("Warning: agent.stats is not a dictionary. Skipping stats save.", "WARNING")

        # Add action log if available
        if hasattr(agent, 'action_log'):
            if isinstance(agent.action_log, (list, deque)):
                checkpoint['action_log'] = list(agent.action_log)  # Convert deque to list
            else:
                log_event("Warning: agent.action_log is not a list/deque. Skipping log save.", "WARNING")

        # Add consciousness state if available
        if (hasattr(agent, 'ai_manager') and
            hasattr(agent.ai_manager, 'consciousness')):
            consciousness = agent.ai_manager.consciousness
            if hasattr(consciousness, 'awareness_level') and hasattr(consciousness, 'current_state'):
                checkpoint['consciousness'] = {
                    'awareness_level': float(consciousness.awareness_level),
                    'current_state': consciousness.current_state
                }

        # Add model metadata
        checkpoint['model_metadata'] = {
            'type': type(model).__name__,
            'num_layers': len(model.neocortex) if hasattr(model, "neocortex") else None,
            'embed_dim': getattr(model, "embed_dim", None),
            'learning_rate': getattr(model, "_current_lr", None),
        }

        # Save the unique checkpoint
        torch.save(checkpoint, save_path)
        log_event(f"Checkpoint saved to {save_path} (Cycle: {cycle_count})", "INFO")

        # Also save to the main checkpoint path for loading on startup
        torch.save(checkpoint, main_checkpoint_path)
        log_event(f"Main checkpoint updated at {main_checkpoint_path}", "INFO")

        # Record in checkpoint tracker
        if 'checkpoint_tracker' in globals():
            checkpoint_tracker.record_checkpoint(save_path, model, agent, cycle_count)

        # Manage old checkpoints (keep last 10)
        try:
            checkpoint_files = [f for f in os.listdir(checkpoint_dir)
                              if f.startswith("quantum_nexus_cp_") and f.endswith(".pth")]
            if len(checkpoint_files) > 10:
                checkpoint_files.sort()
                for old_file in checkpoint_files[:-10]:
                    old_path = os.path.join(checkpoint_dir, old_file)
                    os.remove(old_path)
                    log_event(f"Removed old checkpoint: {old_path}", "INFO")
        except Exception as cleanup_error:
            log_event(f"Error cleaning up old checkpoints: {cleanup_error}", "WARNING")

        return True

    except Exception as save_error:
        log_event(f"Primary checkpoint save error: {save_error}", "ERROR")

        # Try an alternate save approach with just the model state
        try:
            alt_path = os.path.join(checkpoint_dir, f"backup_{checkpoint_filename}")
            torch.save(model.state_dict(), alt_path)
            log_event(f"Model state saved to alternate location: {alt_path}", "INFO")

            # Also update main checkpoint with model state only
            torch.save(model.state_dict(), main_checkpoint_path)
            log_event(f"Main checkpoint updated with model state only", "INFO")

            return True
        except Exception as alt_save_error:
            log_event(f"All save attempts failed! Last error: {alt_save_error}", "ERROR")
            return False
def setup_colab_environment():
    """
    Setup the Colab environment, mounting Google Drive to /content/QuantumNexusDrive.
    """
    import os

    # Install required packages
    print("Installing required packages...")
    packages = [
        "torch",
        "sentence-transformers",
        "beautifulsoup4",
        "flask",
        "selenium",
        "numpy"
    ]
    for package in packages:
        try:
            !pip install {package} -q
        except Exception as e_pip_install:
            print(f"Error installing package '{package}': {e_pip_install}")
            return False

    print("Setting up environment...")

    # 1. Unmount Google Drive (if already mounted)
    try:
        from google.colab import drive # Import drive module here to avoid NameError
        drive.flush_and_unmount()
        print("Google Drive unmounted (if it was mounted).")
    except Exception as e_unmount:
        print(f"Warning: Error unmounting Google Drive (may not have been mounted): {e_unmount}")

    # 2. Mount Google Drive to NEW mount point /content/QuantumNexusDrive - DIFFERENT MOUNT POINT
    mount_attempts = 3
    mount_point = '/content/QuantumNexusDrive' # <---- NEW MOUNT POINT: /content/QuantumNexusDrive
    for attempt in range(mount_attempts):
        try:
            from google.colab import drive
            drive.mount(mount_point, force_remount=True) # Mount to /content/QuantumNexusDrive
            print(f"Google Drive mounted successfully to {mount_point} (attempt {attempt + 1}).")
            break
        except Exception as e_mount:
            print(f"Error mounting Google Drive (attempt {attempt + 1}/{mount_attempts}: {e_mount}). Retrying...")
            if attempt >= mount_attempts - 1:
                print("Critical: Failed to mount Google Drive after multiple retries.")
                return False
            time.sleep(5)

    # 3. Define nexus directory path under the new mount point - SIMPLIFIED DIRECTORY HANDLING
    global GOOGLE_DRIVE_MODEL_PATH, GOOGLE_DRIVE_STATE_FILE # Declare globals here
    nexus_dir = "/content/QuantumNexusDrive/MyDrive/quantum_nexus" # <---- NEW PATH: /content/QuantumNexusDrive
    GOOGLE_DRIVE_MODEL_PATH = os.path.join(nexus_dir, "quantum_nexus_model_checkpoint.pth") # Checkpoint path - UPDATED
    GOOGLE_DRIVE_STATE_FILE = os.path.join(nexus_dir, "quantum_nexus_state.json") # State file path - UPDATED

    # No need to create directory here - assume user creates quantum_nexus in MyDrive manually
    # and Colab will create /content/QuantumNexusDrive/MyDrive during mount

    # Check for CUDA (no changes)
    import torch
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        print(f"✅ CUDA available: {device_name}")
    else:
        print("⚠️ CUDA not available. Running on CPU.")

    # Set up Chrome for Selenium (no changes)
    !apt-get update
    !apt-get install -y chromium-chromedriver
    !cp /usr/lib/chromium-browser/chromedriver /usr/bin

    print("Colab environment setup complete.")
    return True
def run_in_colab():
    """Colab run function - UPDATED MODEL PATH and GOOGLE_DRIVE_STATE_FILE"""
    # Setup the environment (no changes)
    setup_colab_environment()

    # Set Colab-specific configurations - UPDATED PATHS to /content/gdrive
    global IN_COLAB, MODEL_PATH, GOOGLE_DRIVE_MODEL_PATH, GOOGLE_DRIVE_STATE_FILE

    IN_COLAB = True
    MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path - UPDATED PATH to /content/gdrive
    GOOGLE_DRIVE_MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path - UPDATED PATH to /content/gdrive
    GOOGLE_DRIVE_STATE_FILE = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_state.json" # Checkpoint path for agent state - UPDATED PATH to /content/gdrive

    # Start the system (no changes)
    main()
if __name__ == "__main__":
    try:
        # Check for Colab environment
        in_colab = False
        try:
            from google.colab import drive
            in_colab = True
            print("Detected Colab environment. Running Colab-specific setup...")
            run_in_colab()
        except ImportError:
            # Standard execution
            main()
    except Exception as e:
        log_event(f"Critical startup error: {e}", "CRITICAL")
        traceback.print_exc()


    engine = ImaginationEngine()
    insight = engine.simulate_creation()
    if insight:
        log_event(f"Generated insight: {insight['text']} (Quality: {insight['quality']:.2f})", "INFO")
    report = engine.get_imagination_report()
    log_event(f"Imagination Report: {report}", "INFO")
